[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for Food Science",
    "section": "",
    "text": "Preface\nDuring the past decades the production of data in relation to research, production, consumer behavior, social network etc. has increased dramatically. Today we are faced with data structures which were unimaginable just 50 years ago. Traditionally, a system under investigation were characterized by a few samples associated with say one to five descriptors and, carefully selected, responses. Today all aspects of the classical system interrogation has blown up, such that we have many more samples (e.g. production monitoring every minute), more descriptors (e.g. consumer characteristics), and by far more response variables (e.g. high throughput omics technologies). Tools developed for handling traditional scenarios still pertain the corner of how to approach today’s data analytical challenges, however, by the development of computers, it is possible to carry out challenging mathematical procedures in no time and further produce visual graphics as resources for translating information into knowledge. Due to this fact, the traditional tools has gotten a makeover and new tools has been developed.\nFood is, as such, an extremely inherent part of the human life, although one could argue that so is e.g. cardiovascular biology and governmental policy making, these subjects either work autonomously or does not demand everyday mental capacity. Everyday all humans need to eat- and drink in some social context, pay attention to the perception of the meal, and further deal with the possible health- and emotional implications of this process. When studying food science all these aspects are relevant. 14\nFood science constitute a broad range of disciplines spanning controlled artificial model systems, over functional modification of real food matrices, production technology, to the relation between food- and meal composition, taste, perception and health. All by means of data.\nThese notes are thought to cover data analysis within food science. That is to; provide a general understanding of the purpose of data analysis, found a theoretical- and practical basis for understanding various numerical and graphical tools and couple generic tools to concrete issues within related disciplines. To this end by theory, examples and exercises.\nThe Book material used in these notes are mostly from the notes for the course; Introduction to Statistics at DTU by P.B. Brockhoff and co workers. Additionally there are relevant chapters from other sources. All exercises are custom made and deal with real problems within food science.\nWelcome to the course in Fødevaredataanalyse for second year bachelor students in Food Science and Technology - Hope you will enjoy learning about how to use data for getting insight on food systems.\nMorten Arendt Vils Rasmussen\nAugust 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html",
    "href": "chapters/week1/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Hand-in assignment\nThis first week is going to introduce basic descriptive tools for getting a primary overview of data. These are divided into representative numerics, which we call descriptive statistics and various plots. Especially for the plotting part you will be needing a computer program. We strongly encourage you to get familiar with R, that, although not being as intuitive point-and-click as the widely used programs such as excel, is capable of conducting almost any type of sophisticated analysis you may wish, and further will strengthen you to become familiar with a scientific programming language - a generic competence useful whenever working with information.\nIncluded in the week 1 notes are material related to working in R; Installing packages, importing data, working in scripts and debugging your code. This material you should try to cover briefly and then use it when you get stuck on a problem throughout the course (and after). However, there is a huge amount of videos, tutorials, etc. on the web which you can also use, and we encourage you to get familiar with these resources as well. Simply type your problem in google and check if others have experienced something similar.\nExercise 5.2 Analysis of coffee serving temperatura - PCA is to be handed in (through Absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important. If you have problems with R, then try to write what you would have done if you did not experience problems with the machinery.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#exercises",
    "href": "chapters/week1/week_1.html#exercises",
    "title": "Week 1",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 2.1\nExercise 2.2\nExercise 4.1\nExercise 4.2\nExercise 4.3\n\nFor Wednesday work through the following exercises:\n\nExercise 4.4\nExercise 5.3\n\nYou will most likely not be able to complete all exercises within the hours in the classroom, so we recommend that you use some time in advance to initiate the task.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/week_1.html#case-i",
    "href": "chapters/week1/week_1.html#case-i",
    "title": "Week 1",
    "section": "Case I",
    "text": "Case I\nThe first, of a total of four cases, are described in the document “Case1.pdf”. You should work on the case in groups of four, and hand in a slide-show with voice no later than Thursday evening next week. Be aware, that a lot of the technical stuff can be zacked from the exercises, so you might want to finalize those in advance of the case.",
    "crumbs": [
      "Week 1"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html",
    "href": "chapters/week1/r_basics.html",
    "title": "1  R Basics",
    "section": "",
    "text": "1.1 Installing packages\nA package in R is a set of commands which are not a part of the base-set in R. Many of the R-commands which are used throughout this course requires a certain package to be installed on the computer/Mac. It is a good idea to get familiar with Installing packages and loading them onto your R-script mainly so you won’t be missing them at the exercises, casework or examination.\nIn R there are two important commands concerning installation of packages.\nFor example: install.packages(’readxl’) Installs the package readxl on the computer and remove.packages(’readxl’) uninstalls the package readxl from the computer.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#installing-packages",
    "href": "chapters/week1/r_basics.html#installing-packages",
    "title": "1  R Basics",
    "section": "",
    "text": "install.packages() installs the target package on your computer.\nremove.packages() uninstalls the package from your computer.\n\n\n\n1.1.1 Loading a package\nWhen the packages are installed on the computer, you can load them onto your workspace/script at every occasion you initiate your analysis in R. To do this, you use the library() command. library() points at a package-library stored on your computer. Everytime you open a new session of R, you need to load the needed packages again.\nFor example, library(readxl) Loads the package readxl onto the workspace.\nWhen you load a package, you might get warning messages like the following:\n\nlibrary(readxl)\n\nAdvarselsbesked:\npakke ‘readxl’ blev bygget under R version 3.1.3",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "href": "chapters/week1/r_basics.html#sec-r-basics-working-directory",
    "title": "1  R Basics",
    "section": "1.2 Working directory",
    "text": "1.2 Working directory\nIn R you are using something called a working directory or wd for short. This is the folder on your computer in which R saves and finds the projects that you are working on. This also makes it easier to load datasets. The working directory can be changed in R either manually or through code. getwd() and setwd() are the two important commands for changing the working directory.\n\ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/Statistik/Exercises/Week 1”\n\nsetwd(\"~/Documents/R-træning\") # Change the working directory \ngetwd() # Show the current working directory\n\n“/Users/madsbjorlie/Documents/R-træning”",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#importing-a-dataset",
    "href": "chapters/week1/r_basics.html#importing-a-dataset",
    "title": "1  R Basics",
    "section": "1.3 Importing a dataset",
    "text": "1.3 Importing a dataset\nThroughout this course you will need to import a lot of data into R. Getting familiar with the following packages and commands will help minimize your R-related frustration. Datasets can be imported into R in numerous ways. Like changing the working directory, it can be done both manually and through coding. We recommend doing it through coding since this makes it easier to maintain an overview.\nAlmost all of the datasets that will be handed out in this course will be in both the excel file-type .xlsx, as well as the .RData format. R is also capable of importing text-files such as .txt or .csv.\n.xlsx-files are Microsoft Excel’s standard project file type, whereas .csv-files are short for comma separated values and is a term for text-files where the values are separated by a comma (or in the Danish Excel, a semicolon).\nYou can either load .RData files, import datasets through R’s inherent commands or use some data-import packages to import file-types such as .xlsx or .xls. Both methods works fine and which one you will use depends on your personal preference.\n\n1.3.1 Importing an .RData file\nIf someone imported and stored the data as an .RData file, you can simply import it using the load() function. For this you do not need any libraries.\n\nload(file.choose())\nload(\"~/Documents/..../Beer GCMS.RData”)\n\nThe only difference in comparison with the import-methods below, is that you do not “pipe” (the &lt;- function) the object into something you name yourself. The data object will retain the name as it was saved with. However, if you like your objects to be named something special (like X), then simply just add a line below the load() where you define it: e.g., X &lt;- beer.\n\n\n1.3.2 Importing a dataset through R’s own commands\nAs a default, R can not import Excel-files such as .xls and .xlsx. To use R’s read.csv() function, you need to save the Excel dataset as a .csv file. This is done by choosing (in Excel) and then selecting the .csv file-type. This might seem a bit tedious, but it eliminates the demand for other packages.\nread.csv() imports the dataset specified in the parenthesis. This can be done in two ways: by typing the path to the file on your computer or by using the command file.choose() which corresponds to opening a new file. If the dataset is in the working directory, you do not have to type the full path, but just the file-name.\nFor example:\n\nBeer &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(”Beerdata.csv”, header=TRUE, sep=\";\", dec=\",\")  \nBeer &lt;- read.csv(\"~/Documents/R-traening/Øldata.csv”, header=TRUE, sep=\";\", dec=\",\")\n\nThe different arguments: header =, sep =, and dec = tells R how to import the data. header=TRUE tells R that the first row in the dataset is not a part of the data itself but carries the variable names. sep=”;” defines which separator the document uses. By using Danish Excel, this will always be semicolon. This can be checked by opening the dataset in NotePad on windows or TextEditor on Mac. dec=”,” defines which symbol is used for decimals. It is necessary to make sure that the dataset in R is separated by a full stop rather than a comma. This can be checked by using summary commands after the data has been imported.\n\n\n1.3.3 Importing a dataset using packages\nBy using various packages, it is possible to import Excel-documents directly into R. This can be quite handy, but some of the packages will not run on Mac or on Windows due to other programs missing. The most commonly used data-import packages are: gdata, readxl, xlsx and rio. gdata requires Perl which is default on Mac and Linux but not on Windows and therefor it will not run on Windows unless it is installed. The following is a couple of examples using the various packages.\n\nlibrary(readxl)  \nBeer &lt;- read_excel(file.choose())  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- read_excel(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\nlibrary(gdata)  \nBeer &lt;- read.xls(file.choose())  \nBeer &lt;- read.xls(\"~/Documents/R-traening/Beerdata.xls”)  \n\nlibrary(xlsx)  \nBeer &lt;- read.xlsx(file.choose(), sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xls”, sheetIndex = 1)  \nBeer &lt;- read.xlsx(\"~/Documents/R-traening/Beerdata.xlsx”, sheetIndex = 1)  \n\nlibrary(rio)  \nBeer &lt;- import(file.choose())  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xls”)  \nBeer &lt;- import(\"~/Documents/R-traening/Beerdata.xlsx”)  \n\n\n\n1.3.4 Getting an overview of the dataset\nWhen the dataset is imported into R, you can use different commands to check that it was imported correctly. The commands are head(), str() and dim().\n\nhead() shows the first 6 rows in the dataset.\n\nstr() shows the types of the various columns, such as numeric and factor.\n\ndim() shows the dimensions of the data-matrix.\n\n\nCoffee &lt;- read.csv(file.choose(), header=TRUE, sep=\";\", dec=\",\")\nhead(Coffee)\n\n\n\n  Sample Assessor Replicate Intensity  Sour Bitter\n1    31C        1         1      9.30  6.90   6.75\n2    31C        1         2      8.70  8.10   7.95\n3    31C        1         3      9.75  8.70  10.20\n4    31C        1         4     11.70 10.95  11.40\n5    31C        2         1      8.70  5.70  11.40\n6    31C        2         2      8.70  9.30  10.35\n\n\n\nstr(Coffee)\n\n\n\n'data.frame':   192 obs. of  6 variables:\n $ Sample   : Factor w/ 6 levels \"31C\",\"37C\",\"44C\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Assessor : int  1 1 1 1 2 2 2 2 3 3 ...\n $ Replicate: int  1 2 3 4 1 2 3 4 1 2 ...\n $ Intensity: num  9.3 8.7 9.75 11.7 8.7 ...\n $ Sour     : num  6.9 8.1 8.7 10.9 5.7 ...\n $ Bitter   : num  6.75 7.95 10.2 11.4 11.4 ...\n\n\n\ndim(Coffee) # show dimensions of data\n\n\n\n[1] 192   6",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#scripts",
    "href": "chapters/week1/r_basics.html#scripts",
    "title": "1  R Basics",
    "section": "1.4 Scripts",
    "text": "1.4 Scripts\nWe highly recommend that you make your data analysis using a script. A script is simply a flat text file that is given the surname .R such that R can interpret the commands. Here you will have the commands needed to do the analysis from setting necessary functions, import of data, initial inspection, modeling and plots.\nEach analysis task is slightly different, however, almost always there is a set of generic tasks which is always needed. That is: cleaning up the workspace, loading packages, setting work directory, loading data and checking the data structure. That typically fills up the first 5-10 lines of code in every script as follows:\n\nrm(list = ls()) # remove all variables in environment\ngraphics.off() # delete all generated plots\n\nlibrary(ggplot2) # load package ggplot2\nlibrary(readxl) # load package readxl\n\nsetwd(\"~/MyComputer/Courses/FDA/Exercises/Week1\") # set working directory\n\ndata &lt;- read_excel(\"SomeData.xlsx\") # load file\n\nhead(data) # show first 6 lines of data\n\n# plot data\nggplot(df, aes(x, y, color = treatment)) + \n    geom_point()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/r_basics.html#rmarkdown",
    "href": "chapters/week1/r_basics.html#rmarkdown",
    "title": "1  R Basics",
    "section": "1.5 Rmarkdown",
    "text": "1.5 Rmarkdown\nRmarkdown is a framework for producing beautiful reports where code and text is combined side-by-side. It makes it possible to combine scripts (your data analysis) with output (figures, tables and numbers) and narrative (the fairytale on why and discussions etc.) in ONE document. It is rather simple when you get used to how it works, and it really makes life much more easy. Both for this course, but also for all other tasks which involves data analysis and is to be presented as a report.\nIt is refereed to as reproducible data analysis, and is the opposite of Excel-Hell, where the latter is characterized by being non transparent, and really hard to figure out what happened between data and results.\n\nRmarkdown is especially recommended for the hand-in assignemnts doing the course.\n\n\n1.5.1 How to setup a Rmarkdown document\nTo setup a new Rmarkdown document do the following:\n\nIn RStudio go to File -&gt; New File -&gt; RMarkdown\nIn the popup set your title, author name and date (these can all be changed later).\nChoose your default output. HTML or PDF is recommended (this can be changed later).\nClick “OK”.\n\nNow your new document has been created. It contains some settings at the very top:\n\n---\ntitle: \"My first project\"\nauthor: \"Jeppe\"\ndate: \"`r Sys.Date()`\"\noutput: html_document # Change this to pdf_document for pdf output\n---\n\nThese settings can be changed whenever. For more info on fancy settings see guide on HTML outputs or guide on PDF outputs.\n\n\n1.5.2 Mixing code and text in Rmarkdown\n\n\n\n\n\n\nLaTeX distribution for pdf output\n\n\n\nIn order to create a PDF document from Rmarkdown you will need to have a LaTeX distribution installed on your system. TinyTex is recommended for this, and it can easily be installed from within R using the tinytex package:\n\nGo to RStudio and run the following code to install the package used for installing TinyTex:\n\n\ninstall.packages(\"tinytex\")\n\n\nRun the following code to install TinyTex using the tinytex package:\n\n\ntinytex::install_tinytex()\n\n\nYou should now be able to create a pdf output.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html",
    "href": "chapters/week1/descriptive_statistics.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#reading-material",
    "href": "chapters/week1/descriptive_statistics.html#reading-material",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "Chapter 1 of Introduction to Statistics by Brockhoff\n\nEspecially section 1.1 to 1.4.\n\nVideo lecture on central metrics (mean and median).\nVideo lecture on dispersion (variance, standard deviation etc.)\nVideo lecture on both central metrics and dispersion.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/descriptive_statistics.html#exercises",
    "href": "chapters/week1/descriptive_statistics.html#exercises",
    "title": "2  Descriptive statistics",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n\n\n\n\n\n\nExercise 2.1\n\n\n\n\n\n\n\n\n\nExercise 2.1 - Descriptive statistics by hand\n\n\n\n\n\n\nBelow (table 2.1) is listed a vector of ranking (Liking) of coffee served at 56°C by 52 consumers. The data is sorted.\n\n\n\n\nTable 2.1: Liking of coffee served at 56°C as ranked by 52 consumers.\n\n\n\n\n \n  \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n  \n \n\n  \n    2 \n    2 \n    3 \n    3 \n    4 \n    4 \n    4 \n    4 \n    4 \n    4 \n  \n  \n    5 \n    5 \n    5 \n    5 \n    5 \n    5 \n    5 \n    5 \n    5 \n    5 \n  \n  \n    5 \n    6 \n    6 \n    6 \n    6 \n    6 \n    6 \n    6 \n    6 \n    6 \n  \n  \n    6 \n    6 \n    7 \n    7 \n    7 \n    7 \n    7 \n    7 \n    7 \n    7 \n  \n  \n    7 \n    7 \n    7 \n    7 \n    7 \n    7 \n    7 \n    8 \n    8 \n    8 \n  \n  \n    8 \n    9 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n\n\n\n\n\n\n\n\nSome useful numbers:\n\\[\\sum{X} = 301\\]\n\\[\\sum{(X_i - \\bar{X})^2} = 122.7\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for this distribution of data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2\n\n\n\n\n\n\n\n\n\nExercise 2.2 - Descriptive statistics\n\n\n\n\n\n\nServing temperature of coffee seems of importance as to how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a consumer panel of 52 consumers, evaluating coffee served at six different temperatures on a set of sensorical descriptors leading to a total of \\(52 \\times 6 = 312\\) samples.\nIn the dataset the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going through some of the initial descriptive steps.\nIn the table below (table 2.2) a subset of the data is shown.\n\n\n\n\nTable 2.2: A subset of the Results Consumer Test.xlsx data\n\n\n\n\n \n  \n      \n    Sample \n    Temperatur \n    Assessor \n    ServingOrder \n    TemperatureJudgment \n    Liking \n    Intensity \n    Sour \n    Bitter \n    Sweet \n    Male \n    Female \n  \n \n\n  \n    1 \n    31C \n    31 \n    1 \n    6 \n    2 \n    3 \n    4 \n    3 \n    4 \n    2 \n    1 \n    0 \n  \n  \n    2 \n    31C \n    31 \n    2 \n    6 \n    2 \n    3 \n    7 \n    5 \n    8 \n    3 \n    1 \n    0 \n  \n  \n    3 \n    31C \n    31 \n    3 \n    6 \n    1 \n    3 \n    2 \n    1 \n    4 \n    6 \n    0 \n    1 \n  \n  \n    4 \n    31C \n    31 \n    4 \n    6 \n    1 \n    2 \n    5 \n    6 \n    4 \n    3 \n    1 \n    0 \n  \n  \n    5 \n    31C \n    31 \n    5 \n    6 \n    2 \n    2 \n    2 \n    3 \n    2 \n    1 \n    1 \n    0 \n  \n  \n    6 \n    31C \n    31 \n    6 \n    6 \n    2 \n    4 \n    3 \n    4 \n    2 \n    1 \n    1 \n    0 \n  \n  \n    307 \n    62C \n    62 \n    47 \n    6 \n    8 \n    8 \n    8 \n    2 \n    8 \n    1 \n    0 \n    1 \n  \n  \n    308 \n    62C \n    62 \n    48 \n    6 \n    6 \n    7 \n    7 \n    4 \n    3 \n    3 \n    0 \n    1 \n  \n  \n    309 \n    62C \n    62 \n    49 \n    6 \n    5 \n    8 \n    6 \n    6 \n    4 \n    6 \n    1 \n    0 \n  \n  \n    310 \n    62C \n    62 \n    50 \n    6 \n    6 \n    5 \n    7 \n    7 \n    7 \n    4 \n    0 \n    1 \n  \n  \n    311 \n    62C \n    62 \n    51 \n    6 \n    7 \n    6 \n    8 \n    6 \n    7 \n    2 \n    1 \n    0 \n  \n  \n    312 \n    62C \n    62 \n    52 \n    6 \n    6 \n    7 \n    6 \n    6 \n    7 \n    3 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data\n\nBe aware that the function read.xls() is not in the base library, so you need to add the specific library to your computer.\n\nSubsample on one temperature.\n\nBelow (listing 2.1) is listed two alternatives for doing this.\n\n\n\n\n\n\n\n\n\n\nListing 2.1: Importing and subsampling based on temperature.\n\n\nCoffee &lt;- read_excel( \" Results Consumer Test . xlsx \" )\nCoffee_t44_v1 &lt;- Coffee[Coffee$Temperatur == 44,]\nCoffee_t44_v2 &lt;- subset(Coffee, Temperatur == 44)\n\nmean(Coffee_t44_v1$Liking)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the descriptive statistics for centrality (mean and median), dispersion (IQR, standard deviation and range) and extremes (min and max) for this distribution of datapoints for a single descriptor (e.g. )\nNow do it for all temperatures.\n\nYou should get something like the table below (table 2.3).\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Summary table computed in R.\n\n\n\n\n \n  \n    Temp \n    N \n    Mean \n    Median \n    Std \n    Min \n    Max \n  \n \n\n  \n    31 \n    52 \n    3.576923 \n    3 \n    1.649078 \n    1 \n    7 \n  \n  \n    37 \n    52 \n    4.750000 \n    5 \n    1.780890 \n    1 \n    7 \n  \n  \n    44 \n    52 \n    5.826923 \n    6 \n    1.605397 \n    2 \n    9 \n  \n  \n    50 \n    52 \n    5.961538 \n    6 \n    1.596092 \n    2 \n    8 \n  \n  \n    56 \n    52 \n    5.788462 \n    6 \n    1.550920 \n    2 \n    9 \n  \n  \n    62 \n    52 \n    6.173077 \n    6 \n    1.367998 \n    2 \n    8 \n  \n\n\n\n\n\n\n\n\nThis can be quite tedious, and result in a lot of coding. However, the function summary() and aggreggate() are very efficient in producing such results. Try to check out these functions and see if you can use those to generate summary statistics. Below are shown some code which does exactly what you want without too many lines of code.\n\n\n\n\nListing 2.2: Generating a summary table with aggregate().\n\n\n# Include only responses\nCoffeeDT &lt;- Coffee[,2:10]\n\n# Run aggregate for each type of summary\ntmpN &lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'length')\ntmpM&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'mean')\ntmpM2&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'median')\ntmpS&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'sd')\ntmpMi&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'min')\ntmpMx&lt;-aggregate(CoffeeDT,by=list(CoffeeDT$Temperatur),FUN = 'max')\n\n# Merge these into a dataset\ntmp &lt;- cbind(tmpM$Temperatur,tmpN$Liking,tmpM$Liking,tmpM2$Liking,\n             tmpS$Liking,tmpMi$Liking,tmpMx$Liking)\n\n# Add a meaningfull label for each coloumn\ncolnames(tmp) &lt;- c('Temp','N','Mean','Median','Std','Min','Max') \nprint(tmp)\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nThe above is done for , try to do it for some of the other responses.\n\nHint: This can be done by repeating the code and exchange $Liking with e.g. $Bitter. However, putting this in a for loop is another option.\n\nWhat have you learned from analysing these data in terms of importance of serving temperature on the sensorical properties as percieved by consumers?\n\nHint: You can run the code below to get a comprehensive overview. This is based on the mean aggreggate, but you might just as well check some of the other descriptive metrics. For instance, what does the standard deviation tells you about consumers in general, and does the type of sensorical attribute and serving temperature make a difference on the spread in scoring?\n\n\n\n\n\n\n\n\n\n\nListing 2.3: Code for plotting the results of listing 2.2.\n\n\nmatplot(tmpM[,2],tmpM[,6:10],type='l',lwd=3)\ntext(cbind(60,t(tmpM[6,6:10])),colnames(tmpM[,6:10]))\n\n\n\n\nYou might want to fix some of the labels in these figures. Check the documentation by typing ?matplot and see how to add meaning full stuff to the plot.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html",
    "href": "chapters/week1/debugging.html",
    "title": "3  Debugging - Getting R to work",
    "section": "",
    "text": "3.1 Reading material\nWhen using any computer program you now and then encounter that it does not do as intended. However, it is so, that the program do exactly what it is told, which might not be in line with the task you anticipate conducted. R work by interpreting commands which are either written directly on the command line, or in the form of lines in a script which then is submitted to the R compiler. Sometimes instead of producing nice results and plots, R returns red stuff on the screen. Debugging is the process of figuring out why that is so, and change the code to do as anticipated.\nThe notes on debugging Debugging in R.pdf available through Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/debugging.html#exercises",
    "href": "chapters/week1/debugging.html#exercises",
    "title": "3  Debugging - Getting R to work",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\n\n\n\n\n\n\nExercise 3.1\n\n\n\n\n\n\n\n\n\nExercise 3.1\n\n\n\n\n\n\nFor some of you, coding in Rstudio may seem simple. The aim with these debugging tasks is to train you to analyze the errors Rstudio gives you, and to give you some tools to use to avoid issues when coding in Rstudio. Many of the debugging exercises throughout the course will be related to the datasets used in other exercises given in the same week, so you might find it sensible to do the debugging-exercises first, to get to know the datasets and their potential issues, before the struggle starts.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy won’t Rstudio read in the file in the following cases?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nError: could not find function “read_excel”\n\ncoffee &lt;- read_excel(\"Results Consumer Test\")\n\nError: “Results Consumer Test” does not exist in current working directory”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy isn’t the view-function working in the following case?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\nview(coffee)\n\nError: could not find function “view”\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is missing when you call for aggregate to do its calculations?\n\n\n\n\n\n\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\ncoffee_ag &lt;- aggregate(by = list(coffee$Assessor, coffee$Sample), FUN = \"sd\")\n\nError in is.ts(x) : argument “x” is missing, with no default",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Debugging - Getting R to work</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html",
    "href": "chapters/week1/plotting.html",
    "title": "4  Plotting",
    "section": "",
    "text": "4.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#reading-material",
    "href": "chapters/week1/plotting.html#reading-material",
    "title": "4  Plotting",
    "section": "",
    "text": "Chapter 1 in Introduction to Statistics by Brockhoff\n\nEspecially section 1.5 to 1.6.\n\nggplot2 cheat sheet. It is a very nice cheat sheet on how to use ggplot2.\n\nCheat sheets for other libraries also exist. You can find them here.\n\nOnline lectures: There is a lot of how-to-plot on the web. Use it when you are getting stuck at a problem, or if you feel like being inspired.\n\nThese are nice and short:\n\nCharts Are Like Pasta - Data Visualization Part 1: Crash Course Statistics #5.\nPlots, Outliers, and Justin Timberlake: Data Visualization Part 2: Crash Course Statistics #6.\n\nThis one is pretty comprehensive:\n\nVisualising data with ggplot2. Lecture by Hadley Wickham.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/plotting.html#exercises",
    "href": "chapters/week1/plotting.html#exercises",
    "title": "4  Plotting",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe package ggplot2 for R is a versatile tool for producing various plots with the option of modifying them in great detail. The following exercises should guide you through the basic functionality of the ggplot function and show you how you can layer different geoms to produce the plots you want.\n\n\n\n\n\n\nBefore you start\n\n\n\nStart by importing a dataset and specify some necessary packages (If you have not installed them on your local drive, you might need to do so). The following lines of code will do the job, if you specify the correct working directory.\n\n# Load package for importing Excel-files\nlibrary(readxl) \n\n# Import coffee data set\ncoffee &lt;- read_excel(\"Results Consumer Test.xlsx\")\n\nThere are several ways of importing data into R, depending on which format you have them in. From excel, the read.xls() (from the gdata library) or read_excel() (from the readxl library) are two ways of doing it. In either case, make sure that the data is correctly imported, by comparing the imported data with the original - sometimes there are problems with decimals.\n\n\nAssume that you have a single response variable, for instance alcohol content of a series of various types of drinks, measurements of body weight from an nutritional experiment or content of antioxidants for a given product produced under different conditions. For all of these, the variable is continuous in form. As a starting point of every analysis an overview of the distribution of the variable of interest is crucial, and plotting the distribution facilitate insight into character of distribution (bell shaped, skewness, bi modal, zero inflated etc.) and single point information for outlier identification.\n\n\n\n\n\n\nExercise 4.1\n\n\n\n\n\n\n\n\n\nExercise 4.1 - Plotting distributions with histograms\n\n\n\n\n\n\nThe histogram is the most basic representation of continuous data. A simple histogram can be produced in the following way with ggplot2.\n\nggplot(data = coffee, aes(Liking)) +\n  geom_histogram()\n\n\n\n\n\n\n\nFigure 4.1: A simple histogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are numerous additional arguments that can be used to modify this plot. Check out the documentation or google it to see how it is done.\nThe documentation can be found by running ?geom_histogram() or by having the text cursor in the function name while pressing F1.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nChange the color of the bars\nChange the bin width.\nChange the transparency of the bars (aka. alpha-value).\nAdd a title to the plot.\n\n\n\n\n\nThe data here are ”liking” of coffees at different temperatures, and so one might wish to infer this information in the histogram. This can be done by coloring by sample name.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_histogram(\n    position = \"dodge\", # Plot the colors side-by-side\n    binwidth = 0.8 # Set the width of the bins\n  )\n\n\n\n\n\n\n\nFigure 4.2: A histogram of the “Liking” variable in the coffee consumer dataset. The histogram is now colored by the sample name.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nLook at figure 4.2, how does the temperature affect the liking?\n\n\n\n\n\nIf you do not want to overlay the histograms, it is possible to plot them as individual panels. Try running the following code and see what it does.\n\nggplot(coffee, aes(Liking)) +\n  geom_histogram() +\n  facet_wrap(~ Sample) # Wrap by sample\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.2\n\n\n\n\n\n\n\n\n\nExercise 4.2 - Plotting distributions with densitograms\n\n\n\n\n\n\nA densitogram is a smoothed extension of the histogram, and as such represents the same type of information. The bin width in the histogram controls the resolution, whereas the counterpart in the densitogram is the degree of smoothing. By changing the geom used in the ggplot call the plot is changed to a densitogram.\n\nggplot(coffee, aes(Liking)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.3: A densitogram of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nAgain, there are numerous additional arguments that can be used to modify the plot. Read the documentation and try some of them out.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nModify the smoothing of the densitogram by changing the option adjust =\nTry to make the smoothing very refined (e.g. adjust = 0.3). Does this reflect the underlying distribution? What is a suitable smoothing option for these data?\n\n\n\n\n\nExactly as for the histogram, it is possible to infer additional information on the densitogram.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density()\n\n\n\n\n\n\n\nFigure 4.4: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable.\n\n\n\n\n\nThis plot is not optimal, as only the densitogram for temperature 62C is fully visible.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry adjusting the transparency by of the plot by setting alpha = 0.5.\n\n\n\n\n\nThe colors used in the plot could be more intuitive as they refer to temperature. There are several ways the to add a different color scheme. One is to add a layer to the plot specifying either a predefined color scheme, a modification of a predefined color scheme or simply by specifying each of the colors used. Here we just use a predefined Red to Blue-palette from ColorBrewer.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\"\n    )\n\n\n\n\n\n\n\nFigure 4.5: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer.\n\n\n\n\n\nHowever, the colors are in a counter intuitive direction. This can easily be corrected for by adding direction = -1 to the color scheme call.\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  )\n\n\n\n\n\n\n\nFigure 4.6: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using the Red to Blue-palette from ColorBrewer - this time in the “correct” direction.\n\n\n\n\n\nIf nothing seems to fit your ideal color-world, then you can simply specify the exact colors (HINT: try googling color codes for ggplot2 ).\n\ncustom_colormap &lt;- c(\"#0033FF\", \"#0099FF\",\"#00EEFF\", \n                     \"#FFCCCC\", \"#FF9999\",\"#FF0000\")\n\nggplot(coffee, aes(Liking, fill = Sample)) +\n  geom_density() +\n  scale_fill_manual(\n    values = custom_colormap\n  )\n\n\n\n\n\n\n\nFigure 4.7: A densitogram of the “Liking” variable in the coffee consumer dataset. The plot is now colored according to the “Sample” variable using manually specified colors.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots, and try to use other predefined color schemes. Further all these plots suffers from lack of transparency, so fix that as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.3\n\n\n\n\n\n\n\n\n\nExercise 4.3 - Boxplot, Jitterplot and Violinplot\n\n\n\n\n\n\nIn the above, the different temperatures were infered on the plot by overlaying histograms. However, the x-axis can be used for keeping track of this information. This is especially useful when you are comparing more than four levels. The code below produces four different plots for this purpose.\n\nggplot(coffee, aes(Sample, Liking, fill = Sample)) +\n  geom_boxplot() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Boxplot\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nFigure 4.8: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, color = Sample, shape = Sample)\n) +\n  geom_jitter() +\n  labs(title = \"Jitter plot\") +\n  theme_light()\n\n\n\n\n\n\n\nFigure 4.9: A jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_boxplot() +\n  geom_jitter() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Box n' jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 4.10: A box n’ jitter plot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\n\nggplot(data = coffee, \n       aes(Sample, Liking, fill = Sample, shape = Sample)\n) +\n  geom_violin() +\n  scale_fill_brewer(\n    palette = \"RdBu\",\n    direction = -1\n  ) +\n  labs(title = \"Violin plot\") +\n  theme_classic()\n\n\n\n\n\n\n\nFigure 4.11: A boxplot of the “Liking” variable in the coffee consumer dataset.\n\n\n\n\n\nThere are several things to notice from the plots above:\n\nThe title of a plot can be added by using labs(title = \"Title goes here) or ggtitle(\"Title goes here).\nThe background, and some other stuff, of the plots are different, and is inferred by adding + theme_XXX() (the default is theme is theme_gray()).\nIn the boxplot the colors are added by fill = Sample and in the jitter plot the colors are added by color = Sample. This is because the the points in the jitter plot do not have a “fill” (they are not “filled” with a color).\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to reconstruct these plots.\nThe boxplot has several features, such as a box with a line in the middle, some so-called whiskers and also maybe a few actual points. Check out what these refer to in the data, and calculate them directly on data to verify that the computer is not off.\n\n\n\n\n\n\n\n\n\n\n\nHow to save plots\n\n\n\nThe plots can saved as high resolution files by using the ggsave-function. Unless otherwise specified the plots are saved in the same aspect ratio as seen in the plot window of RStudio.\n\n# Save plot as PNG with a resoultion of 300 DPI (standard print resolution)\nggsave(\"MyPlot.png\", dpi = 300) \n\n# Save plot as jpeg with a resoultion of 72 dpi\nggsave(\"MyPlot.jpeg\", dpi = 72) \n\n# Save plot as pdf\nggsave(\"MyPlot.pdf\") \n\nThe plot is saved in the current working directory.\nSometimes it can be necessarry to play around with the scaling factor scale =. Try increasing it if the text looks too small.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.4\n\n\n\n\n\n\n\n\n\nExercise 4.4 - Analysis of coffee serving temperature - data inspection\n\n\n\n\n\n\nServing temperature of coffee seems of importance on how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a trained Panel of eight judges, evaluating coffee served at six different temperatures on a set of sensorical descriptors. Each judge is presented with each temperature in a total of four replicates leading to a total of \\(6 \\times 8 \\times 4 = 192\\) samples.\nIn the dataset Results Panel.xlsx the results are listed. Taking these data from A to Z involves descriptive analysis for understanding variation within judge, between judge and between different temperatures, further outlier detection, and finally determination of structure between sensorical descriptors. In this exercise we are only going to briefly explore the data with emphasis on uncertainty.\nThis is data from a trained panel, meaning each judge have been trained to be an objective instrument returning the same response when presented the same sample. However, there is always uncertainty in such responses, and especially when the instrument is a human being. We are interested in how big the deviation is between the four replicates, across judges and samples.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and check that it is matching the excel file using head().\nUse the summarise() or aggreggate() function to extract certain descriptive measures (e.g. mean or standard deviation) from the data.\nPlot this descriptive measure for a single descriptor across temperature (x-axis) and join the points from the same judge.\nWhat can you say about the individual judges? And is scoring more difficult for higher temperature than lower?\n\n\n\n\n\nThe code below does (some) of the job.\n\nClassic RTidyverse\n\n\n\n# Compute the mean of the replicates\ncoffee_ag &lt;- aggregate(coffee, \n                       by = list(coffee$Assessor, coffee$Sample), \n                       FUN = mean)\n\n# Rename some of the variables \ncoffee_ag &lt;- rename(coffee_ag, c(Judge = Group.1, Temp = Group.2))\n\n# Make an initial plot of the result\nggplot(coffee_ag, aes(Temp, Intensity, color = Temp)) +\n  geom_boxplot() +\n  geom_jitter()\n\n\n\n\ncoffee |&gt; \n  group_by(Assessor, Sample) |&gt;\n  summarise(across(\n    .cols = where(is.numeric), # Choose all numeric columns\n    .fns = mean # Compute mean\n  )) |&gt; \n  ggplot(aes(Temperatur, Intensity, color = Sample)) +\n  geom_boxplot() +\n  geom_jitter()",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html",
    "href": "chapters/week1/pca.html",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "5.1 Reading material",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#reading-material",
    "href": "chapters/week1/pca.html#reading-material",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "",
    "text": "Principle Component Analysis (PCA) - in short\nVideos on PCA\n\nPCA main ideas by StatQuest\nPCA Introduction 1 by Rasmus Bro\nPCA Introduction 2 by Rasmus Bro\n\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff\nChapter 4 (4.1 to 4.5) in Chemometrics With R: Multivariate Data Analysis in the Natural Sciences and Life Sciences by Ron Wehrens (2012). Springer, Heidelberg. Available in Absalon.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#sec-pca-intro",
    "href": "chapters/week1/pca.html#sec-pca-intro",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.2 Principle Component Analysis (PCA) - in short",
    "text": "5.2 Principle Component Analysis (PCA) - in short\nPrincipal Component Analysis (PCA) is a method for understanding multivariate data. By multivariate data we mean a set of samples/observations (\\(n\\)) which are characterized on a number of different features (\\(p\\)). For example:\n\nDifferent beers (\\(n = 20\\)) analyzed for \\(p = 60\\) chemical variables reflecting the aroma composition.\nSix coffee samples (\\(n = 6\\)) assessed on a set of sensorical descriptors (\\(p = 12\\)) by sensorical panel of eight judges (\\(n = 48\\)).\nA range of oil samples (\\(n = 40\\)) analyzed by near infrared spectroscopy (\\(p = 400\\)).\n\nThe data is often arranged in a data table (referred to as \\(\\mathbf{X}\\)) with \\(n\\) rows (samples) and \\(p\\) columns (variables). For almost all real life applications such multivariate data are correlated. That is; some of the variables carry the same type of information, and the interesting information in these data is captured by this so-called correlation structure. A nice visualization of the correlation is done via a scatter plot of two variables. For a few variables (say \\(p = 5\\)) it is possible to interpret all combinations of two variables. If \\(p = 5\\) that amounts to \\(\\frac{p(p-1)}{2} = 10\\) plots. However, when \\(p\\) is high this becomes in-practical. PCA deals with this issue by compressing the data into a set of components:\n\\[\n\\mathbf{X} = \\mathbf{t}_1 \\mathbf{p}_1^T + \\mathbf{t}_2 \\mathbf{p}_2^T + \\cdots + \\mathbf{t}_k \\mathbf{p}_k^T + \\mathbf{E}\n\\]\nNotation wise, \\(\\mathbf{X} \\sim (n, p)\\) is a matrix, \\(\\mathbf{t_i} \\sim (n,1)\\) and \\(\\mathbf{p_i} \\sim (p,1)\\) are vectors.\n\n\\(\\mathbf{t}_1\\) are scores for component 1 (\\(\\mathbf{t}_2\\) are scores for component 2 and so forth) and tells something about the multivariate sample distribution.\n\\(\\mathbf{p}_1\\) are loadings for component 1 (\\(\\mathbf{p}_2\\) are loadings for component 2 and so forth) and tells something about the multivariate correlation structure between the variables.\nA set of \\(\\mathbf{t}_1\\) and \\(\\mathbf{p}_1\\) is referred to as a component.\n\nThe power of PCA is when the mathematical decomposition into scores and loadings is combined with visualization of these. That is:\n\nScore plot - scatter plots of combination of scores (often \\(\\mathbf{t}_1\\) vs. \\(\\mathbf{t}_2\\)).\nLoading plot - scatter plots of combination of loadings (often \\(\\mathbf{p}_1\\) vs. \\(\\mathbf{p}_2\\)).\nBi plot - overlayed score- and loading plot.\n\n\n\n\n\n\n\nExample 5.1\n\n\n\n\n\n\n\n\n\nExample 5.1 - Natural phenolic antioxidants for meat preservation - PCA\n\n\n\n\n\n\nThis data originates from a study investigating the effect of natural phenolic antioxidants against lipid and protein oxidation during sausage production and storage. Bologna-type sausages were prepared and treated with either green tea (GT) or rosemary extract (RE) as antioxidants, and a control batch was also included. The three types of sausages were evaluated by a sensory panel including 8 assessors, on 18 different descriptors within the categories smell, color, taste and texture. The sausages were evaluated immediately after production (week0) and after four weeks of storage (week4).\nData is from: Jongberg, Sisse, et al. ”Effect of green tea or rosemary extract on protein oxidation in Bologna type sausages prepared from oxidatively stressed pork.” Meat Science 93.3 (2013): 538-546.\nStep 1 - Load libraries\nNote that these need to be installed beforehand.\n\nlibrary(ggplot2)\nlibrary(ggbiplot)\n\nStep 2 - Load the data\nRemember to set your working directory.\n\nload(\"meat_data.RData\")\n\nStep 3 - Calculate the PCA model\nThe data frame X consists of \\(p = 20\\) columns, however only the last 18 are response variables, whereas the first two refers to the study design. In PCA only the response variables are used to calculate the model, whereas the design is used for e.g. coloring of the score plot.\n\nPCA &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\nStep 4 - Plot the model\nThe ggbiplot() function nicely plots a so-called biplot. Below a lot of features are added to the plot, but ggbiplot(PCA) will produce something which is similar.\n\nggbiplot(PCA, groups = X$Treatment, \n         obs.scale = 1, var.scale = 1, \n         ellipse = T, ellipse.fill = F) + # Create the biplot\n  ggtitle(\"PCA on sensory data, colored according to treatment\") + # Add title\n  ylim(-4, 6) + # Set y-axis limits\n  xlim(-7, 6) + # Set x-axis limts\n  theme_bw() + # Set theme as \"black and white\"\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\")) # Adjust title \n\n\n\n\n\n\n\nFigure 5.1: A biplot on PC1 and PC2 based on the meat preservation sensory data. Colored according to treatment. GT = Green tea; RE = rosemary extract.\n\n\n\n\n\nIn figure 5.1 above you see a biplot of the PCA model on the sensory data on the sausages from both weeks. The points represent the scores for all samples, colored according to antioxidant treatment.The ellipse = T in the R-code draws ellipses representing the distribution of each treatment. The arrows indicate the loadings of the sensory variables. The scores can be used to evaluate the differences between samples, and the underlying reason behind the differences is interpreted through the directionality and magnitude of the loadings. It is evident that there is a difference between the control and treated samples, since they are grouped differently, primarily in the PC1 direction. When compared to the loadings it seems that the control samples are characterised by more old, and rancid tastes and smells, which are sensorical attributes of lipid oxidation. Furhter the control samples have a boiled egg texture compared to the treated samples. On the other hand the treated samples are more spicy and bitter/acidic in taste and grey in color.\nThere are not registered any greater difference between the two green tea and rosemary extract samples.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2\n\n\n\n\n\n\n\n\n\nExample 5.2 - Near infrared spectroscopy of marzipan - PCA\n\n\n\n\n\n\nThe following example illustrates how principal component analysis (PCA) can be used to explore your data. The dataset in this example consists of 32 measurements on marzipan bread (marzipanbrød) made from 9 different recipes. The measurements have been acquired using near infrared spectroscopy (NIR) where light is passed through a sample and the transmitted light analysed. The output measurement is a spectrum showing how much light the sample has absorbed at each wavelenght.\nStep 1 - Load packages and data\nWe start by loading the relevant libraries and importing the data.\n\nlibrary(ggplot2)\nload(\"Marzipan.RData\")\n\nStep 2 - Inspect raw spectra\nWe can now plot the spectra colored according to sugar content (figure 5.2)\n\nggplot(Xm, aes(wavelength, value, color = sugar)) +\n  geom_line() +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.2: 32 NIR spectra measured on 9 different marcipan breads colored according to sugar content.\n\n\n\n\n\nLooking at the raw spectral data we see that there is a concentration gradient in the spectra when we colour according to the sugar content. It seems that the main variation in the spectra has something to due with the sugar content.\nWe can also plot the same spectra colored according to recipe (figure 5.3).\n\n\nCode\n# Extract recipe from variable name\nXm$recipe &lt;- substr(Xm$variable, 1, 1)\n\nggplot(Xm, aes(wavelength, value, color = recipe)) +\n  geom_line() +\n  labs(title = \"Marzipan NIR spectrum\",\n       y = \"Absorbance\",\n       x = \"Wavelength (nm)\",\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 5.3: 32 NIR spectra measured on 9 different marcipan breads colored according to recipe.\n\n\n\n\n\nHere we see that we can distinguish some of the recipes from each other. This can be explained by the varying sugar content in the recipes. Also, if we look in the region below 1100 nm and into the visible (≈ 370 − 750 nm) we note that samples made with recipe c is different compared to the other samples.\nStep 3 - Calculate PCA model\nWe now make a PCA on the data and plot PC1 vs PC2 coloured according to sugar content.\n\n# Transposing the data and removing the wavelength column\nXt = t(X[,-1])\n\n# Making PCA on mean centered Xt\nmarzipan = prcomp(Xt, center = TRUE, scale = FALSE)\n\n# Extracting scores for plotting\nscores = data.frame(marzipan$x, sugar = Y$sugar)\n\n# Extracting % explained variance for plotting\nvarPC1 = round(summary(marzipan)$importance[2,1]*100)\nvarPC2 = round(summary(marzipan)$importance[2,2]*100)\n\nStep 4 - Plot the scores\nWe can now plot the scores colored according to sugar content (figure 5.4).\n\nggplot(scores, aes(PC1, PC2, color = sugar)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Sugar content (%)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.4: Score plot for NIR spectra of marzipan bread colored by sugar content.\n\n\n\n\n\nWe see that PC1 explains 61% of the variation in the data and that it seems to capture the variation in the sugar content. The samples are ordered from left to right in increasing concentration. Also, a group of samples are laying away from the rest when looking at PC2 which is explaining 33% of the variation in the data.\nWe can also color the scores according to recipe (figure 5.5).\n\n# Extract recipe from sample name\nscores$recipe &lt;- substr(Y$sample, 1, 1)\n\nggplot(scores, aes(PC1, PC2, color = recipe)) +\n  geom_point(size = 3) +\n  labs(x = paste(\"PC1 - \",varPC1, \"%\", sep = \"\"), # Insert exp.var. as label\n       y = paste(\"PC1 - \",varPC2, \"%\", sep = \"\"),\n       color = \"Recipe\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.5: Score plot for PCA based on the NIR spectra of marzipan bread colored by recipe.\n\n\n\n\n\nIf we look at PC2 we see that it is the samples from recipe c that is laying away from the other samples. What is the reason for that? Let us look at the loadings. We start by looking at the second loading (figure 5.6) as it is dividing the samples from recipe c from the other samples.\n\n# Extract loadings from PCA model\nloadings = as.data.frame(marzipan$rotation)\n\nggplot(loadings, aes(wl, PC2)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"2nd loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.6: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nThe main contribution to PC2 is the peak around 550 nm. So the reason why the samples from recipe c is different from the other is related to colour. This actually makes sense as this recipe has cocoa powder added to the recipe which will influence the colour of the marzipan bread.\nLastly, we look at the first loading (figure 5.7).\n\nggplot(loadings, aes(wl, PC1)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Wavelength (nm)\",\n       y = \"1st loading\"\n       ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5.7: Loading plot for PCA based on the NIR spectra of marzipan bread.\n\n\n\n\n\nIt is not straight forward to see which peaks are related to sugar. However, the peaks around 1200, 1400, 1875 and 2100 nm has the highest magnitude and therefore the main reason for the sugar content gradient we see in PC1. Actually all 4 peaks are related to either the C-H (Carbon - Hydrogen) or O-H groups in sugar or O-H in water. You will learn more about assigning peaks to chemical information in other courses later on.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#videos",
    "href": "chapters/week1/pca.html#videos",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.3 Videos",
    "text": "5.3 Videos\n\n5.3.1 PCA main ideas - StatQuest\n\n\n\n5.3.2 PCA Introduction 1 - Rasmus Bro\n\n\n\n5.3.3 PCA Introduction 2 - Rasmus Bro",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week1/pca.html#exercises",
    "href": "chapters/week1/pca.html#exercises",
    "title": "5  Principal Component Analysis (PCA)",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n\n\n\n\n\nExercise 5.1\n\n\n\n\n\n\n\n\n\nExercise 5.1 - McDonalds Data\n\n\n\n\n\n\nThe purpose of this exercise is to get familiar with PCA on a small intuitive dataset. The data - McDonaldsScaled.xlsx - constitutes of different fast food products and their nutritional content.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data using an appropriate package / function (e.g. r read_excel() from the package readxl), and set up the data with row names etc.\n\n\n\n\n\n\nMcD &lt;- read_xlsx(\"McDonaldsScaled.xlsx\")\n\n# The first column has no name - change that\ncolnames(McD)[1] &lt;- \"Item\"\n\nhead(McD)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake some initial descriptive plots for the five response variables, that indicate the distribution (center and spread).\nMake some bi-variate scatter plots examining the relation between different variables, and comment on whether this relation is obvious and further, which types of samples are responsible for the relation. You can use the ggplot2 with stat_smooth() for this. Set method = \"lm\" to choose a linear model and se = F to disable the confidence intervals around the line.\n\n\n\n\n\n\nggplot(McD, aes(Energy, Protein)) +\n  stat_smooth(method = \"lm\", se = F) + # Plot a linear model\n  geom_point(size = 3) +\n  geom_text(aes(label = Item)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow, make a PCA on the data. What does the two options center = ... and scale. = ... refer to?\n\n\n\n\n\n\nMcD_pca &lt;- prcomp(McD[2:6], center = TRUE, scale. = TRUE)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nIf \\(X_1, X_2, .., X_{19}\\) is the protein variable in the dataset (i.e. \\(X_1\\) is the protein content of sample 1 and so forth), how do you calculate the centered and scaled representation of data, which could be called: \\(X_1^{auto}, X_2^{auto}, .., X_{19}^{auto}\\)?\nPlot the PCA results and comment on them. There are several ways of doing this. The first described here, is to zack out the parameters (scores and loadings) and then use the ggplot2 functionality to plot those. Try to comprehend what is actually produced from the list of functions listed below.\n\n\n\n\n\n\n# Zack out the individual parameters (scores and loadings and item names)\nscores &lt;- data.frame(McD_pca$x)\nloadings &lt;- data.frame(McD_pca$rotation)\nItems &lt;- McD$Item\n\n# Score plot\nggplot(scores, aes(PC1, PC2, label = Items)) +\n  geom_text()\n\n# Loading plot\nggplot(loadings, aes(PC1, PC2, label = row.names(loadings))) +\n  geom_text()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAlternatively you can utilize a package ggbiplot for making nice plots from PCA objects (see #exa-phenolic-antiox-pca) for more info on how to use this package).\n\n\n\n\n\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(label = Items) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a vector that indicates the different fast food types (Burger, Drinks, etc.). You can either do this by extending the excel file with a column, or do it in R (see this below). Infer this class information on the plot as color (or maker shape or size).\n\n\n\n\n\n\nCategories &lt;- c(\n  rep(\"Burger\", 9),\n  rep(\"Drink\", 3),\n  rep(\"Icecream\", 3),\n  rep(\"Other\", 2),\n  rep(\"Salad\", 2)\n)\n\nggbiplot(McD_pca, obs.scale = 1, var.scale = 1) +\n  geom_text(aes(label = Items, color = Categories)) +\n  theme_bw()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTry to modify the PCA by removing scaling and/or centering. What happens to the plots of the results? What do you think is going on?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.2\n\n\n\n\n\n\n\n\n\nExercise 5.2 - Analysis of coffee serving temperatura - PCA\n\n\n\n\n\n\nIn the dataset Results Panel.xlsx the sensorical results from a panel of eight judges, evaluating coffee served at six different temperatures each four times are listed. In this exercise, we are going to first average over judge and temperature followed by PCA to evaluate sensorical descriptor similarity as well as the effect of serving temperature on the perception of coffee.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAfter import of data, and initial sanity check, calculate the average response across the four replicates. Use the summarise() or aggreggate() function with mean functions to make a dataset with the average response for the six different temperatures for each judge. For an example on how to do this see Section A.4.\n\nHint: The number of samples should be reduced by a factor of 4.\n\nUse this data as input for construction of a PCA model. Which variables do you think should be included?\nMake a biplot of this PCA model and interpret it.\n\nWhich descriptors go together and which are oppositely correlated?\nAre there, from this analysis, a clear difference between the different serving temperatures?\nWhat do you think blurs the picture?\n\n\n\n\n\n\nThe code below can be used for inspiration. Be aware, that we need to set a series of dependencies in order for this to work.\n\nUsing summarise()Using aggregate()\n\n\n\n# Calculate mean per group\ncoffee_ag &lt;- coffee |&gt;\n  summarise(\n    across(where(is.numeric), mean), # Compute mean over numeric variables \n    .by = c(Assessor, Sample) # Group by judge and temp\n  )\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n# Define variables to group by\ngroup_by &lt;- list(\n  \"Judge\" = coffee$Assessor, \n  \"Temp\" = coffee$Sample\n  )\n\n# Calculate mean per group\ncoffee_ag &lt;- aggregate(coffee, group_by, mean)\n\n\n# Compute PCA model\ncoffee_pca &lt;- prcomp(coffee_ag[your_variables_here])\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nCheck out the ggbiplot() syntax (by ?ggbiplot). by adding stuff to the plot, it is modified to look exactly like you want it. Here we change legend appearance (for inferring temperature) and color scheme for the scores matching temperature. In order to also remove variation due to differences between judges, the dataset is compressed such that the rows reflect the average for each temperature (across judges and replicates). Then this dataset is used for constructing a PCA model and visualized.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nuse the aggreggate() function to average across judges and replicates.\n\nHint: Modify the .by = argument in summarise() or the group_by-list in aggregate().\n\nMake a PCA model on this dataset. Visualise and interpret it.\nUse the code below to vizualize the model again - Can you figure out why this model on a data-matrix of 6 samples is different from the previous model (on a data-matrix of 48 samples)?\n\n\n\n\n\n\n# Plot PCA model\nggbiplot(coffee_pca, groups = coffee_ag$Temp,\n         ellipse = T, circle = T, ellipse.fill = F) +\n  scale_color_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    name = \"Temperature\"\n    ) +\n  theme(legend.position = \"bottom\")\n\nThe plots are hard to interpret because\n\nsome of the labels are masked and\nthe points are way to small.\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the function xlim(c(low,high)) and geom_point(, aes(color = cofffee_ag$Temp) size = 5) and add them to the plot to fix these problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.3\n\n\n\n\n\n\n\n\n\nExercise 5.3 - Wine aromas\n\n\n\n\n\n\nThis exercise will take you through plotting, descriptive stats and PCA. Wine based on the same grape variety (Cabernet Sauvignon) from four different countries (Argentina, Australia, Chile and South Africa) were analyzed for aroma compound composition with GC-MS (gas chromatography coupled with mass spectrometry). The dataset can be found in the file “Wine.xlsx”, and it will form the basis for working with basic descriptive statistics, plots and PCA.\n\nDescriptive statistics\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart by importing the dataset “Wine.xlsx” to R and try to get an overview of it.\n\nHint: use the summary() function in R and/or have a look at the raw data in the Excel file.\n\n\nHow many wines were analyzed from each country?\nHow many variables are there in the dataset, and how many constitutes the aroma profile?\n\n\n\n\n\n\nFor the descriptive statistics, only two of the aroma compounds are selected. Choose two on your own or make the calculations for the aroma compounds benzaldehyde (almond like aroma) and 3-Methylbutyl acetate (sweet fruit/banana like aroma).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean, variance, standard deviation, median and inner quartile range for the selected aroma compounds from each of the four different countries.\n\nHint: it can be helpful to create a separate dataset for each country, which can be done with the function subset().\n\n\n\n\n\n\n\n\nPlots\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a boxplot, a jitterplot and a combination of the two with all 4 countries in one plot. Use the R-commands from the notes as inspiration.\nWhat do you see? Discuss pros and cons of the different plots.\nAdjust the layout of your favorite plots (e.g. color, background, title etc.). Think about how the data is presented in the best way. Actually, it can be rather beneficial to specify a generic theme including title and label font size, background color of the plot etc, which then can be added to each plot produced.\n\n\n\n\n\n\n\nPCA\nWorking with a dataset with many variables, PCA provides a very nice tool to give an overview of the dataset. First we define the data we want to include in the analysis. With logical indexing wine[4:20] we choose the variables to include (i.e. we are only interested in analyzing the aroma compounds).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse the `prcomp function to calculate a PCA model on scaled aroma data. What does the arguments scale. = T and center = T do the to the data?\n\nTry changing them to = F (false).\n\nMake a score plot and a loading plot.\n\n\n\n\n\n\n# Compute PCA model\nwine_pca &lt;- prcomp(wine[4:58], scale. = T, center = T)\n\n# Create score plot (biplot without arrows)\nggbiplot(wine_pca, groups = wine$Country, point.size = 3,\n         var.axes = F)\n\n# Extract loadings and loading names\nwine_loadings &lt;- data.frame(wine_pca$rotation)\nloading_names &lt;- rownames(wine_loadings)\n\n# Create \"manual\" loading plot\nggplot(wine_loadings, aes(PC1, PC2)) +\n  geom_text(aes(label = loading_names,\n                color = loading_names),\n            show.legend = F) +\n  theme_bw()\n\n# Create biplot\nggbiplot(wine_pca, groups = factor(wine$class), point.size = 3)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat do you see in the score and the loadings plot?\nCan you see a grouping of the data? If so, how are the groups different?",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principal Component Analysis (PCA)</span>"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html",
    "href": "chapters/week2/week_2.html",
    "title": "Week 2",
    "section": "",
    "text": "Hand-in assigment\nThis week is going to focus on three very central subjects, namely correlation, the normal distribution and in relation to this the Central Limit Theorem.\nExercise 6.1 Correlation between aroma compounds questions 1 – 6 (not the PCA part) is to be handed in (through Absalon or as hard-copy Wednesday night).",
    "crumbs": [
      "Week 2"
    ]
  },
  {
    "objectID": "chapters/week2/week_2.html#exercises",
    "href": "chapters/week2/week_2.html#exercises",
    "title": "Week 2",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 6.2\nExercise 6.3\n\nFor Wednesday work through the following exercises:\n\nExercise 6.4\nExercise 7.1\nExercise 7.3",
    "crumbs": [
      "Week 2"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html",
    "href": "chapters/week2/correlation.html",
    "title": "6  Correlation",
    "section": "",
    "text": "6.1 Reading material",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#reading-material",
    "href": "chapters/week2/correlation.html#reading-material",
    "title": "6  Correlation",
    "section": "",
    "text": "Correlation and covariance - in short\nVideos on correlation and covariance\n\nCovariance, clearly explained\nPearson’s correlation, clearly explained\nCorrelation Doesn’t Equal Causation: Crash Course Statistics #8\nWhat is correlation?\n\nChapter 1.4.3 and 2.5 of Introduction to Statistics by Brockhoff\nChapter 2 in Biological Data analysis and Chemometrics by Brockhoff",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#sec-correlation-intro",
    "href": "chapters/week2/correlation.html#sec-correlation-intro",
    "title": "6  Correlation",
    "section": "6.2 Correlation and covariance - in short",
    "text": "6.2 Correlation and covariance - in short\nA covariance or correlation is a scalar measure for the association between two (response-) variables. Covariance bewteen two variables \\(X = (x_1, x_2, ..., x_n)\\) and \\(Y = (y_1, y_2, ..., y_n)\\) is defined as:\n\\[\n\\text{cov}_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]\nCovariance depends on the scale of data (\\(X\\) and \\(Y\\)), and as such is hard to interpret. The correlation is however a scale in-variant version\n\\[\n\\text{corr}_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y},\n\\]\nwhere \\(s_X\\) and \\(s_Y\\) are the standard deviation for \\(X\\) and \\(Y\\) respectively (see 2.4.1 for details).\nDividing the covariance by the individual standard deviations put the correlation coefficient in the range between −1 and 1\n\\[\n-1 \\leq \\text{corr}_{XY} \\leq 1.\n\\]\nA correlation (and covariance) close to zero indicates that there are no association between the two variables (see figure 6.1).\n\n\nCode\nset.seed(4321)\n\nx &lt;- 1:30\nn &lt;- length(x)\n\ndata.frame(\n  \"x\" = x,\n  \"y1\" = x,\n  \"y2\" = x + rnorm(n, sd = 6),\n  \"y3\" = x + rnorm(n, sd = 10),\n  \"y4\" = rep(11, n) + rnorm(n, sd = 10),\n  \"y5\" = -x + rnorm(n, sd = 3),\n  \"y6\" = -x + rnorm(n, sd = 20)\n) |&gt; \n  pivot_longer(cols = !x) |&gt; \n  ggplot(aes(x, value)) +\n  geom_smooth(method = \"lm\", se = F, \n              linewidth = .6, color = \"black\") +\n  geom_point(size = 2, color = \"steelblue\", alpha = .5) +\n  stat_cor(\n    aes(label = after_stat(r.label)),\n    geom = \"label\",\n  ) +\n  labs(y = \"y\") +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme_bw() +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 6.1: Correlation \\(R\\) between one variable x and different y variables.\n\n\n\n\n\n\n\n\n\n\n\nExample 6.1\n\n\n\n\n\n\n\n\n\nExample 6.1 - Natural phenolic antioxidants for meat preservation - Correlation\n\n\n\n\n\n\nThis example continues where we left of in Example 5.1 with the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control.\nLoad the data and libraries\nRemember to set your working directory.\n\nlibrary(ggplot2)\nlibrary(gridExtra) # For showing multiple plots in a grid\nload(\"meat_data.RData\")\n\n\n\nCode\npca &lt;- prcomp(X[,3:20], center = TRUE, scale. = TRUE)\n\n# Define loadings to keep\nshow_loadings &lt;- c(\"Color_Pink\", \"Smell_Old\", \"Taste_Old\", \n                  \"Taste_Bitter\", \"Color_Grey\")\n\n# Remove loadings\npca$rotation[!rownames(pca$rotation) %in% show_loadings, ] &lt;- 0\n\nggbiplot(pca,\n         select = rownames(pca$rotation) %in% show_loadings,\n         alpha = 0, \n         varname.size = 4) +\n  labs(title = \"Loading plot\") +\n  theme_bw() +\n    theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n\n\n\n\n\n\n\nFigure 6.2: Select loadings from the PCA model computed in the previous example.\n\n\n\n\n\nIn figure 6.2 the loading plot corresponding to some of the loadings found in the PCA model calculated in the previous example is shown. The selected variables show positive, negative and non-correlated loadings. In a PCA loading plot, loadings with opposite directions are negatively correlated, while loadings pointing in the same direction are possitively correlated. Loadings which are orthogonal (90 degree angle) with respect to each other, are not correlated. For the plot above, this means that:\n\nTaste_Old and Smell_Old are positively correlated.\nTaste_Bitter and Color_Pink are uncorrelated.\nColor_Pink and Color_Grey are negatively correlated.\n\nTo check if the correlation holds for the raw data, scatter plots are made for each of the encircled three examples. (Note that the interpretation of these correlations are only valid with respect to the variation described in the model)\nCreate scatter plots\n\n# Define a common theme\nmy_theme &lt;- theme_bw() +\n  theme(plot.title = element_text(hjust = .5, face = \"bold\"))\n\n# Plot 1\nplt1 &lt;- ggplot(X, aes(Smell_Old, Taste_Old)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Positive correlation\") +\n  my_theme\n\n# Plot 2\nplt2 &lt;- ggplot(X, aes(Taste_Bitter, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"No correlation\") +\n  my_theme\n\n# Plot 3\nplt3 &lt;- ggplot(X, aes(Color_Grey, Color_Pink)) +\n  geom_point(size = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Negative correlation\") +\n  my_theme\n\n# Arrange the plots in a 1 x 3 grid (requires gridExtra package)\ngrid.arrange(plt1, plt2, plt3, ncol = 3)\n\n\n\n\n\n\n\nFigure 6.3: The between three pairs of select variables from the meat data.\n\n\n\n\n\nThe correlation values are calculated using the cor() function.\n\n# Calculate correlation\ncor1 &lt;- cor(X$Smell_Old, X$Taste_Old)\ncor2 &lt;- cor(X$Taste_Bitter, X$Color_Pink)\ncor3 &lt;- cor(X$Color_Grey, X$Color_Pink)\n\n# Display in a fancy way\ncat(\"Correlation coefficients\",\n    \"\\n Smell_Old vs Taste_Old: \\t\\t\", cor1,\n    \"\\n Taste_Bitter vs Color_Pink: \\t\", cor2,\n    \"\\n Color_Grey vs Color_Pink: \\t\\t\", cor3)\n\nCorrelation coefficients \n Smell_Old vs Taste_Old:         0.595475 \n Taste_Bitter vs Color_Pink:     0.007446373 \n Color_Grey vs Color_Pink:       -0.604673\n\n\nConclusions\nHere, it can be seen that the pink and grey color attributes, which are oppositely directed in the PCA loadings, display a moderate negative correlation in the raw data. The pink color and bitter taste, which are orthogonal to each other in the loadings, are not correlated at all. The old smell and old taste with similar directionality in the PCA are moderately postively correlated. With regards to food chemistry, do you think these conclusions make sense?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#videos",
    "href": "chapters/week2/correlation.html#videos",
    "title": "6  Correlation",
    "section": "6.3 Videos",
    "text": "6.3 Videos\n\n6.3.1 Covariance, clearly explained - StatQuest\n\n\n\n6.3.2 Pearson’s correlation, clearly explained - StatQuest\n\n\n\n6.3.3 Correlation doesn’t equal causation - CrashCourse\n\n\n\n6.3.4 What is correlation - Melanie Maggard",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/correlation.html#exercises",
    "href": "chapters/week2/correlation.html#exercises",
    "title": "6  Correlation",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n\n\n\n\n\n\n\nExercise 6.1\n\n\n\n\n\n\n\n\n\nExercise 6.1 - Correlation between aroma compounds\n\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat does a correlation coefficient of -1, 0 or +1 tell you?\nIn the table below (table 6.1) the amount of Nonanal and Ethyl.2.methyl.propanoate in the six Argentine wines are listed. Fill out the blank spaces in the table and calculate the correlation coefficient (r).\n\nHelp can be found in example 1.19 in Introduction to Statistics by Brockhoff.\n\n\n\n\n\n\n\n\n\nTable 6.1: Noanal and Ethyl-2-methyl-propaonate in six Argentine wines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine number (\\(i\\))\n1\n2\n3\n4\n5\n6\n\n\n\n\nNonanal (\\(X_i\\))\n0.003\n0.003\n0.005\n0.006\n0.008\n0.005\n\n\nEthyl.2.methyl.propanoate (\\(Y_i\\))\n0.106\n0.165\n0.150\n0.155\n0.149\n0.141\n\n\n\\(X_i - \\bar{X}\\)\n\n\n\n\n\n\n\n\n\\(Y_i - \\bar{Y}\\)\n\n\n\n\n\n\n\n\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that the sample covariance and correlation are given as\n\\[\n\\text{cov}_{XY} = s_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1},\n\\]\n\\[\n\\text{corr}_{XY} = r_{XY} = \\frac{\\text{cov}_{XY}}{s_X \\cdot s_Y}.\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a plot with Nonanal vs. Ethyl.2.methyl.propanoate and calculate the correlation coefficient in R (use the function cor() in R). Include only wines from Argentina.\nCalculate the correlation coefficient for a, b, c and d, where you include wine data from all the countries.\n\nDiethyl.succinate and Ethyl.lactate\nEthyl.acetate and Ethanol\n2.Butanol and Ethyl.hexanoate\nBenzaldehyde and Hexyl.acetate\n\nWhat happens with the covariance and correlation if we multiply the Diethyl.succinate amount with 10 or −4?\nWhat happens with the covariance and correlation if we add 10 or 100 to the Diethyl.succinate amount?\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a PCA including wines from all the countries (similar to the one from week 1 on the same dataset).\n\nWhich variables are responsible for the grouping of the countries?\n\nCompare the calculated correlation coefficients in question 4 with the loading plot of the PCA in question 5.\n\nHow does the position of the variables in the loading plot make the correlation coefficients negative, positive, close to \\(\\pm\\) 1 or 0?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2\n\n\n\n\n\n\n\n\n\nExercise 6.2 - Covariance and Correlation - by hand\n\n\n\n\n\n\nCertain types of characteristics ”go together”, for instance will a variable that measures the chocolate smell of some food type be related to a variable measuring the chocolate taste of the same food type. We talk about these two types of information being correlated. Below (table 6.2) is 48 corresponding measures of the sensorical attributes sour and roasted for coffees at different serving temperatures and different judges (each based on the average of four measurements). Calculate the correlation between these two variables based on the metrics listed below.\n\n\n\nTable 6.2: Ratings of the sensorical attributes sour and roasted for coffees at different serving temperatures and different judges (each based on the average of four measurements).\n\n\n\n\n\n\nObservation\nSour\nRoasted\n\n\n\n\n1\n1.70\n1.06\n\n\n2\n1.93\n1.10\n\n\n3\n3.59\n3.87\n\n\n4\n2.30\n1.86\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n45\n1.30\n1.89\n\n\n46\n0.26\n1.93\n\n\n47\n1.12\n1.05\n\n\n48\n3.84\n5.41\n\n\n\\(\\sum\\)\n91.59\n76.42\n\n\n\\(\\hat{\\sigma}^2\\)\n0.90\n0.94\n\n\n\\(\\sum_{XY}\\)\n\n172.73\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate mean and standard deviation for the two variables using the statistics listed below data. That is WITHOUT importing data into the computer!\nThe covariance between the variables is 0.57. Calculate the correlation between the two variables.\nWhat happens with the covariance and correlation if we multiply the Sour ratings with 2 or -3?\nWhat happens with the covariance and correlation if we add 10 or -1234 to the Roasted ratings?\nAs an upcoming coffee expert, why do you think these two attributes are correlated?\nHARD! Calculate \\(\\sum (X - \\bar{X})(Y - \\bar{Y})\\) from \\(\\sum XY\\), \\(\\sum X\\) and \\(\\sum Y\\) (where X is Sour and Y is Roasted).\n\nHint: You need to multiply out the product of the two parenthesis and reduce the resulting part using the relation between \\(\\bar{X} = \\sum(X) / n\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.3 - Correlation and PCA\n\n\n\n\n\n\nIn this exercise the sensorical data from ranking of coffee served at different temperatures are used. The aim is to see how correlations is the vehicle for PCA analysis. The data is named “Results Panel.xlsx”.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data and remove the replicate effect by averaging over these.\nMake a scatter plot of the attribute Sour versus Roasted. Comment on what you see in terms of relation between these two variables.\nNow make a comprehensive figure where all the sensorical attributes (there are 8) are plotted against each other.\nCalculate all the pairvise correlations between the variables. How does this correspond with the figure?\nMake a PCA model on this dataset (same as in Exercise 4.4).\nComment on the (dis-)similarity between the correlation matrix, the multiple pairwise scatterplot and the PCA model.\n\n\n\n\n\nBelow is some code which might be useful for this purpose (You need to install or add dependencies via install.packages(GGally) or library(GGally) in order for the functions to be recognized by R). In the pairwise scatterplot a straight line is added by + geom_abline(), try also to add a smooth curve by + geom_smooth(). What are the difference between those two representation of similarity between the variables?\n\nggplot(coffee_ag, aes(Roasted, Sour)) +\n  geom_abline(color = \"red\", linewidth = 1) +\n  geom_point(size = 2.5, color = \"steelblue\", alpha = .75) +\n  theme_bw()\n\nggpairs(coffee_ag, columns = 4:11)\n\n\n\n\n\n\n\nExercise 6.3\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.4 - Olive oil adulteration\n\n\n\n\n\n\nQuick detection of adulteration of oils is of growing importance, since high quality oils, such as olive oil, are becoming increasingly popular and expensive, increasing the incentive for adulterating with cheaper oils. Spectroscopic techniques are the preferred measurement choice because they are quick, often non-destructive and in many cases highly selective for oil characterization.\nThe purpose of this exercise is to introduce you to how multivariate techniques, such as PCA, can be applied on spectral datasets. They are in fact, very useful on datasets such as these, because of the very high number of variables that can be included in the modelling.\nThe samples in this dataset are mixtures of olive oil and thistle oil. Olive oil and thistle oil are almost exclusively made up of triglycerides, consisting of a glycerol backbone with three fatty acid chains attached. An example of a triglyceride is shown in the top right of figure 6.4 (a). Fatty acids are characterized by the amount of unsaturation. Olive oil consists mostly of monounsaturated fatty acids, while thistle oil is largely comprised of polyunsaturated fatty acids (ie: they have more double bonds). Additionally a few of the samples were spiked with a free trans fatty acid. The structure of the added trans fatty acid is shown in the top left of figure 6.4 (a).\nThe samples were measured with infrared (IR) spectroscopy. Some of the relevant peaks for oil characterization are shown in the raw spectra in figure 6.4 (a). The dataset consists of 30 oil samples and 1794 variables. The first 1790 variables are the absorbance at 1790 wavelengths. The last 4 columns in the dataset describe the concentration of olive oil, thistle oil and transfat, and lastly the sample ID.\nSince it is a bit more difficult to work with spectral data in R, we have decided to be merciful and give you the data directly as an .RData file, which you simply get into R by load(\"OliveOilAdult.RData\"). Be sure to be in the correct working directory, or add the path to the load command.\n\n\n\n\n\n\n(a) Top right) Triglyceride. Left part: Glycerol backbone. Right part from top to bottom: Unsaturated, monounsaturated and polyunsaturated fatty acid. Top left) Trans fatty acid. Bottom) Raw IR spectrum of mixed oil.\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nLoad the “oliveoil” dataset in R. To be able to plot the raw spectral data, run the following R code. Use the dim() command to understand the difference between input and output to melt() on how the data is organised in the two formats.\n\n\n\n\n\n\nlibrary(reshape2)\n\n# Melt before plotting\nfor_raw &lt;- melt(Oliveoil, \n                id.vars = c(\"sample_id\", \"oliveoil\", \"thistleoil\", \"transfat\"))\n\n# Define correct x-axis with wavelength vector\nfor_raw$wl &lt;- sort(rep(wl, nrow(Oliveoil)))\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow make a plot of the raw data. Use the R-code below as a base, and customize it to your liking. Use the handy plotly package to plot an interactive plot that we can zoom in.\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\nraw &lt;- ggplot(for_raw, aes(wl, value, group = sample_id)) +\n  geom_line()\n\nggplotly(raw)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nInspect the raw data plot.\n\nHow does the raw data look? Are there any outliers? in ggplotly you can identify specific samples by hovering the curser over them.\nTry to color according to the different types of oil, one at a time. Then, zoom in on some of the peaks highlighted in figure 6.4 (a). Can you observe a correlation between these peaks and the oil concentration?\n\nNow that you have a feel for the raw data, try to estimate the scores and loadings for a PCA model. Why do we only center, and not autoscale the spectral data?\n\n\n\n\n\n\n# Define PCA\noliveoil_pca &lt;- prcomp(Oliveoil[1:1790], center = T, scale = F)\n\nscores &lt;- data.frame(oliveoil_pca$x)\nloads &lt;- data.frame(oliveoil_pca$rotation)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUsing ggplot() and ggplotly(), make a plot of the PC1 vs PC2 scores, and color according to oliveoil content.\nMake two separate loading plots with first PC1 loads vs the wavelength (wl) vector and then PC2 loads vs. the wavelength. It is very important that the loadings are plotted separately, and not against each other when working with spectral data. Can you see why?\nInspect the scoreplot. Are there any outlying samples? If so, identify them. Did you notice this outlier in the raw spectra plot? Remove the outlier by using the code below and inserting the correct sample id number.\n\n\n\n\n\n\n# Remove outlier\n\nOliveoil &lt;- Oliveoil[-c(\" insert outlier ID number here\"), ]\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nNow rerun your whole script and see how the removal of the outlier has changed the score and loading plots.\nUsing the score plot, try to colour them by different oil types and elucidate what information about the samples is being described by the first and second PCA component, respectively.\nInspect the loading plots and compare them to the information in figure 6.4 (a). Can you re-find some of the important peaks in the loadings? And how does this relate to the findings in the scoreplot?.\n\nHint: Remember that the dataset is mean centred, so the peaks will be positive or negative. Negative scores have a high absorbance in the negative loading ”peaks”, and positive scores have a high absorbance in the positive loading ”peaks”.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.4",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html",
    "href": "chapters/week2/normal_distribution.html",
    "title": "7  The normal distribution",
    "section": "",
    "text": "7.1 Reading material",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#reading-material",
    "href": "chapters/week2/normal_distribution.html#reading-material",
    "title": "7  The normal distribution",
    "section": "",
    "text": "The normal distribution - in short\nVideos giving short condensed introduction to the normal distribution:\n\nThe Normal Distribution\nAn Introduction to the Normal Distribution\nThe Normal Distribution, Clearly Explained\n\nVideo on confidence intervals:\n\nConfidence intervals\n\nChapter 2 and 3 of Introduction to Statistics by Brockhoff\n\nEspecially 2.1 to 2.4 (you can skip 2.2 on discrete distributions, which is the subject of week 4) and 3.1 - 3.1.6.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#sec-intro-to-normal",
    "href": "chapters/week2/normal_distribution.html#sec-intro-to-normal",
    "title": "7  The normal distribution",
    "section": "7.2 The normal distribution - in short",
    "text": "7.2 The normal distribution - in short\nIf \\(X\\) is normaly distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the probability density function for a given draw from that distribution \\(x\\) is given by\n\\[\nP(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^{{{ - \\left( {x - \\mu } \\right)^2 } {2\\sigma ^2 }}}\n\\]\nFurther, the cumulative density function can be found by integration of this formula \\[\nP(X \\leq x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }} \\int_{-\\inf}^x e^{{{ - \\left( {x - \\mu } \\right)^2 } {2\\sigma ^2 }}} \\, dx\n\\]\nThe short notation for this distribution is\n\\[\nX \\sim \\mathcal{N}(\\mu,\\sigma^2)\n\\]\n\n7.2.1 Estimation of \\(\\mu\\) and \\(\\sigma\\)\nMore often than not, the distribution is unknown. That is the values of \\(\\mu\\) and \\(\\sigma\\) are unknown and must then be estimated from data.\n\\(\\mu\\) characterizes the center of the distribution, and is naturally estimated by the mean-value of the data-points (\\(\\bar{X}\\)). \\(\\sigma\\) reflects the spread around the mean, and is in a similar fashion estimated by the standard deviation (\\(\\hat{\\sigma}\\) or \\(s\\)).\n\\[\n\\hat{\\mu} = \\bar{X} = \\sum_{i = 1}^n X_i / n = (X_1 + X_2 + X_3 + \\ldots + X_n) / n\n\\]\n\\[\n\\hat{\\sigma}^2 = s^2 = \\sum_{i = 1}^n (X_i - \\bar{X})^2 / (n-1) = ((X_1 - \\bar{X})^2 + (X_2 - \\bar{X})^2 + (X_3 - \\bar{X})^2 + \\ldots + (X_n - \\bar{X})^2 ) / (n-1)\n\\]\n\n\n\n\n\n\nExample 7.1\n\n\n\n\n\n\n\n\n\nExample 7.1 - Effect of caffeine on activity\n\n\n\n\n\n\nCaffeine is a central neurvous system stimulant, which can have several positive- and negative effects. In this study, the level of activity is up for examination, and for this purpose a model-system; the runing activity of mice in a wheel reflected as the number of rounds pr minutes recorded over 7 minutes. Here a total of 249 mice from two species; one breed to be high in running performance, and one control, where given either Water, Gatorade or Red Bull followed by measuring their voluentary wheel run activiy. Of interest is the average actvity within each mouse type and for each caffeine type and how large the spread is.\nThe data is listed in a table with 249 rows (here, the first five samples are shown):\n\nhead(X)\n\n       MouseType gender Caffeine    RPM7\n1 Control runner   Male Red Bull 10.5461\n2    High runner Female Gatorade 24.2718\n3    High runner Female Gatorade 19.7215\n4 Control runner   Male Red Bull 13.5099\n5 Control runner   Male Red Bull 15.3210\n6    High runner   Male Gatorade 30.7953\n\n\nTo get the experimental design, the table() function nicely summarize the numbers within each group:\n\ntable(X$gender:X$MouseType, X$Caffeine)\n\n                       \n                        Water Gatorade Red Bull\n  Male:Control runner      10       14       12\n  Male:High runner         29       31       29\n  Female:Control runner    11       13        9\n  Female:High runner       28       32       31\n\n\nBefore doing anything else, the data is plotted to visualize the response as function of the design (figure 7.1).\n\nggplot(X, aes(Caffeine, RPM7, color = Caffeine)) +\n  geom_boxplot() +\n  geom_jitter(alpha = .75) + # Alpha controls opacity\n  facet_wrap(~MouseType) +\n  theme_bw() +\n  theme(legend.position = \"none\") # Remove the legend\n\n\n\n\n\n\n\nFigure 7.1: Box n’ jitter plot of the RPM7 variable as a function of treatment.\n\n\n\n\n\nAs an example, the mean and standard deviation is calculated for females of the unselected breed given water. The example is given using both base R and Tidyverse.\n\nUsing base RUsing Tidyverse\n\n\nFirst, the information is extracted from the dataset:\n\nextracted_x &lt;- X[\n  X$MouseType == \"Control runner\" & X$gender == \"Female\" & X$Caffeine == \"Water\", # Rows\n  \"RPM7\"] # Column\n\nAnd then the extracted information can be fed into the mean() and sd() functions.\n\nmean(extracted_x)\n\n[1] 8.649382\n\nsd(extracted_x)\n\n[1] 2.07804\n\n\n\n\nIn Tidyverse the filter-function is used to filter by the MouseType, gender, and Caffeine levels of interest. Afterwards, the mean and standard deviation are computed using summarise.\n\nX |&gt; \n  filter(\n    MouseType == \"Control runner\",\n    gender == \"Female\",\n    Caffeine == \"Water\"\n  ) |&gt; \n  summarise(\n    mean = mean(RPM7),\n    sd = sd(RPM7)\n  )\n\n      mean      sd\n1 8.649382 2.07804\n\n\n\n\n\n\nBoth methods output a mean and standard deviation of\n\\[\n\\bar{x} = \\sum_{i=1}^{n} \\frac{x_i}{n} \\approx 8.6\n\\]\n\\[\n\\hat{\\sigma} = \\sum_{i=1}^{n} \\frac{(x_i - \\bar{x})}{n - 1} \\approx 2.1\n\\]\nfor the particular group of interest.\nA more efficient way of doing this would be to calculate the mean and standard deviation of each combination and display them in a table. The example below is given using both base R and Tidyverse.\n\nUsing base RUsing Tidyverse\n\n\nIn base R aggregate() can be used to calculate a function over a group.\n\n# Compute number of observations, mean and sd across groups\nn &lt;- aggregate(X$RPM7, by = list(X$MouseType, X$gender, X$Caffeine), length)\nm &lt;- aggregate(X$RPM7, by = list(X$MouseType, X$gender, X$Caffeine), mean)\ns &lt;- aggregate(X$RPM7, by = list(X$MouseType, X$gender, X$Caffeine), sd)\n\n# Combine and set column names\nstat_table &lt;- cbind(n, m$x, s$x)\ncolnames(stat_table) &lt;- c(\"MouseType\",\"Gender\",\"Caffeine\",\"N\",\"mean\",\"sd\")\n\n# Print as nice markdown table\nkable(stat_table, digits = 2)\n\n\n\n\nMouseType\nGender\nCaffeine\nN\nmean\nsd\n\n\n\n\nControl runner\nMale\nWater\n10\n8.55\n1.59\n\n\nHigh runner\nMale\nWater\n29\n26.03\n6.35\n\n\nControl runner\nFemale\nWater\n11\n8.65\n2.08\n\n\nHigh runner\nFemale\nWater\n28\n24.59\n7.25\n\n\nControl runner\nMale\nGatorade\n14\n8.55\n2.09\n\n\nHigh runner\nMale\nGatorade\n31\n25.04\n5.52\n\n\nControl runner\nFemale\nGatorade\n13\n9.31\n2.42\n\n\nHigh runner\nFemale\nGatorade\n32\n22.65\n5.86\n\n\nControl runner\nMale\nRed Bull\n12\n10.73\n2.95\n\n\nHigh runner\nMale\nRed Bull\n29\n24.81\n3.65\n\n\nControl runner\nFemale\nRed Bull\n9\n10.18\n2.14\n\n\nHigh runner\nFemale\nRed Bull\n31\n24.50\n6.50\n\n\n\n\n\n\n\nIn Tidyverse, the group_by()-function is used to group the observations by everything but the column RPM7 (so that will be type, gender and caffeine). Then, the number of observations, mean and standard devations are calculated across groups, giving the information for each group in one go.\n\nX |&gt; \n  group_by(across(-RPM7)) |&gt; \n  summarise(\n    N = n(),\n    mean = mean(RPM7),\n    sd = sd(RPM7)\n  ) |&gt; \n  kable(digits = 2) # Output pretty table in markdown\n\n`summarise()` has grouped output by 'MouseType', 'gender'. You can override\nusing the `.groups` argument.\n\n\n\n\n\nMouseType\ngender\nCaffeine\nN\nmean\nsd\n\n\n\n\nControl runner\nMale\nWater\n10\n8.55\n1.59\n\n\nControl runner\nMale\nGatorade\n14\n8.55\n2.09\n\n\nControl runner\nMale\nRed Bull\n12\n10.73\n2.95\n\n\nControl runner\nFemale\nWater\n11\n8.65\n2.08\n\n\nControl runner\nFemale\nGatorade\n13\n9.31\n2.42\n\n\nControl runner\nFemale\nRed Bull\n9\n10.18\n2.14\n\n\nHigh runner\nMale\nWater\n29\n26.03\n6.35\n\n\nHigh runner\nMale\nGatorade\n31\n25.04\n5.52\n\n\nHigh runner\nMale\nRed Bull\n29\n24.81\n3.65\n\n\nHigh runner\nFemale\nWater\n28\n24.59\n7.25\n\n\nHigh runner\nFemale\nGatorade\n32\n22.65\n5.86\n\n\nHigh runner\nFemale\nRed Bull\n31\n24.50\n6.50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.2\n\n\n\n\n\n\n\n\n\nExample 7.2 - Effect of caffeine on activity - probability\n\n\n\n\n\n\nExample Example 7.1 estimates the activity distribution of mice based on experimental data. Under the assumption that these metrics, the mean and the standard deviation, perfectly describe the distribution of the data, and further that this distribution is the normal distribution, we wish to calculate the probability of a given female mice, from normal breed, assigned water to perform higher than 10 RPM7.\nThat is:\n\\[\\begin{align*}\nP(X \\geq 10) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{10}^{\\infty} e^{-\\frac{(z - \\mu)^2}{2\\sigma}} dz \\\\\n&= 1 - P(X &lt; 10) \\\\\n%X &= X \\\\\n%X &= X\n&= 1 - \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{10} e^{-\\frac{(z - \\mu)^2}{2\\sigma}} dz \\\\\n%N(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x} e^{-\\frac{1}{2}z^2} dz\n\\end{align*}\\]\nwhere \\(X\\) is assummed normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThat is: \\(X \\sim \\mathcal{N} (\\mu,\\sigma^2)\\). \\\nA small illustration below (figure 7.2) shows the area of interest.\n\n# Create normal distributed data using estimated mean and sd\ndf &lt;- data.frame(x = seq(2, 16, 0.01)) |&gt; \n  mutate(y = dnorm(x, mean(extracted_x), sd(extracted_x))) \n\n# Plot normal curve \nggplot(df, aes(x, y)) +\n  geom_line(linewidth = 1) +\n  geom_area(data = subset(df, x &gt;= 10), fill = \"red\") +\n  labs(\n    x = \"RPM7\",\n    y = \"Probability\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 7.2: Normal density function fitted to the selected mouse population. The red area is the density for RPM7 &gt;= 10.\n\n\n\n\n\nThe size of the red area is simply calculated via the pnorm() function.\n\n1 - pnorm(10, mean(extracted_x), sd(extracted_x))\n\n[1] 0.2578628\n\n\n\\[\nP(X \\geq 10) \\approx 0.26\n\\]\nThe probability of a single female mice, from normal breed, administered with water to perform higher than \\(10 RPM7\\) is hence \\(0.26\\).\n\n\n\n\n\n\n\n7.2.2 Confidence interval for \\(\\mu\\)\nThe confidence interval for the center of a normal distribution (\\(\\mu\\)) is calculated as follows:\n\\[\nCI_{\\mu,1-\\alpha}:  \\hat{\\mu} \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma} / \\sqrt(n)\n\\] \\[\n\\bar{X} \\pm t_{1-\\alpha/2,df} \\cdot s / \\sqrt(n)\n\\]\nwhere \\(t_{1-\\alpha/2,df}\\) is a fractile, a number, which determines the coverage. Here, \\(\\alpha\\) is the left ot part. I.e. if a \\(90 \\%\\) confidence interval is wanted, the left out part is \\(\\alpha = 0.10\\). \\(n\\) is the number of samples on which the mean (\\(\\bar{X}\\)) is estimated, and \\(df\\) is the degrees of freedom, which refers to how well the standard deviation is estimated.\nFor instance, if one needs a \\(95 \\%\\) confidence interval (\\(\\alpha = 0.05\\)) based on a sample of \\(20\\) observations \\(t_{1-\\alpha/2,df} = t_{0.975,20-1} = 2.093\\).\nIt is further noticed that the interval is symmetric around the mean, and that the more samples (\\(n\\)) the lower the spread as the standard deviation is divided by \\(\\sqrt{n}\\).\n\n\n\n\n\n\nExample 7.3\n\n\n\n\n\n\n\n\n\nExample 7.3 - Effect of caffeine on activity - confidence intervals\n\n\n\n\n\n\nIn example Example 7.1 concerning the relation between caffeine and voluntary activity of mice, the running activity of mice in a wheel is recorded. In this example we wish to estimate the mean activity of the two groups with either water- or Red bull as caffeine source in control runner female mice, and give a confidence interval for these means.\nFirst, the data is subset to only include the two groups of interest.\n\nx_subset &lt;- X |&gt; \n  filter(\n    MouseType == \"Control runner\",\n    gender == \"Female\",\n    Caffeine %in% c(\"Red Bull\", \"Water\")\n  )\n\nAfter subsetting the data to only include these two groups a plot of the raw data looks like the following, and consists of \\(n = n_1 + n_2 = 11+ 9 = 20\\) samples (figure 7.3).\n\nggplot(x_subset, aes(Caffeine, RPM7, color = Caffeine)) +\n  geom_boxplot() +\n  geom_jitter() +\n  theme_bw() +\n  theme(legend.position = \"none\") # Remove legend\n\n\n\n\n\n\n\nFigure 7.3: Box n’ jitter plot of the mouse data subset.\n\n\n\n\n\nThe mean and standard deviation for the two groups can be calculated using summarise() via Tidyverse or aggregate() in base R (see Example 7.1 for an example on aggregate).\n\nx_subset |&gt; \n  group_by(Caffeine) |&gt; \n  summarise(\n    N = n(),\n    mean = mean(RPM7),\n    sd = sd(RPM7)\n  ) |&gt; \n  kable(digits = 2) # For pretty tables in markdown\n\n\n\n\nCaffeine\nN\nmean\nsd\n\n\n\n\nWater\n11\n8.65\n2.08\n\n\nRed Bull\n9\n10.18\n2.14\n\n\n\n\n\nThe confidence interval is calculated as follows:\n\\[\\begin{align}\nCI_{\\mu}: & \\hat{\\mu} \\pm t_{0.975,n-1} \\cdot \\hat{\\sigma} / \\sqrt(n) \\\\\n& \\bar{X} \\pm t_{0.975,n-1} \\cdot s / \\sqrt(n)\n\\end{align}\\]\nWith a total of \\(df_1 = n_1 - 1 = 10\\) degress of freedom the t-fractileat level \\(95 \\%\\) is \\(t_{0.975,10} = 2.23\\) why the confidence interval for the water treated group is:\n\\[\nCI_{\\mu_{water}}:  [8.6 - 2.23 \\cdot 2.1 / \\sqrt{11}; 8.6 + 2.23 \\cdot 2.1/ \\sqrt{11}]  =  [7.3; 10.0]\n\\]\n\nn &lt;- 11 # Number of obs in water group\nt_frac &lt;- qt(1 - 0.05/2, n - 1) # T-value\ns &lt;- sd(x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"])\nm &lt;- mean(x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"])\n\nci_lower &lt;- m - t_frac * s / sqrt(n)\nci_upper &lt;- m + t_frac * s / sqrt(n)\n\ncat(\"95% confidence interval for water group:\\n\", c(ci_lower, ci_upper))\n\n95% confidence interval for water group:\n 7.253336 10.04543\n\n\nAnd similar for the Red bull treated group.\n\\[\\begin{equation}\nCI_{\\mu_{Red Bull}}: [8.5; 11.8]\n\\end{equation}\\]\nIn R this can be assesed via the t.test() function.\n\nt.test(x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"])\n\n\n    One Sample t-test\n\ndata:  x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"]\nt = 13.805, df = 10, p-value = 7.744e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  7.253336 10.045428\nsample estimates:\nmean of x \n 8.649382",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#videos",
    "href": "chapters/week2/normal_distribution.html#videos",
    "title": "7  The normal distribution",
    "section": "7.3 Videos",
    "text": "7.3 Videos\n\n7.3.1 The Normal Distribution - CrashCourse\n\n\n\n7.3.2 An Introduction to the Normal Distribution - jbstatistics\n\n\n\n7.3.3 The Normal Distribution, Clearly Explained - StatQuest\n\n\n\n7.3.4 Confidence Intervals - CrashCourse",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/normal_distribution.html#exercises",
    "href": "chapters/week2/normal_distribution.html#exercises",
    "title": "7  The normal distribution",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\n\n\n\n\n\nExercise 7.1\n\n\n\n\n\n\n\n\n\nExercise 7.1 - Normal Distribution\n\n\n\n\n\n\nSerum cholestorol level is a biomarker of health in relation to a range of life-style related diseases. A population of adults have mean values of \\(178 mg/100mL\\) and \\(207 mg/100mL\\), standard deviations of \\(31 mg/100mL\\) and \\(37 mg/100mL\\) in males and females respectively.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nDraw curves (by hand - with pen on paper!) and plug in the parameters for these distributions.\nWhat percentage are below \\(150, 178\\) and \\(200 mg/100 mL\\) respectively (for males and females separately)?\nWhat percentage are above \\(140 mg/100 mL\\)\n\n\n\n\n\nA level above \\(240 mg/100 mL\\) is considered high risk, whereas the range between \\(200\\) and \\(239 mg/100 mL\\) is considered borderline high risk.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nHow big a proportion is considered at high risk (for males and females seperately)?\nHow big a proportion is considered borderline high risk (for males and females seperately)?\nNow assume that the above metrics describing the distribution (mean and standard deviation) is estimated based on samples of \\(30\\) males and \\(30\\) females. What it the changes to the calculations above?\n\n\n\n\n\nA population consists of \\(40\\%\\) males and \\(60\\%\\) females.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the propability that a random person from that population have a serum cholesterol level higher than \\(240 mg/100mL\\).\n\n\n\n\n\nUse that the mean and variance of a sum of two populations is the sum of the means and variance respectively. That is; Let \\(X\\) and \\(Y\\) be two indepedent random variables with mean \\(\\mu_X\\) and \\(\\mu_y\\) and variance \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\) respectively. And let:\n\\[\nZ = c_1X+ c_2Y\n\\]\nThen the mean and variance of \\(Z\\) is: \\[\n\\mu_Z = c_1\\mu_X + c_2\\mu_Y\n\\] \\[\n\\sigma^2_Z = c_1\\sigma^2_X + c_2\\sigma^2_Y\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the population mean and standard deviation of a population made up of \\(40\\%\\) males and \\(60\\%\\) females?\nUse this mean and standard deviation to calculate the probability of a random person from that population having a serum cholesterol level higher than \\(240 mg/100mL\\). How is that different from question 7? and why so?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.2\n\n\n\n\n\n\n\n\n\nExercise 7.2 - Transformations and the normal distribution\n\n\n\n\n\n\nThe normal distribution (also known as the Gaussian distribution) are central in biology as such, and in data analysis especially.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhy do we (sometimes) transform data before making statistical analysis?\nMake a histogram plot and a qqplot (use qqnorm() and qqline() functions) on the variable Ethyl.pyruvate from the “Wine” data set. Are the data normal distributed?\nTry to Log transform the variable Ethyl.pyruvate. Make a new histogram plot and a corresponding qqplot. Compare with question 1.\n\nUse the command par(mfrow = c(1,2)) (in front of your plotting commands) to get the plots side by side which makes it easier to compare the results.\nAdd a suitable title on both plots using the argument main=\"a very nice title\".\n\nDoes it seem possible to get all samples to match the same distribution?\nDo the same for the variable Benxyl.alcohol.\nDoes the log transformation work? What is the problem?\nTry the square root transformation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.3\n\n\n\n\n\n\n\n\n\nExercise 7.3 - WHO height and weight - standard normal distribution\n\n\n\n\n\n\nHeight and weight are important measures of growth during childhood. However, the largest factor impacting these measures are naturally the age of the child. In order to be able to compare children with slightly different ages such data are transformed using so-called growth curves. These are provided by the WHO and are constructed to represent the world wide distribution at specific ages for boys and girl. In this exercise you are supposed to take height and weight measures from African children living under different circumstances, transform them using WHO numbers for the world wide distribution, and further look for explanatory variables causing differences in growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart by importing the GrowthData.xlsx dataset into R. Check that all the info from the excel file was imported correctly using the commands dim() and head(). You can ’chop’ the dataframe in R to isolate the variables of importance (length, sex, age etc.).\nUsing the following graphs (figure 7.4), find the mean and standard deviation for boys and girls at age 15 months.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Z-scores for the length-to-age ratio for girls\n\n\n\n\n\n\n\n\n\n\n\n(b) Z-scores for the length-to-age ratio for boys.\n\n\n\n\n\n\n\nFigure 7.4: Z-scores for the length-to-age ratio for girls and boys aged 0 to 2 years. Source: WHO. Click on the images to enlargen them.\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nBased on the dataset imported, calculate mean and standard deviation for the girls and the boys, how does it relate to the data from WHO?\n\n\n\n\n\nYou can use the command X &lt;- X[complete.cases(X),] to remove the empty rows in the dataframe X, or google how to compute descriptive statistics in the case of missing values.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the Z-value? How can you use it to relate the dataset to the WHO data?\nCalculate the Z-scores for all the boys and girls.\n\n\n\n\n\nThis can be done by creating a new vector inserting the length-vector from the previous dataframe to the equation for Z-score.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat does it mean that a child is above or below 0 in Z-score? How many children has a Z-score below -2 and -3?\nHow many children are longer/taller than the world average?\nInvestigate whether some of the other variables (source of water primary_watersource or treatment of water water_treatment_type) impact the length at 15 months. Use ggplot() to do the plotting, and then use ggplot(...) + facet_wrap(~SEX) to split the plot in two corresponding to the variable SEX. Are the trends similar for the two sexes?",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The normal distribution</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html",
    "href": "chapters/week2/central_limit_theorem.html",
    "title": "8  Central limit theorem",
    "section": "",
    "text": "8.1 Reading material",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#reading-material",
    "href": "chapters/week2/central_limit_theorem.html#reading-material",
    "title": "8  Central limit theorem",
    "section": "",
    "text": "Central Limit Theorem (CLT) - in short\nVideos on the central limit theorem:\n\nThe Central Limit Theorem, Clearly Explained\nWhat is the central limit theorem? And why is it so hard to understand?\n\nChapter 8.3 in Introduction to Probability and Statistics Using R by J.G. Kern\n\nAvailable on Absalon",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#sec-intro-to-clt",
    "href": "chapters/week2/central_limit_theorem.html#sec-intro-to-clt",
    "title": "8  Central limit theorem",
    "section": "8.2 Central Limit Theorem (CLT) - in short",
    "text": "8.2 Central Limit Theorem (CLT) - in short\nIn short, the Central Limit Theorem (CLT) says, that the central parameter (e.g. the mean) obtained from a sample (stikprøve) from ANY distribution is normal distributed with variance equal to the sample variance divided by the sample size. That is:\n\\[\n\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n) \\quad for \\: n \\rightarrow \\infty\n\\]\nAs we are dealing with samples of finite size, and further needs to estimate the parameters (mean and variance) based on this sample, the Normal distribution is exchanged by the T-distribution. The T-distribution take the uncertainty of having finite data into account. That is, if \\(X_1,X_2,..,X_n\\) is randomly sampled from some distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (approximated by \\(s_X^2\\)), then:\n\\[\n\\frac{\\bar{X} - \\mu}{s_X/\\sqrt{n}} \\sim \\mathcal{T}(df) \\quad for \\: finite \\: n\n\\]\nWhere \\(df\\) (degrees of freedom) is equal to how well the variance is estimated (usually \\(df = n-1\\)).",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#videos",
    "href": "chapters/week2/central_limit_theorem.html#videos",
    "title": "8  Central limit theorem",
    "section": "8.3 Videos",
    "text": "8.3 Videos\n\n8.3.1 The Central Limit Theorem, Clearly Explained - StatQuest\n\n\n\n8.3.2 What is the central limit theorem? And why is it so hard to understand? - MrNystrom",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week2/central_limit_theorem.html#exercises",
    "href": "chapters/week2/central_limit_theorem.html#exercises",
    "title": "8  Central limit theorem",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\n\n\n\n\n\n\n\n\nExercise 8.1 - Quality control and central limit theorem\n\n\n\n\n\n\nEnsuring the end product quality in food production is naturally of utmost interest. In doing so, the production is monitored at several critical points, in order to check that it is . This is referred to as statistical process control (SPC) and is under the umbrella of process analytical technology (PAT).\nIn short, a critical point in a production is monitored by a so-called control card, that register a value, e.g. pH, temperature, or other essential product/production parameters. When a point is above or below certain limits, there is an alarm. However, due to natural variation, now and then such alarms occur without there being any systematic faults. These are called false positives.\nA cheese production monitors the 24 hours pH for every production batch within a day. Under normal conditions the false alarms follow a distribution with mean \\(\\mu = 3\\) and variance \\(\\sigma^2 = 3\\) (the distribution is a Poisson distribution, but that is actually not so important here). The production runs very well, so the monitoring of these alarms are only used for retrospective follow up. Suddenly there are reclamations based on the last years production, and the boss want you to check up on whether it is the 24 hours pH causing the trouble.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nYou check the control card and finds that during the last year there were on average \\(3.2\\) alarms. Does this indicate problems with the 24 hours pH?\nUse the rpois() to generate data from \\(1000\\) years, and check whether your results match.\nImagine that the aggregated measure were over a month (30 days) or over a week (7 days) or as low as average over 2 days. How does this affect the agreement between the analytical probability (via central limit theorem) and the simulated probability (based on the actual underlying distribution)?\n\n\n\n\n\nYou might want to use the following code as inspiration for how it can be done in R.\n\nn &lt;- ? # How many samples to average over\nl &lt;- ? # Population mean\ns &lt;- ? # Population standard deviation\nx &lt;- ? # The odd observation\n\n1 - pnorm(x, mean = ?, sd = ?)\n\n# Simulation \nk &lt;- 10000 # The number of e.g. years to simulate\n\nX &lt;- matrix(rpois(n*k, l), k, n) # Simulate data\n\nmX &lt;- apply(X, 1, mean) # Compute row-wise mean\n\nhist(mX)\n\nsum(mX &gt; x) / length(mX)\n\n\n\n\n\n\n\nExercise 8.1",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Central limit theorem</span>"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html",
    "href": "chapters/week3/week_3.html",
    "title": "Week 3",
    "section": "",
    "text": "Hand-in assigment\nIn this week we are introducing the concept of inferential statistics. That is to be able to answer a specific question based on observed data. Biological or scientific questions are, in statistical terms, formulated as a hypothesis, which can be tested using data. One of the most widely used tests, are the t-test for comparison of the mean from two distributions.\nExercise 9.3 Diet and Fat metabolism - T test - in R is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#hand-in-assigment",
    "href": "chapters/week3/week_3.html#hand-in-assigment",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nComplete Exercise 9.2 prior to the hand-in assignment to make sure you understand what R does during a t-test.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#exercises",
    "href": "chapters/week3/week_3.html#exercises",
    "title": "Week 3",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 9.1\nExercise 9.2\nExercise 9.4\nExercise 9.5\n\nFor Wednesday work through the following exercises:\n\nExercise 9.7\nExercise 9.8\nExercise 10.1",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/week_3.html#case-ii",
    "href": "chapters/week3/week_3.html#case-ii",
    "title": "Week 3",
    "section": "Case II",
    "text": "Case II\nThe second case is described in the document “Case II.pdf”. You should work on the case in groups of four, and hand in a slide-show with voice no later than Thursday evening next week.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html",
    "href": "chapters/week3/t_test.html",
    "title": "9  T-test",
    "section": "",
    "text": "9.1 Reading material",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#reading-material",
    "href": "chapters/week3/t_test.html#reading-material",
    "title": "9  T-test",
    "section": "",
    "text": "Independent T-test - in short\nPaired T-test - in short\nVideos on the elements of statistical testing focusing on hypothesis testing:\n\nHypothesis Testing and The Null Hypothesis, Clearly Explained\np-values: What they are and how to interpret them\n\nVideos on the test statistics and the T-test:\n\nTest Statistics\nT-Tests: A Matched Pair Made in Heaven\n\nChapter 3 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#sec-independent-t-test",
    "href": "chapters/week3/t_test.html#sec-independent-t-test",
    "title": "9  T-test",
    "section": "9.2 Independent T-test - in short",
    "text": "9.2 Independent T-test - in short\n\n9.2.1 Models of two samples\nFirst of all the data are characterized by two models, here normally distributed samples:\n\\[\nX_{11},X_{12},X_{13},...,X_{1n_1} \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)\n\\]\n\\[\nX_{21},X_{22},X_{23},...,X_{2n_2} \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2)\n\\]\n\n\n9.2.2 Hypothesis\nThe research question goes on the difference between the two samples. Such as question is formalized statistically using the parameters of the two models under investigation and called a hypothesis. Further, a question of is formalized as the opposite; similarity.\nThat is the null hypothesis of no difference between the two sample means is formalized as:\n\\[\nH0: \\mu_1 = \\mu_2\n\\]\nIf this hypothesis turns out not to be true. That is; there is a difference between the two distribution means, then this is referred to as the alternative hypothesis:\n\\[\nHA: \\mu_1 \\neq \\mu_2\n\\]\nor directional:\n\\[\nH_A: \\mu_1&gt;\\mu_2 \\ \\ \\ or \\ \\ \\  H_A: \\mu_1&lt;\\mu_2\n\\]\n\\[\nH_A: \\mu_1&lt;\\mu_2\n\\]\n\n\n9.2.3 Test statistic\nFrom these data, a t-statistic \\(t_{obs}\\) can be calculated:\n\\[\nt_{obs} = \\frac{\\bar{X_1} - \\bar{X_2}}{s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nwhere \\(s_{pooled}\\) is the pooled standard deviation, which is simply a weighted mean of the variances.\n\\[\ns_{pooled}^2 = \\frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2-2}\n\\]\nAlternatively, this formulation is equivalent:\n\\[\nt_{obs} = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{s_{X_1}^2}{n_1} + \\frac{s_{X_2}^2}{n_2}}}\n\\]\n\n\n9.2.4 Test probability\nThis t-statistics for the non-directional two-sided test can be translated into a probability under the null hypothesis (of no difference).\n\\[\nP = 2 \\cdot P(T_{df} \\geq |t_{obs}|) = 2 \\cdot (1 - P(T_{df} \\leq |t_{obs}|))\n\\]\nAlternatively, one can calculate a confidence interval for the differences between the means, which leads to the same interpretation of the results:\n\\[\n\\bar{X_1} - \\bar{X_2} \\pm t_{1 - \\alpha/2}s_{X_{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nThe standard deviation used here (\\(s_{X_{pooled}}\\)) is based on weighted average of the individual sample variances.\n\n\n\n\n\n\nExample 9.1\n\n\n\n\n\n\n\n\n\nExample 9.1 - Effect of caffeine on activity - hypothesis test\n\n\n\n\n\n\nIn Example 7.1 concerning the relation between caffeine and voluntary activity of mice, the running activity of mice in a wheel is recorded. In this example we wish to compare the mean activity of the two groups with either water- or Red bull as caffeine source in control runner female mice, and give a confidence interval for these means.\nAs such, from Example 7.3 the activity seems higher in the Red Bull treated group. The question we wish to answer is: Is there a difference between the two population means. In statistical terms this question is formalized as a hypothesis.\nFirst, models for the data is suggested\n\\[\nX_1 \\sim \\mathcal{N}(\\mu_1,\\sigma^2)\n\\] \\[\nX_2 \\sim \\mathcal{N}(\\mu_2,\\sigma^2)\n\\]\nNote, that the spread \\((\\sigma)\\) is assumed to be similar in the two groups.\nHypothesis\nIf we are interested in a difference, then we formulate the opposite, that is; the two population means are equal.\n\\[\nH0: \\mu_1 = \\mu_2\n\\]\nIf this turns out not to be true, then the alternative is suggested to be:\n\\[\nHA: \\mu_1 \\neq \\mu_2\n\\]\nA statistical test done via first constructing a measure of how far from the \\(H0\\) the data is. This is referred to as the test-statistic and then secondly this measure is translated in to a probability.\nTest statistic\n\\(H0\\) sets the two means to be equal, so naturally a measure of how well this fits with data is reflected as the difference between the observed averages:\n\\[\n\\bar{X}_1 - \\bar{X}_2\n\\]\nwhere a large (positive or negative) value indicates descrepancy from \\(H0\\).\nThis distance depends on the scale of the data, why some kind of normalization needs to be encountered. Further, the number of samples used to calculate the means should also weight in. In total this leads to the t-test statistic.\n\\[\nt_{obs} = \\frac{\\bar{X}_1 -\\bar{X}_2}{s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nWe see that almost all the ingredients for calculating this is given via the descriptive statistics. The only thing needed is the pooled standard deviation (\\(s_{pooled}\\)), which is simply a weighted mean of the variances.\n\\[\\begin{align}\ns_{pooled}^2 &= \\frac{(n_1 - 1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2-2} \\\\\n&= \\frac{(11 - 1) \\cdot 2.1^2 + (9-1)\\cdot 2.1^2}{11 + 9-2} \\\\\n&= 2.1^2   \n\\end{align}\\]\nThis leads to:\n\\[\\begin{align}\nt_{obs} &= \\frac{\\bar{X}_1 -\\bar{X}_2}{s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\\\\n&= \\frac{8.6 -10.2}{2.1 \\sqrt{\\frac{1}{11} + \\frac{1}{9}}} \\\\\n&= 1.61   \n\\end{align}\\]\nP-value\nThe test statistics is translated to a probability by the following:\n\\[\nP = 2 \\cdot P(T_{df} \\geq |t_{obs}|) = 2 \\cdot (1 - P(T_{df} \\leq |t_{obs}|))\n\\]\nwhich corresponds to the colored areas shown in figure 9.1.\n\n\nCode\n# Dummy data to make geom_area actually plot\ndf &lt;- data.frame(x = 1, y = 42)\n\n# Arguments for pt function\ndtArgs &lt;- list(df = 20 - 2)\n\n# Critical values\nlwrCrit &lt;- qt(0.05/2, 30)\nuprCrit &lt;- qt(1 - 0.05/2, 30)\n\n# Plot\nggplot(df, aes(x, y)) +\n  xlim(-4, 4) +\n  geom_function(fun = dt, args = dtArgs, \n                linewidth = 1\n  ) +\n  geom_area(stat = \"function\", fun = dt, args = dtArgs, # Lower crit region\n            xlim = c(-5, lwrCrit), \n            fill = \"darkgreen\", alpha =.5\n  ) +\n  geom_area(stat = \"function\", fun = dt, args = dtArgs, # Upper crit region\n            xlim = c(uprCrit, 5), \n            fill = \"darkgreen\", alpha =.5\n  ) +\n  labs(x = \"t-statistic\",\n       y = \"Probability\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 9.1: The critical regions of the t-statistic for a t-test with \\(\\alpha = 0.05\\) (i.e. the total area of the shaded areas is equal to \\(0.05\\)) and \\(df = 20 - 2\\).\n\n\n\n\n\nThis has no analytical solution, but can be assessed from tables or via pt() in R:\n\nt_obs &lt;- 1.610\ndf &lt;- 20 - 2\n\n2 * (1 - pt(t_obs, df))\n\n[1] 0.1247948\n\n\nIn conclusion; Although a difference in activity is observed when mice are given Red Bull compared to water, it is Not unlikely (\\(p = 0.12\\)) that this could be due to chance. Actually this would happen one out of eight times.\nThe entire procedure can be done in R:\n\nt.test(x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"],\n       x_subset[x_subset$Caffeine == \"Red Bull\", \"RPM7\"],\n       var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"] and x_subset[x_subset$Caffeine == \"Red Bull\", \"RPM7\"]\nt = -1.615, df = 18, p-value = 0.1237\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.5208153  0.4604011\nsample estimates:\nmean of x mean of y \n 8.649382 10.179589",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#sec-paired-t-test",
    "href": "chapters/week3/t_test.html#sec-paired-t-test",
    "title": "9  T-test",
    "section": "9.3 Paired T-test - in short",
    "text": "9.3 Paired T-test - in short\nA special case, is where the two samples (\\(X_{11},X_{12},X_{13},...,X_{1n}\\) and \\(X_{21},X_{22},X_{23},...,X_{2n}\\)) are paired. That is for example measurements before and after treatment on the same set of samples or when the same assessor in a sensorical panel scores two different product types.\nThe question of interest is still the same; namely is there a difference between the two populations, however, the statistics is slightly different.\n\n9.3.1 Model\nThe model of the data is extended with a sequence of differences\n\\[\\begin{align}\n    & D_1,D_2,\\ldots,D_n  \\nonumber  \\\\\n=   & (X_{12} - X_{11}), (X_{22} - X_{21}), \\ldots , (X_{n2} - X_{n1}) \\\\\n\\sim & \\mathcal{N}(\\mu_{D},\\sigma_{D}^2)  \\nonumber\n\\end{align}\\]\nWhere \\(D_i\\) is the difference between the responses for each of the paired observations \\(i\\) (\\(i = 1,2,\\ldots,n\\)), and \\(s_D\\) is the observed standard deviation calculated on \\(D_1,D_2,\\ldots,D_n\\).\n\n\n9.3.2 Hypothesis\nThe hypothesis of similarity is then formalized on \\(\\mu_D\\):\n\\[\\begin{equation}\nH0: \\mu_D = 0\n\\end{equation}\\]\nOften with the alternative of a difference:\n\\[\\begin{equation}\nHA: \\mu_D \\neq 0\n\\end{equation}\\]\n\n\n9.3.3 Test Statistic\n\\[\\begin{equation}\nt_{obs} = \\frac{\\bar{D} }{s_{D} / \\sqrt{n}}\n\\end{equation}\\]\nWhich is translated to a p-value through a \\(\\mathcal{T}_{df}\\) with \\(df = n-1\\) degrees of freedom.\nAs such, the test statistics only sees the original data via paired differences, and hence become a specialty of the single-sample case.\n\n\n\n\n\n\nExample 9.2\n\n\n\n\n\n\n\n\n\nExample 9.2 - Natural phenolic antioxidants for meat preservation - paired T-test\n\n\n\n\n\n\nThis example is based on the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control, used previously in Example 5.1\nT-tests can be used to evaluate the effects (the response of a single variable) of two treatments against each other. Here, we use the week 4 data, and compare the rosemary extract treatment to the control.\nLoad the data and libraries\nRemember to set your working directory.\n\nlibrary(ggplot2)\nload(\"meat_data.RData\")\n\nPlot data\nFirst the data under investigation is plotted - only week 4, control, and RE samples are used (figure 9.2).\n\n# Subset data such that Treatment = \"Control\" or \"RE\" AND week = 4 \ndf &lt;- X[X$Treatment %in% c(\"Control\", \"RE\") & X$week == 4, ]\n\nggplot(df, aes(Treatment, Texture_Boiled_egg_white, group = Assessor)) +\n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 9.2: Paired sets of data points for the boiled egg texture variable in week 4 of the experiment. Each line represents a single assessor scoring both samples.\n\n\n\n\n\nThe figure clearly shows that there is huge variation related to Assessor (connected by lines). Although the distributions of the two groups are overlapping, ALL assessors scores the RE sample lower than the Control sample.\nDefine subsets of data\n\n# Subset data such that Treatment = \"Control\" AND week = 4\ncontrolWeek4 &lt;- X[X$Treatment == \"Control\" & X$week == 4, ]\n\n# Subset data such that Treatment = \"RE\" AND week = 4\nREWeek4 &lt;- X[X$Treatment == \"RE\" & X$week == 4, ]\n\nBe aware that the Assessors are ordered ensuring pairing in the two arrays controlWeek4 and REWeek4. Take precaution, as this might not always be the case!\nModel of data\nAs it is the same assesors that are used for evaluation of both products the model of the data becomes:\n\\[\\begin{equation}\nX_{control} \\sim \\mathcal{N}(\\mu_{control},\\sigma_{control}^2) \\ \\ \\text{and} \\ \\ X_{RE} \\sim \\mathcal{N}(\\mu_{RE},\\sigma_{RE}^2)\n\\end{equation}\\]\n\\[\\begin{align}\nD_1,D_2,\\ldots,D_n \\sim \\mathcal{N}(\\mu_{D},\\sigma_{D}^2)\n\\end{align}\\]\nwhere \\(D_i\\) is the difference between rosemary extract- and control treated sausages with respect to the sensorical score (texture - boiled egg) for assessor \\(i\\) (\\(i = 1,2,\\ldots,n\\))\n\\[\\begin{align}\n& D_1,D_2,\\ldots,D_n = (X_{1,RE} - X_{1,control}), (X_{2,RE} - X_{2,control}), \\ldots , (X_{n,RE} - X_{n,control}).\n\\end{align}\\]\n*Hypothesis\nIf we are interested in a difference, then we formulate the opposite, that is; on average the difference between sensorical scores are \\(0\\).\n\\[\\begin{equation}\nH0: \\mu_D = 0\n\\end{equation}\\]\nIf this turns out not to be true, then the alternative is suggested to be:\n\\[\\begin{equation}\nHA: \\mu_D \\neq 0\n\\end{equation}\\]\nT-test on the boiled egg texture variable\nThe T-test should be a paired T-test, since it is the same assosors tasting each of the treated sausages.\n\nt.test(controlWeek4$Texture_Boiled_egg_white,\n       REWeek4$Texture_Boiled_egg_white,\n       paired = T\n       )\n\n\n    Paired t-test\n\ndata:  controlWeek4$Texture_Boiled_egg_white and REWeek4$Texture_Boiled_egg_white\nt = 3.7747, df = 7, p-value = 0.00694\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.20123 5.23002\nsample estimates:\nmean difference \n       3.215625 \n\n\nThe p-value is \\(p = 0.007\\) and the null-hypothosis must therefore be rejected meaning there is a very significant difference in boiled egg texture between the control and rosemary extracted treated samples.\nT-test on the salty taste variable\nIn a similar fashion another sensorical variable is tested; namely salty taste.\n\nt.test(controlWeek4$Taste_Salty,\n       REWeek4$Taste_Salty,\n       paired = T\n       )\n\n\n    Paired t-test\n\ndata:  controlWeek4$Taste_Salty and REWeek4$Taste_Salty\nt = 0.98163, df = 7, p-value = 0.359\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.5811623  1.4061623\nsample estimates:\nmean difference \n         0.4125 \n\n\nThe p-value is \\(p = 0.36\\) and the null-hypothosis can therefore not be rejeceted, meaning there is no significant difference in salty taste between the control and rosemary extract treated samples.\nComparison with PCA results\nWhen the results obtained from the T-tests are compared to a PCA on the week 4 data, it can be seen that they are in agreement. The very significant effect of the rosemary extract treatment on the boiled egg texture corresponds well with the loading being strongly associated with the control samples, in a direction which seems to be explanatory for the difference between the roseary extract- and control treatment groups. The non-significant effect on the salty taste variable is congruent with its loading being located in a direction which does not seem to be of importance in the separation of the control and treated groups.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#videos",
    "href": "chapters/week3/t_test.html#videos",
    "title": "9  T-test",
    "section": "9.4 Videos",
    "text": "9.4 Videos\n\n9.4.1 Hypothesis Testing and The Null Hypothesis, Clearly Explained - StatQuest\n\n\n\n9.4.2 p-values: What they are and how to interpret them - StatQuest\n\n\n\n9.4.3 Test Statistics - CrashCourse\n\n\n\n9.4.4 T-Tests: A Matched Pair Made in Heaven - CrashCourse",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/t_test.html#exercises",
    "href": "chapters/week3/t_test.html#exercises",
    "title": "9  T-test",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\n\n\n\n\n\nExercise 9.1\n\n\n\n\n\n\n\n\n\nExercise 9.1 - T-test\n\n\n\n\n\n\nT-test comparison of a womens football team and a mens football team. The mean weight of the mens team is \\(\\bar{X}_{men} = 82kg\\) with a standard deviation of \\(s_{men} = 8kg\\), while the womens team has a mean weight of \\(\\bar{X}_{women} = 61kg\\) and a standarddeviation of \\(s_{women} = 6kg\\). There are 11 players on each team.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the model and hypothesis\nCalculate the test statistic (\\(t_{obs}\\))\nCalculate the p-value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.2\n\n\n\n\n\n\n\n\n\nExercise 9.2 - Diet and fat metabolism - T-test by hand\n\n\n\n\n\n\nThe diet is a central factor involved in general health, and especially in relation to obesity, where a balance between intake of protein, fat and carbohydrates, as well as type of these nutrients seems important. Therefor various controlled studies are conducted to show the effect of different diets. A study examining the effect of protein from milk (casein or whey) and amount of fat on growth biomarkers of fat metabolism and type I diabetes was conducted in \\(40\\) mouse over an intervention period of \\(14\\) weeks.\nFor this exercise we are going to focus on cholesterol as a biomarker related to fat metabolism, and on a low fat diet (LF) and a high fat diet (HF). The cholesterol level at the end of the \\(14\\) week intervention is listed below (table 9.1).\n\n\n\nTable 9.1: Cholesterol samples from the end of week 14 for the low-fat (LF) and high-fat (HF) diet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sum X\\)\n\\(\\sum X^2\\)\n\n\n\n\nLF\n3.97\n3.69\n2.61\n4.03\n2.98\n3.51\n3.62\n2.81\n3.62\n3.53\n\n\n\n\n\n34.37\n120.20\n\n\nHF\n4.68\n3.60\n4.84\n4.92\n3.70\n4.83\n3.38\n4.62\n4.60\n4.84\n4.84\n4.54\n5.27\n4.26\n4.3\n67.29\n305.92\n\n\n\n\n\n\n\nThis exercise is supposed to be done by hand where the computer only is used as a pocket calculator.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate descriptive statistics for these data.\nSketch these results in a graph (by pen and paper - no computer)\nGive a frank guess on cholesterol differences between diets.\nState a hypothesis, and calculate the test statistics and the corresponding p-value.\n\nWhat assumptions did you make concerning the variances in the two distributions? What alternatives do you have?\n\nGive a confidence interval for the difference between diets.\nHow does the t-test result (p-value) correspond to the confidence interval?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.3\n\n\n\n\n\n\n\n\n\nExercise 9.3 - Diet and fat metabolism - T-test in R\n\n\n\n\n\n\nThis exercise repeats some of the elements from Exercise 9.2 using R including an extension of the panel of relevant biomarkers and utilization of PCA to get a comprehensive overview.\nThe data can be found in the file Mouse_diet_intervention.xlsx. The code below imports the data, and subsets on two (of the three diets). The factor diet is called diet_fat, whereas cholesterol is called cholesterol.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRepeat the analysis of Exercise 9.2 using R, including plotting and testing.\n\nHint: For testing you might want to predefine the response and factor and then use the function t.test() to run the analysis, which actually returns most of the relevant results - see the code box below.\n\nHow robust are the results towards transformation of the response or extreme samples?\n\n\n\n\n\nIn addition to cholesterol, other biomarkers have been measured, such as insuline, triglycerides etc.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse one of these additional biomarkers as measure of health status, and repeat the analysis comparing dietary fat (including plotting).\nMake a PCA on the biomarkers insulin, cholesterol, triglycerides, NEFA, glucose, and HbA1c, and plot the results.\n\nNEFA = nonesterified fatty acids.\n\nComment on the results. Do the T-test results fit with the PCA results? Are there other markers of dietary fat intake? Does the correlation structure make sense? (The later is hard to answer without physiological knowledge, but give it a shot)\n\n\n\n\n\n\n# Import data\nmouse &lt;- read_excel(\"Mouse_diet_intervention.xlsx\")\n\n# Include only rows where diet_protein = casein\nmouse &lt;- mouse[mouse$diet_protein == \"casein\", ]\n\n# Predefine response and factor\ndiet &lt;- mouse$diet_fat\ny &lt;- mouse$cholesterol\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.4\n\n\n\n\n\n\n\n\n\nExercise 9.4 - Fiber and cholesterol\n\n\n\n\n\n\nFiber is suspected to have benificial physiological activity if being a part of a regular diet. In this study \\(n = 13\\) healthy young men and women were enroled in a trial to unravel the effect of fiber supplement. At baseline (that is before dietary intervention), the persons were screned for a range of biomarkers in the blood including cholesterol fractions. Then, they were put on a diet with supplementation of dietary fibers for a period of \\(30\\) days. In the dataset FiberData.xlsx, the baseline levels (_t0), and end of trial levels (_t30) are listed for total-, hdl- and ldl cholesterol. The data is a part of a larger study conducted at Department of Nutrition Exercise and Sports.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\nAnalyse if there is an effect of the fiber intervention on these biomarkers. That includes; inspection of raw data, possible transformations, visulization of effects, descriptive statistics, test of effect and presentation of relevant confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.5\n\n\n\n\n\n\n\n\n\nExercise 9.5 - Stability of oil under different conditions\n\n\n\n\n\n\nOil are primary made up of triglycerides, where some of the fatty acids are unsaturated. This causes such a product to be susceptible to oxidation both from chemical oxidative agents such as metal ions, or from exposure to light. Oxidation of the unsaturated fatty acids changes the sensorical and physical properties of the oil.\nIn the southern part of Africa grows a robust bean - the Marama bean. This bean has a favorable dietary composition, including dietary fibers, fats and proteins, as most similar types of nuts, therefor this crop could be utilized for making healthy products by the locals for the locals. One such product is Marama bean oil. A study has been conducted to investigate the oxidative stability of the oil under various conditions. In the dataset MaramaBeanOilOx.xlsx are listed the results from such an experiment (including data from both normal and roasted beans). The experimental factors are:\n\nStorage time (Month)\n\nProduct type (Product)\nStorage temperature (Temperature)\nStorage condition (Light)\nPackaging air (Air)\n\nAnd the response variable reflecting oxidative stability is peroxide value and named PV.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data, and subset so that you only include data related to product type Oil.\nMake descriptive plots of the response variable PV imposing storage-temperature, condition and time.\n\nHint: factor(Temperature):Light specifies all combinations of these two factors. facet_grid(.~Month) splits the plot into several plots according to Month.\n\nWhat do you observe in terms of storage- time, temperature and condition from this plot?\n\n\n\n\n\nSome of the differences is so pronounced that testing seems irrelevant. However, there are small differences between storage temperature at storage condition dark.\n\nZack out the data for these two groups at storage time = 0.5 month, and make a comparison.\nDo the same at storage time = 1 month.\n\nWe would like to generate profiles for each condition over time, reflecting centrality and dispersion. In order to get there, we need to massage the data a little. The function summarySE() from the package Rmisc is a nice tool to construct a dataset with summary statistics. Try to run the code below - what have we learned about oil oxidation from this study?\n\n# Compute summary table for \"PV\"\nmaramaSummary &lt;- summarySE(maramaOil, measurevar = \"PV\",\n                           groupvars = c(\"Light\", \"Temperature\", \"Month\"))\n\n# Repeat time = 0 for all conditions\nM1 &lt;- maramaSummary[1, ]\nM1$Light &lt;- \"light\"\nM1$Temperature &lt;- 25\n\nM2 &lt;- maramaSummary[1, ]\nM2$Temperature &lt;- 35\n\nMtot &lt;- rbind(maramaSummary, M1, M2)\n\n# Plot a nice oxidation graph with errorbars\nggplot(Mtot, aes(x = Month, y = PV, color = factor(Temperature):Light)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = PV - se, ymax = PV + se), width = .03) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.6\n\n\n\n\n\n\n\n\n\nExercise 9.6 - Aroma in milk and cheese\n\n\n\n\n\n\nIn order to increase the biodiversity of hayfields it is of interest growing different kinds of herbs together with the hay. Hay is used for feeding of cows. Type of feed may affect the amount and type of flavor components in the milk as well as the flavor components in cheese made from the milk. Hence, increasing the biodiversity of hayfields used for cow feed may impact the taste of milk and cheese. An experiment was conducted in order to investigate this. In the experiment cows were fed three different diets:\n\nFeed 1: Ryegrass and white clover\nFeed 2: Feed 1 + red clover, chicory and ribwort plantain\nFeed 3: Feed 2 + lucern, birdsfoot trefoil, melilot, caraway, yarrow and salad burnet.\n\nFlavor components were quantified in the raw milk as well as in the cheese after \\(12\\) months of ripening. Three farmers (G, H, and P) participated in the experiment. Furthermore, controls (K), consisting of bulk milk samples from the dairy, are included. The data are found in the file Milk_Cheese_Aroma.xlsx.\nData are originating from the EcoServe project and is kindly provided by Thomas Bæk Pedersen, Department of Food Science, University of Copenhagen.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data set.\nOpen the data and get familiar with the data set (that could include something like; how many samples, how many samples from each combination of feed and farmer, how many variables, and some descriptive plots of some of them).\n\n\n\n\n\nIn this exercise we are interested in making a pairwise comparison to investigate whether differences exists between the feedings. Hence, for now we will exclude the controls.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake two subsets of the data; one subset for milk samples and one subset for cheese samples. Exclude controls in the subsets.\n\n\nInvestigate a given aroma compound (e.g. Limonene) in the milk subset. This can be done by e.g. making a boxplot with feed on the x-axis and filling color according to farmer. Do you see any systematic effect of feed?\n\n\n\n\n\n\n# Investigate the code and create the two subsets (one for cheese and one for milk).\n# What does the \"&\", \"|\", and \"!\" do?\nMilk &lt;- subset(Data, Sample == \"Milk\" & Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" & !Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" | Farmer == \"K\")\nMilk &lt;- subset(Data, Sample == \"Milk\" | !Farmer == \"K\")\n\n# Make a boxplot\nggplot(yourDataHere, aes(xVariable, yVariable, fill = yourFactor)) +\n  geom_boxplot()\n\nIn a paired t-test we will investigate whether there is a difference between feed 1 and feed 3. In a paired t-test we are investigating if the mean of differences between pairs is significantly different from zero. The following steps will take you through the test.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the difference (for e.g. Limonene) between each pair. Note that these differences now represent your sample\n\n\n\n\n\n\nfeed1 &lt;- subset(Milk, Feed == \"I\") # Extract feed I\nfeed1 &lt;- feed1[with(feed1, order(Farmer)), ] # Sort according to farmer\n\nfeed3 &lt;- subset(Milk, Feed == \"III\") # Extract feed III\nfeed3 &lt;- feed3[with(feed3, order(Farmer)), ] # Sort according to farmer\n\ndiff &lt;- feed1 - feed3 # Compute the difference\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a boxplot of the differences.\n\n\nAdd to the boxplot a zero line and the mean of the differences\nDiscuss the plot\nWhy do we add a line with intercept y=0? Why do we add the mean?\n\n\n\n\n\n\nfeed1 &lt;- subset(Milk, Feed == \"I\") # Extract feed I\nfeed1 &lt;- feed1[with(feed1, order(Farmer)), ] # Sort according to farmer\n\nfeed3 &lt;- subset(Milk, Feed == \"III\") # Extract feed III\nfeed3 &lt;- feed3[with(feed3, order(Farmer)), ] # Sort according to farmer\n\ndiff &lt;- data.frame(\"diff\" = feed1$Limonene - feed3$Limonene) # Compute the difference\nmeanDiff &lt;- mean(diff$diff) # Calculate mean of difference\n\nggplot(diff, aes(x = 1, y = diff)) +\n  geom_boxplot() +\n  geom_hline(aes(yintercept = 0), color = \"red\", linetype = \"dotted\") +\n  geom_point(x = 1, y = meanDiff, color = \"darkred\", size = 5)\n\nWe have now calculated the differences and we have calculated the mean of the differences. However, the mean is an estimated value and in order to find out whether there is a significant difference between feed 1 and feed 3, we need to calculate the confidence interval round the mean.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the mean and the standard deviation of the differences and use these calculations to find the confidence interval of the mean.\n\nFor a 95% confidence interval you can find the critical t-value by calling abs(qt(0.025,df)) for a two sided test. How would the R-call look if you were calculating the critical t-value for a one-sided test?\n\nIs zero included in the confidence interval? What does this mean?\nCalculate the t-statistic. Is there a significant difference between the two feedings?\nUnderstand the following code and call it in R.\n\nUnderstand the output and compare the confidence interval and the t-statistics with what you calculated in question 6 and question 7.\n\n\n\nt.test(feed1$Limonene, feed3$Limonene,\n       mu = 0, alt = 'two.sided', paired = T, conf.level = 0.95)\n\n\nNow investigate the cheese-subset. Pick the same aroma compound you were working on during the milk-subset. Do you think there is a difference in the aroma compound between feed I and feed III when looking at the cheese-samples? Write the hypothesis and test it using a paired t-test.\n\nUse the R-call t.test(……).\n\n\nIf you were a farmer, would you be worried that increasing the biodiversity of your hayfields with herbs would result in altered taste in the milk/cheese?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.7\n\n\n\n\n\n\n\n\n\nExercise 9.7 - Power of paired tests\n\n\n\n\n\n\nIn Exercise 9.4 Fiber and Cholesterol you have probably used either a paired t-test or a two-sample t-test. Try to conduct both of these tests and see the difference. What needs to be full filled in order to make a paired test? How many participants would have been enrolled in the study, in the case where a two sample t-test were used for inferential analysis of the data?\n\n\n\n\n\n\n\n\n\n\nExercise 9.8\n\n\n\n\n\n\n\n\n\nExercise 9.8 - Confidence intervals\n\n\n\n\n\n\nThe datafile Wine.xlsx lists aroma compounds from different wines from four countries. The compound Acetic acid has a skewed distribution, and therefore needs an appropriate transformation. Calculate the center of the distribution for each country and give a confidence interval for this parameter, where you take into account the need for transformation, but still want to report the results in original units.\nThe R function t.test() is capable of doing some of the calculations.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>T-test</span>"
    ]
  },
  {
    "objectID": "chapters/week3/hypothesis_testing.html",
    "href": "chapters/week3/hypothesis_testing.html",
    "title": "10  Hypothesis testing",
    "section": "",
    "text": "10.1 Reading material",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/week3/hypothesis_testing.html#reading-material",
    "href": "chapters/week3/hypothesis_testing.html#reading-material",
    "title": "10  Hypothesis testing",
    "section": "",
    "text": "Video on the ideas behind hypothesis testing:\n\nThe Essential Guide To Hypothesis Testing\n\nChapter 3 of Introduction to Statistics by Brockhoff\n\nEspecially chapter 3.1 - 3.1.3 and 3.1.6.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/week3/hypothesis_testing.html#videos",
    "href": "chapters/week3/hypothesis_testing.html#videos",
    "title": "10  Hypothesis testing",
    "section": "10.2 Videos",
    "text": "10.2 Videos\n\n10.2.1 The Essential Guide To Hypothesis Testing - Very Normal\n\n\n\n10.2.2 Exercises\n\n\n\n\n\n\nExercise 10.1\n\n\n\n\n\n\n\n\n\nExercise 10.1 - Hypothesis testing\n\n\n\n\n\n\nThis exercise is conceptual. The idea is, that you should think about a problem, and try to figure out what the null hypothesis is, and further what could be a relevant metric for measuring the distance to this null hypothesis.\n\nYou playing with a six-sided dice, and you are observing abnormally high number of fives. You set up an experiment to test whether the dice is skew, where you trow dice and register the outcome a number of time. What is the null hypothesis in this experiment?\nTwo response variables in a sample of size \\(n\\) seem to track, i.e. is positively correlated. What is the null hypothesis for testing this relation? and what measures the distance to this hypothesis?\nIn an experiment you have a treatment with three levels (placebo, treatment A and treatment B) and some relevant response variable. You are interested in whether there is a difference between the treatments.And in particular whether A is different from placebo and whether B is different from placebo. What is the null-hypothesis for the former question? and what would be a relevant metric for measuring the distance to that hypothesis? What is the null hypothesis for the pairwise comparisons and what further relevant metrics for measuring this distance.\nAccording to theory there is a proportional linear relation between \\(y\\) and \\(x\\). You fit a line between the two (\\(y = a + bx\\)). What is the null hypothesis concerning proportionality? and what measures the distance to this hypothesis?\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.2\n\n\n\n\n\n\n\n\n\nExercise 10.2 - Association or causality?\n\n\n\n\n\n\nThis exercise is intended to show, that you need to be careful with drawing conclussions solely based on statistical numbers (confidence intervals, p-values,…), and that you need to be critical and think about the study design, biology, life, etc.\nA study wants to investigate a certain biomarker in the discovery of cancer. From a population of cancer patients a sample of \\(n = 123\\) patients is taken, and their blood is investigated for a specific biomarker (BMa). The mean and standard deviation of this sample is estimated to \\(\\bar{x} = 3.4\\) mg/L and \\(s_x = 1.5\\) mg/L respectively.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the standard error for the mean of the distribution.\nMake a confidence interval for the mean of the distribution.\n\n\n\n\n\nThe average in this population seems rather high from a biological point of view. However, the researchers want to verify that this is indeed the case, and therefore go out and recruites a population of healthy individuals of size \\(n = 130\\). The discriptive statistics for this group is \\(\\bar{x} = 2.9\\) mg/L and \\(s_x = 1.3\\) mg/L.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a confidence interval for the mean in the healthy population.\nSketch the two population distributions. Are there an overlap?\nSketch the two confidence intervals and contemplate over similarity/differences between these two populations.\n\n\n\n\n\nThe researchers ask the question of whether the two distributions are similar.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFormulate the question as a null- and alternative hypothesis.\nTest the hypothesis, and comment on the question raised.\n\n\n\n\n\nThe answer to the question seems to indicate differences between the two populations. Now the researchers take this one step further, and claims, that this must be due to cancer status.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is problematic in drawing the conclusion, that differences in BMa is caused by cancer status?\n\nHint: Think about study-design, and other differences between the two populations such as age, lifestyle etc.\n\nIn order to be certain about cancer leading to increased levels of BMa, which circumstances must be fullfilled? Is possible to make such studies on humans? Mice?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html",
    "href": "chapters/week4/week_4.html",
    "title": "Week 4",
    "section": "",
    "text": "Hand-in assignment\nIn this week we are going to discuss non-continuous data, and look at binary and count data. This task involves probability distributions such as binomial distribution and the Poisson distribution.\nExercise 11.4 Fig bars is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html#exercises",
    "href": "chapters/week4/week_4.html#exercises",
    "title": "Week 4",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 11.1\nExercise 11.2\nExercise 11.3\n\nIf you really think that this is easy stuff, then try to work through exercise Exercise 11.5\nFor Wednesday work through the following exercises:\n\nExercise 12.1\nExercise 12.2\nExercise 12.3",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/week_4.html#case-ii",
    "href": "chapters/week4/week_4.html#case-ii",
    "title": "Week 4",
    "section": "Case II",
    "text": "Case II\nThe second case should be handed in as a slide-show with voice no later than Thursday evening.",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html",
    "href": "chapters/week4/binomial_data.html",
    "title": "11  Binomial data",
    "section": "",
    "text": "11.1 Reading material",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#reading-material",
    "href": "chapters/week4/binomial_data.html#reading-material",
    "title": "11  Binomial data",
    "section": "",
    "text": "Binomial data - in short\nIntroductory videos to the binomial distribution:\n\nThe Binomial Distribution\nAn Introduction to the Binomial Distribution\nThe Binomial Distribution and Test, Clearly Explained\n\nVideo on probability distribution:\n\nThe Main Ideas behind Probability Distributions\n\nChapter 2.1 and 2.2 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#sec-intro-to-binom",
    "href": "chapters/week4/binomial_data.html#sec-intro-to-binom",
    "title": "11  Binomial data",
    "section": "11.2 Binomial data - in short",
    "text": "11.2 Binomial data - in short\nIn short, binomial data are of the type either / or, and is normally recorded as a list of \\(0\\)’s and \\(1\\)’s. The binomial distribution is characterized by how many trials there is conducted (\\(n\\)) and the probability of a positive (\\(\"1\"\\)) response (\\(p\\)). In notation that is:\n\\[\\begin{equation}\nX \\sim \\mathcal{B}(n,p)\n\\end{equation}\\]\nThe probability density function (pdf) evaluating the probability of \\(x\\) (out of \\(n\\)) positive responses is:\n\\[\\begin{equation}\n%\\binom{N}{K}\nP(X=x) = \\binom{n}{x} \\cdot p^x(1-p)^{n-x}\n\\end{equation}\\]\nWhere\n\\[\\begin{equation}\n%\\binom{N}{K}\n\\binom{n}{x} = \\frac{{n!}}{{x!\\left( {n - x} \\right)!}}\n\\end{equation}\\]\nis the binomial coefficient, which calculates how many combinations of \\(x\\) in \\(n\\) there exists.\nThe cumulative density function (cdf) simply sums op the individual point probabilities.\n\\[\\begin{equation}\n%\\binom{N}{K}\nP(X \\leq x) = P(X=0) + P(X=1) + ... + P(X=x)\n\\end{equation}\\]\nBased on data from \\(n\\) trials with \\(x\\) positive responses, the parameter \\(p\\) can be estimated as the frequency:\n\\[\\begin{equation}\n\\hat{p} = \\frac{x}{n}\n\\end{equation}\\]\nWith the following standard error:\n\\[\\begin{equation}\nS_{p} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\end{equation}\\]\nFrom the Central Limit Theorem follows that the point estimate for \\(p\\) is approximately normally distributed, why the confidence interval is as follows:\n\\[\\begin{equation}\nCI_{p}: \\hat{p} \\pm z_{1-\\alpha/2}S_{p}\n\\end{equation}\\]\nBe aware that the extreme cases where \\(x = 0\\) or \\(x = n\\) does not comply with the above mentioned method for calculating the confidence interval, as \\(S_{p} = 0\\). A surpass in this situation is to find a value for \\(p\\) under which the observed data is still likely. For instance; find \\(p\\) such that:\n\\[\\begin{equation}\nP(X=0) = (1-p)^{n}&gt;\\alpha\n\\end{equation}\\]\n\n\n\n\n\n\nExample 11.1\n\n\n\n\n\n\n\n\n\nExample 11.1 - Quality control - estimation\n\n\n\n\n\n\nAs a production engineer you are responsible for keeping the product quality high, which specifically means that the number of nonconforming products from the your production facility should be low.\nIn order to establish what the rate of nonconforming products is, you choose to randomly select \\(200\\) products and check their quality.\nOf these 6 are nonconforming. So on average \\(3\\%\\).\nHowever, the production facility produces a very large number of elements, and your boss wish to know whether the \\(3\\%\\) holds for the entire production. In order to answer this you calculate a confidence interval:\n\nn &lt;- 200 # Number of samples\nx &lt;- 6 # Number of \"positive\" (non-conforming)\n\npHat &lt;- x / n # Estimate of probability of \"positive\"\n\nseHat &lt;- sqrt(pHat * (1 - pHat) / n) # Standard error of pHat\n\nzFrac &lt;- 1.96 # Critical value of 95% standard normal\n\nlwrCI &lt;- pHat - zFrac * seHat # Lower value of 95% confidence interval\nuprCI &lt;- pHat + zFrac * seHat # Upper value of 95% confidence interval\n\nc(lwrCI, uprCI)\n\n[1] 0.006357817 0.053642183\n\n\nYou return these numbers to your boss, stating that there is probably 3% nonconforming products, and that it is very unlikely that there are more than 5.4% based on the 95% confidence bounds.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#videos",
    "href": "chapters/week4/binomial_data.html#videos",
    "title": "11  Binomial data",
    "section": "11.3 Videos",
    "text": "11.3 Videos\n\n11.3.1 The Binomial Distribution - CrashCourse\n\n\n\n11.3.2 An Introduction to the Binomial Distribution - jbstatistics\n\n\n\n11.3.3 The Binomial Distribution and Test, Clearly Explained - StatQuest\n\n\n\n11.3.4 The Main Ideas behind Probability Distributions - StatQuest",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/binomial_data.html#exercises",
    "href": "chapters/week4/binomial_data.html#exercises",
    "title": "11  Binomial data",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\n\n\n\n\n\n\nExercise 11.1\n\n\n\n\n\n\n\n\n\nExercise 11.1 - Chocolate and binomial data\n\n\n\n\n\n\nA chocholate factory discards 68 chocolates out of 300 produced in a day.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the model for these data\nCalculate the probability of discarding a chocolate (\\(\\hat{p}\\))\nCalculate the confidence interval for \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.2\n\n\n\n\n\n\n\n\n\nExercise 11.2 - Triangle test\n\n\n\n\n\n\nIn sensorical science several types of experiments can be used for measuring (dis-)similarity between products. One of those is the Triangle Test.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCheck out on the internet how this test is conducted.\nAssume that you run a Triangle test with \\(n\\) judges. What is the nature- and statistical model for the data obtained from such a trial?\nState a null hypothesis, relating to the question of similarity of products in the study.\nAssume that you include \\(n = 20\\) judges in your experiment. How many correct answers do you need in order to statistically prove difference between products?\n\nYou need to define what statistically prove means.\nHint: The result is found by trial and error computation - see code below.\n\n\n\n\n\n\n\np &lt;- 1/3 # Parameter under the null hypothesis\nn &lt;- 20 # Number of trials \nx &lt;- 1:n # Define all possible outcomes\n\n# Outcome probability under the null distribution\nnullProb &lt;- 1 - pbinom(x - 1, n, p)\n\n# Put results into dataframe\ndf &lt;- data.frame( \n  nCorrect = x,\n  nullProb\n)\n\n# Plot data\nggplot(df, aes(nCorrect, nullProb)) +\n  geom_line() +\n  geom_hline(yintercept = 0.05) # Alpha value\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nGeneralize this to \\(n = 1,2,3,...,100\\) and plot the results (x-axis: number of trials, y-axis: frequency of correct answers).\n\nHint: You need to put this in a for loop over \\(n\\) and record the least number of samples needed for a significant results within each iteration. The code below can be used as inspiration.\n\n\n\n\n\n\n\np &lt;- 1/3 # Parameter under the null hypothesis\na &lt;- 0.05 # Set the significance threshold \nresults &lt;- c() # Predefine vector for storing results\n\nfor (n in 1:100) {\n  \n  # Define all possible outcomes for this round of the loop\n  x &lt;- 1:n\n  \n  # Outcome probability under the null distribution\n  nullProb &lt;- 1 - pbinom(x - 1, n, p) \n  \n  # Find all x-values where the null prob. is less than alpha\n  underAlpha &lt;- x[nullProb &lt; a]\n  \n  # Find the minimum x where the null prob is less than alpha and divide with n\n  results[n] &lt;- min(underAlpha) / n\n}\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nIn a population you think that \\(40\\%\\) are capable of answering correct for discrimination of two products. The rest (\\(60\\%\\)) will simply just give a blind guess. How large a proportion of correct answers would you expect in such a population?\n\nHint: Think of \\(100\\) persons.\n\nCompare the results for the later two questions, and give a frank idea of the number of participants needed for an experiment with the purpose of discriminating the products.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.3\n\n\n\n\n\n\n\n\n\nExercise 11.3 - Uncertainty of the binomial distribution\n\n\n\n\n\n\nThis exercises examines the relation between the probability parameter \\(p\\) and the uncertainty on this \\(S_p\\) for the binomial distribution.\n\nLet \\(X\\) describe the number of successes out of \\(n\\) trial. Write op the model for \\(X\\)\nLet \\(x\\) be the observed number of successes from such a trial. Estimate the parameter of interest.\nUse the central limit theorem to approximate the standard error on this estimate, and write up the standard error for the parameter.\nDraw the relation between the parameter estimate and the standard error for the same estimate in a graph.\nAt which point is the uncertainty lowest/highest?\n\nHint: You can either solve this analytically by differentiation with respect to \\(\\hat{p}\\) or use the graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.4\n\n\n\n\n\n\n\n\n\nExercise 11.4 - Fig bars\n\n\n\n\n\n\nA company is producing fig and date bars and they are considering changing their date supplier. The company would like to produce fig and date bars with the same taste, smell and appearance as they have done for many years. It is therefore important that a new date supplier will not result in fig and date bars that differ from the original bars. The company asked the Department of Food Science at University of Copenhagen to perform a sensory test with five possible date suppliers. The Department of Food Science performed a triangle test with a trained sensory panel to detect if the new date suppliers would change the organoleptic (in this case smell, taste and sight) properties of the bars. The same \\(23\\) sensory judges performed the triangle test on smell, taste and appearance.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nOpen the file dates.xlsx in Excel or in R. The results of the triangle test were either \\(1\\) for a correct assignment of the deviating sample or \\(0\\) for an incorrect assignment. Get a first impression of the data by looking at the raw data (the \\(1's\\) and \\(0's\\)).\n\nIs there a judge that is good at finding the deviating sample from the smell but bad a finding the deviating sample from the taste?\nIs there a judge that is good at finding the deviating sample from smell, taste and appearance?\n\nImport the data into R by making a data matrix like this (i.e. no need to read the excel file):\n\n\n\n\n\n\n# Import data into matrix\ndates &lt;- matrix(c(10, 11, 14, 19, 11, 9, 9,\n                  16, 9, 9, 18, 14, 22, 16, 11), \n                ncol = 5, byrow = TRUE) \n\n# Set column and row names\ncolnames(dates) &lt;- c(\"A - R\", \"B - R \", \"C - R \", \"D - R \", \"E - R\")\nrownames(dates) &lt;- c( \"Smell\", \"Taste\", \"Appearance\")\n\n# Compute the percent of correct\ndatesPercent &lt;- dates / 23\n\n# Show summary table\nsummary(dates)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nMake a descriptive plot to visualize the data.\n\nYou can for example take inspiration from the plot in exercise 7.23 from Introduction to Statistics by Brockhoff.\nYou can also use ggplot2 by first making dates into a Tidy dataframe: reshape::melt(dates) and then use geom_bar(stat = 'identity').\n\nBy looking at the plotted data, does it seem likely that some of the new date producers could be used to produce fig and date bars that are similar to the reference/original bar?\nState a model for \\(X\\), where \\(X\\) is the number of correct answers in comparing a single product with the reference for a single sense, and state the chance probability. What is the probability of by random chance guessing the correct deviating sample?\nNow, state a null hypothesis and an alternative hypothesis.\n\nIs the alternative hypothesis directional (\\(&gt;\\) or \\(&lt;\\)) or non-directional (\\(\\neq\\))?\nWhat does the three alternatives correspond to in terms of probability of correct answer compared to unqualified guessing?\n\nTest this hypothesis for a triangle test result of \\(19\\) correct and \\(4\\) incorrect assignments (which is the result of the D - R smell triangle test).\n\n\n\n\n\nNote: A number of tests could be applied to test the hypothesis but since the sample size is fairly small then an exact binomial test is a good choice. This is found in the R functionbinom.test() (remember to specify the alternative hypothesis).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nOn the basis of this exercise, which date suppliers would you recommend to the company?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.5\n\n\n\n\n\n\n\n\n\nExercise 11.5 - Distribution of extreme values in the normal distribution\n\n\n\n\n\n\nThis exercise combines the (standard) normal distribution with the binomial distribution in order to calculate the distribution of the maximum (or minimum) value from a sample of size \\(n\\).\n\n\n\n\n\n\nImportant\n\n\n\nThis is a hard exercise, and not a part of the curriculum. However, the ideas used for solving the task is central in a number of applications.\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nConsider a single random draw \\(X\\) from the standard normal distribution. Calculate the probability of getting \\(x\\) or less. That is \\(P(X\\leq x)\\). If it helps, then set \\(x\\) to some specific number, say \\(1.2\\).\nNow consider a draw of size \\(n\\) from the same distribution (\\(X_1,..,X_n\\)). Write up the model for the number of data points less than \\(x\\) (from Q1).\nSet \\(n\\) to a specific number, and calculate the probability of non of the data points being larger than \\(x\\).\nGeneralize the above and write up the distribution for the maximum value in a finite sample from the standard normal distribution.\nSimulate some data, and check that your analytical solution match.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html",
    "href": "chapters/week4/poisson_data.html",
    "title": "12  Poisson data",
    "section": "",
    "text": "12.1 Reading material",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#reading-material",
    "href": "chapters/week4/poisson_data.html#reading-material",
    "title": "12  Poisson data",
    "section": "",
    "text": "Poisson distribution - in short\nA video with an introduction to the Poisson distribution:\n\nAn Introduction to the Poisson Distribution\n\nChapter 2.2 of Introduction to Statistics by Brockhoff\n\nEspecially 2.2.4",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#sec-intro-to-poisson",
    "href": "chapters/week4/poisson_data.html#sec-intro-to-poisson",
    "title": "12  Poisson data",
    "section": "12.2 Poisson distribution - in short",
    "text": "12.2 Poisson distribution - in short\nIn short, the Poisson distribution are characterized by observations that are non-negative integers. That is \\(X \\in (0,1,2,...)\\), where each observation is a in a fixed volume, time, space etc.. I.e. the number of bacteria in \\(1gram\\) of sample or the number of events within one day. The Poisson distribution only have a single parameter \\(\\lambda\\), which is both the mean and the variance of the distribution. Formalized the distribution can be written as:\n\\[\\begin{equation}\nX \\sim Po(\\lambda)\n\\end{equation}\\]\nFor calculation of a point probability, the probability density function (pdf) is as follows:\n\\[\\begin{equation}\nP\\left(X = x \\right) = \\frac{\\lambda ^x {e^{ - \\lambda } }}{{x!}}\n\\end{equation}\\]\nThe cumulative probability function (cdf) is simply the sum over the individual point probabilities (just as in the binomial distribution).\nIn the case where we have data (\\(X_1,X_2,...,X_n\\)) following the Poisson distribution, we can use these to estimate the parameter \\(\\lambda\\). This is simply done by using the mean of \\(X\\).\n\\[\\begin{equation}\n\\hat{\\lambda} = \\bar{X}\n\\end{equation}\\]\nA confidence interval for this parameter is found by using the Central Limit Theorem, which states that the distribution of the mean approximately follow a normal distribution. That is:\n\\[\\begin{equation}\nCI_{\\lambda}: \\hat{\\lambda} \\pm z_{1-\\alpha/2}\\sqrt{\\hat{\\lambda}/n}\n\\end{equation}\\]\nWhere \\(z_{1-\\alpha/2}\\) is a fractile from the standard normal distribution. If \\(\\alpha = 0.05\\), then \\(z_{1-\\alpha/2} = 1.96\\).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#videos",
    "href": "chapters/week4/poisson_data.html#videos",
    "title": "12  Poisson data",
    "section": "12.3 Videos",
    "text": "12.3 Videos\n\n12.3.1 An Introduction to the Poisson Distribution - jbstatistics",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week4/poisson_data.html#exercises",
    "href": "chapters/week4/poisson_data.html#exercises",
    "title": "12  Poisson data",
    "section": "12.4 Exercises",
    "text": "12.4 Exercises\n\n\n\n\n\n\nNote\n\n\n\nSometimes, when solving exercises you end up by doing what you are asked, without understanding why. The next two exercises tries to surpass this, by presenting the task in a short form, from which you should specify the questions which will give answers to the task. It is up to you, but maybe try to use the short version, and see how far that will get you\n\n\n\n\n\n\n\n\nExercise 12.1\n\n\n\n\n\n\n\n\n\nExercise 12.1 - Quality assurance\n\n\n\n\n\n\nShort version\nA production of a food material is checked batch wise for contamination of unwanted bacteria. The procedure is to take out \\(n\\) samples of a given size, innoculate each sample in a relevant media at a relevant temperature for a relevant number of hours, and then check each sample for growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nDerive the underlying distribution of bacteria in the production batch.\nWhich assumptions do you impose? and how would you ensure the validity of these in the sampling procedure?.\n\n\n\n\n\nA batch were sampled following the procedure with \\(n = 5\\) and there were not observed any growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the upper confidence bound for the bacteria concentration in the sample under this observation?\n\n\n\n\n\nYou probably assume that in order to observe growth in a single sample there need to be at least \\(1\\) viral bacteria cell present. However, the way growth is measured is via checking the optical density a method which is not extremly sensitive, why there need to be at least \\(500\\) viral bacteria cells present in original sample in order to detect growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRecalculate the upper confidence bound for the bacteria concentration under this opservation.\n\n\n\n\n\nLong version\nA production of a food material is checked batch wise for contamination of unwanted bacteria. The procedure is to take out \\(n\\) samples of a given size, inoculate each sample in a relevant media at a relevant temperature for a relevant number of hours, and then check each sample for growth.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhich distribution do the number of bacteria cells in each sample follow?\nCalculate the probability of observing growth. I.e. that a sample contain at least a single bacteria cell.\nWhich distribution does the observation of growth/no growth in the \\(n\\) samples follow? and what are the parameters for this distribution?\n\nWhat assumptions naturally follow? and how would you ensure the validity of these in the sampling procedure?\n\n\n\n\n\nA batch were sampled following the procedure with \\(n = 5\\) and there were not observed any growth in any of the \\(5\\) samples.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nEstimate the binomial parameter and calculate a confidence interval for this.\n\nHint: For this extreme case (\\(0\\) positive) you can not use the approximation by the normal distribution. Instead look for a value of \\(p\\) that would produce the observed result with some certainty (you have to specify). You can do this analytically or trial and error using the dbinom() in R.\n\n\nWhat is the upper confidence bound for the bacteria concentration in the sample under this observation?\n\nYou probably assume that in order to observe growth in a single sample there need to be at least \\(1\\) viral bacteria cell present. However, the way growth is measured is via checking the optical density a method which is not extremely sensitive, why there need to be at least \\(500\\) viral bacteria cells present in original sample in order to detect growth.\n\nRecalculate the upper confidence bound for the bacteria concentration under this observation. Hint: This can not be done analytically, so you need to try with different values of \\(\\lambda\\) and see which one that matches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 12.2\n\n\n\n\n\n\n\n\n\nExercise 12.2 - Quality assurance - poisson and binomial distribution\n\n\n\n\n\n\nShort version\nA quality assurance program uses the test described in Exercise 12.1 with \\(n\\) samples. Derive the sensitivity of the test, that is; the probability of detecting contamination, given that is indeed there, as a function of the number of samples and the true concentration in the batch under investigation. Draw some curves visualizing this relation.\nLong version\nA quality assurance program uses the test described in Exercise 12.1 with \\(n\\) samples.\n\nSet \\(\\lambda = 1\\), calculate the binomial parameter for detecting growth in a single sample.\nSet \\(n=5\\), calculate the probability of observing no growth in any of the samples.\nWhat is the sensitivity of the test, that is; the probability of detecting contamination, given that it is indeed there.\nWhat happens if the sample size is changed for instance by a factor of \\(2\\) or \\(1/2\\)?\nGeneralize this for other values of \\(\\lambda\\) and \\(n\\), and draw curves for varying \\(n\\) with sensitivity on the y-axis and \\(\\lambda\\) on the x-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 12.3\n\n\n\n\n\n\n\n\n\nExercise 12.3 - Quality assurance - chance of false rejections\n\n\n\n\n\n\nFor some types of microorganisms (bacteria or yeast) a product is damaged for very low concentrations, due to the possibility of growth during storage. However, a number of organisms are only damaging the product when present in high amounts - if present in low amounts, they do not affect the quality.\nFor a specific type of bacteria a concentration above a value \\(C\\) (measured in CFU/g) leads to a damaged product, that should not be put on market, whereas a concentration below this value results in a good product, at least within the labeled shelf life.\nYou have the responsibility for quality assurance at your company. In order to be able to export to Japan and USA (those countries are the most pernickety with respect to product safety) you need to document a thorough eigen-control system. So you set up a procedure.\nThis procedure is set up to measure the concentration in a product sample. That is, a predefined number of samples (\\(n\\)) is sampled from the batch, diluted, spread on plates, incubated and the number of colony forming units are counted. That leads to the observations: \\(X_1,X_2,\\ldots,X_n\\).\n\nBased on these observations give an estimate of the concentration in the entire batch.\nAdditionally, give a confidence interval for this concentration, where you approximate the distribution of the mean concentration with a normal distribution (using the central limit theorem).\nWhich of the three numbers (central estimate, lower and upper confidence bound) do you think is essential for determination of whether the batch is ok or not?\nWhich rule would you suggest for making this decision?\nWhat is the change of rejecting an ok product under this rule?\nSimulate the scenario for varying concentration parameters, varying sample size (\\(n\\)) and determine the rate of rejection of ok batches.\n\nFor construction of random Poisson data use the function rpois() of fictious samples.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Poisson data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html",
    "href": "chapters/week5/week_5.html",
    "title": "Week 5",
    "section": "",
    "text": "Hand-in assignment\nIn this week we are going to discuss a last subject for non continuous data, that is, multinomial data and the multinomial distribution. The Chi squared test is a central test for discrete data types and can be used for inferential testing. Further, we are going to deal with the notion of power, that is; if there truly is a difference, how likely is it that the statistical tests will find it?\nExercise 14.4 Power calculation - triangle test is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html#exercises",
    "href": "chapters/week5/week_5.html#exercises",
    "title": "Week 5",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 14.1\nExercise 14.2\n\nFor Wednesday work through the following exercises:\n\nExercise 14.4\nExercise 14.5\n\nFurther, this week might allow you to recap on some of the exercises you did not make during the last weeks.",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/week_5.html#case-iii",
    "href": "chapters/week5/week_5.html#case-iii",
    "title": "Week 5",
    "section": "Case III",
    "text": "Case III\nThe third case should be handed in as a slide-show with voice no later than Thursday evening next week.",
    "crumbs": [
      "Week 5"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html",
    "href": "chapters/week5/mutinomial_data.html",
    "title": "13  Multinomial data",
    "section": "",
    "text": "13.1 Reading material",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#reading-material",
    "href": "chapters/week5/mutinomial_data.html#reading-material",
    "title": "13  Multinomial data",
    "section": "",
    "text": "Multinomial data - in short\nA video going through \\(\\chi^2\\)-test (Goodness of fit test) for frequency tables:\n\nChi-Square Tests\n\nChapter 7 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#sec-intro-to-multinomial",
    "href": "chapters/week5/mutinomial_data.html#sec-intro-to-multinomial",
    "title": "13  Multinomial data",
    "section": "13.2 Multinomial data - in short",
    "text": "13.2 Multinomial data - in short\nMultinomial data are categorical data with more than two groups. If there is only two groups, the data follow the binomial distribution. These data are naturally organized in a so-called frequency- or contingency table (see for example Exercise 14.1). In general terms such a table with \\(n\\) rows and \\(k\\) columns can be shown as in table 13.1.\n\n\n\nTable 13.1: A table of multinomial data. Each column is a category and each row is a sample\n\n\n\n\n\n\nCategory I\nCategory II\n\\(\\cdots\\)\nCategory \\(k\\)\n\n\n\n\nSample I\n\\(N_{11}\\)\n\\(N_{12}\\)\n\\(\\cdots\\)\n\\(N_{1k}\\)\n\n\nSample II\n\\(N_{21}\\)\n\\(N_{22}\\)\n\\(\\cdots\\)\n\\(N_{2k}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\nSample \\(n\\)\n\\(N_{n1}\\)\n\\(N_{n2}\\)\n\\(\\cdots\\)\n\\(N_{nk}\\)\n\n\n\n\n\n\nSuch data can be collected in two ways following two slightly different models.\nOne distribution\n\\(N\\) samples, that are put into \\(nk\\) categories. For example, \\(256\\) people are selected and categorized according to gender and color of hair. The model for this case is:\n\\[\nN_{11},N_{12},...,N_{nk} \\sim Multinomial(N,p_{11},p_{12},...,p_{nk})\n\\]\nwhere\n\\[\nN = N_{11} + N_{12} + ... + N_{nk} \\quad \\text{and} \\quad p_{11} + p_{12} + ... + p_{nk} = 1$.\n\\]\ni.e. one distribution\nSeveral distributions\nSeveral \\(N_i\\) samples, that are put into \\(k\\) categories. In this case, we predefine the number of samples within each row and distribute those over the categories. For example, \\(100\\) men and \\(123\\) women, distributed on color of their hair. The model for this case is:\n\\[\nN_{i1},N_{i2},...,N_{ik} \\sim Multinomial(N_{i},p_{i1},p_{i2},...,p_{ik})\n\\]\nwhere\n\\[\nN_{i} = N_{i1} + N_{i2} + ... + N_{ik} \\quad \\text{and} \\quad p_{i1} + p_{i2} + ... + p_{ik} = 1$ for $i = 1,...,n.\n\\]\ni.e. several (\\(n\\)) distributions.\nThe natural question for both types of data is whether there is independence between the columns (or rows). For the example that is; Is the distribution similar regardless of gender. However, the null hypothesis is stated differently depending on the model.\n\n\\(H0:\\) \\(p_{ij} = p_{i \\cdot}p_{\\cdot j}\\) where \\(p_{i \\cdot}\\) refers to the row probability (i.e. the probability of having gender \\(i\\)), and \\(p_{\\cdot j}\\) refers to the column probability (i.e. the probability of having hair color \\(j\\)).\n\\(H0:\\) \\(p_{1j} = p_{2j} = ... = p_{nj} = p_{\\cdot j}\\) i.e. equal probability of hair color across gender.\n\nIn both cases the test is the same, and based on calculating the expected number of observations under the null hypothesis, and comparing those with the observed number of observations. If this number is large, then there is large differences between the observed and the expected, why the null hypothesis is rejected.\n\n\n\nTable 13.2: A table of multinomial data with row and column sums.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategory I\nCategory II\n\\(\\cdots\\)\nCategory \\(k\\)\n\n\n\n\n\nSample I\n\\(N_{11}\\)\n\\(N_{12}\\)\n\\(\\cdots\\)\n\\(N_{1k}\\)\n\\(N_{1 \\cdot} = \\sum{N_{1j}}\\)\n\n\nSample II\n\\(N_{21}\\)\n\\(N_{22}\\)\n\\(\\cdots\\)\n\\(N_{2k}\\)\n\\(N_{2 \\cdot} = \\sum{N_{2j}}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nSample \\(n\\)\n\\(N_{n1}\\)\n\\(N_{n2}\\)\n\\(\\cdots\\)\n\\(N_{nk}\\)\n\\(N_{n \\cdot} = \\sum{N_{nj}}\\)\n\n\n\n\\(N_{\\cdot 1} = \\sum{N_{i1}}\\)\n\\(N_{\\cdot 2} = \\sum{N_{i2}}\\)\n\\(\\cdots\\)\n\\(N_{\\cdot k} = \\sum{N_{ik}}\\)\n\\(N_{\\cdot \\cdot} = \\sum{N_{ij}}\\)\n\n\n\n\n\n\nThe expected value \\(E\\) for each cell in the table is calculated as:\n\\[\\begin{equation}\nE_{ij} = \\frac{N_{i \\cdot} N_{\\cdot j}}{N_{\\cdot \\cdot}}\n\\end{equation}\\]\nI.e. the row sum multiplied with the column sum and divided by the total sum (see table 13.2). The test statistic \\(X^2_{obs}\\) is calculated as:\n\\[\\begin{equation}\nX^2_{obs} = \\sum_{ij}{\\frac{(E_{ij} - N_{ij})^2}{E_{ij}}}\n\\end{equation}\\]\nI.e. the (squared) discrepancy between the expected (\\(E_{ij}\\)) and the observed (\\(N_{ij}\\)) divided by the expected value. Summed across all cells.\nUnder the null hypothesis \\(X^2_{obs}\\) follow a so-called chi-squared distribution (\\(\\chi^2\\)) with \\(df = (n-1)(k-1)\\) degrees of freedom. The P-value is one-sided:\n\\[\\begin{equation}\nP = P(\\chi^2_{df} &gt; X^2_{obs})\n\\end{equation}\\]\nOBS: Be aware that too low expected values (\\(E_{ij}\\)) makes this test unstable. A rule of thumb is that \\(80\\%\\) of the cells should be above \\(5\\) and ALL should be above \\(1\\). In the case where this is violated, cells can be merged by summing both the expected and observed values.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#videos",
    "href": "chapters/week5/mutinomial_data.html#videos",
    "title": "13  Multinomial data",
    "section": "13.3 Videos",
    "text": "13.3 Videos\n\n13.3.1 Chi-Square Tests - CrashCourse",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/mutinomial_data.html#exercises",
    "href": "chapters/week5/mutinomial_data.html#exercises",
    "title": "13  Multinomial data",
    "section": "13.4 Exercises",
    "text": "13.4 Exercises\n\n\n\n\n\n\nExercise 13.1\n\n\n\n\n\n\n\n\n\nExercise 13.1 - Comparison of senses\n\n\n\n\n\n\nA study wants to compare two types of trout samples, being meat stored under different conditions. The instrument used is a sensorical panel of \\(23\\) judges using either their visual sense, smelling sense or tasting sense. At each trial, each judges is presented with three pieces of meat - two similar and one odd. The task for the judge is to identify the odd sample using one of the senses. Data from such an experiment is presented below (table 13.3).\n\n\n\nTable 13.3: Counts of outcomes for each sense.\n\n\n\n\n\n\n\nCorrect\nNot correct\n\n\n\n\nSmell\n14\n9\n\n\nTaste\n16\n7\n\n\nVisual appearance\n22\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFor now, stick to the sense Taste. State a statistical model for the outcome of each trial.\nFormulate a null hypothesis based on the model, and test how different the observed results are compared to this hypothesis.\n\n\n\n\n\nBased on the previous result, it seems like the different meat pieces is identifiable based on tasting. Now the questions is whether the two other senses performs similar in identification of the odd sample? (Hint: In this exercise you should use the method sketched in Method 7.19 and Method 7.21 in the eNotes).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nGive a frank ranking of the senses based on the observed data.\nFormulate a model for each of the three senses.\nState a null hypothesis in relation to the question of similarity between senses.\nCompute by hand the expected values under this null hypothesis, \\(X^2_{obs}\\) and the degrees of freedom.\n\nUse the pchisq() to test the null hypothesis.\nTest the hypothesis using a function in R (try to figure out which one that does the job in a single line). Compare the results with your own calculation.\n\nReport the results in such a way, that differences between the three senses are communicated.\n\nHint: This can either be done by pairwise contrasts or confidence intervals for the central parameters.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Multinomial data</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html",
    "href": "chapters/week5/power_calculation.html",
    "title": "14  Power calculation",
    "section": "",
    "text": "14.1 Reading material",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html#reading-material",
    "href": "chapters/week5/power_calculation.html#reading-material",
    "title": "14  Power calculation",
    "section": "",
    "text": "Power calculation - in short\nVideos on power calculation:\n\nPlaying with Power: P-Values Pt 3\nPower Analysis, Clearly Explained\nVisualizing a Power Calculation\n\nA video on how to do power calculation in R:\n\nCalculating Power in R\n\nChapter 3.2.4 and 3.1.9 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html#sec-intro-to-power",
    "href": "chapters/week5/power_calculation.html#sec-intro-to-power",
    "title": "14  Power calculation",
    "section": "14.2 Power calculation - in short",
    "text": "14.2 Power calculation - in short\n\n\n\n\n\n\nExample 14.1\n\n\n\n\n\n\n\n\n\nExample 14.1 - Quality control - power calculation\n\n\n\n\n\n\nThis example extends Example 11.1.\nThe \\(3\\%\\) nonconforming products is unsatisfactory, and so you use a lot of money and (hopefully) make a lot of improvements. After a year you and your colleagues feel that the end quality has improved, and you wish to test that this improvement is indeed also statistically provable.\nYou believe that the number of nonconforming products is reduced by a factor of \\(2\\), that is down to \\(1.5\\%\\) nonconforming products.\nThe question is: In a trial, how many samples (\\(n\\)) should you select in order to statistically prove this change?.\nThe null hypothesis is \\(H0: p = p_0 = 0.03\\).\nWith the one-sided alternative: \\(HA: p &lt; p_0 = 0.03\\).\nFrom a study on say \\(n = 200\\) (\\(X_{HO} \\sim \\mathcal{B}(n,p = 0.03)\\)) with \\(x\\) nonconforming products the probability for accepting \\(H0\\) is:\n\\[\\begin{equation}\nP(X_{HO} \\leq x) \\geq \\alpha = 0.05\n\\end{equation}\\]\n\nn &lt;- 200\nx &lt;- 0:3\npbinom(x, n, 0.03)\n\n[1] 0.002261241 0.016248299 0.059290946 0.147151194\n\n\nSo, in order to reject \\(H0\\) (at level \\(\\alpha = 0.05\\)) you should among \\(200\\) samples find \\(0\\) or \\(1\\) nonconforming products (at \\(x = 2\\), the p-value is \\(p = 0.059\\)).\nIf you believe that the true probability is \\(p = 0.015\\) (\\(X_{HA} \\sim \\mathcal{B}(n,p = 0.015)\\)) then the probability for getting this number of positive samples (or what is even more extreme compared to \\(H0\\)) can be calculated as\n\\[\\begin{equation}\nP(X_{HA} \\leq 1) = P(X_{HA}=0) + P(X_{HA}=1).\n\\end{equation}\\]\n\nn &lt;- 200\nx &lt;- 1\npbinom(x, n, 0.015)\n\n[1] 0.1968966\n\n\nThis is the power of the trial under these assumptions. In detail that means, that only one out of five times you would be able to prove that the investment were worth the effort.\nIf you which to have a higher power for the study, then the number of samples should be increased e.g. to \\(n = 400\\):\n\nalpha &lt;- 0.05\nn &lt;- 400 # Number of trials\nx &lt;- 0:100 # Number of successes\n\n# The probability of outcome x under the null hypothesis that p = 0.03\nnullProb &lt;- pbinom(x, n, 0.03) \n\n\n# The maximum x-value that returns a probability smaller than alpha\nxMax &lt;- max(x[nullProb &lt; alpha])\n\npbinom(xMax, n, 0.015)\n\n[1] 0.6063058\n\n\nOr assessed for a sequence of \\(n\\) values (figure 14.1).\n\nalpha &lt;- 0.05\nN &lt;- 1:800 # Number of trials\npwr &lt;- c() # Initiate vector for storage\n\n\nfor (n in N) {\n  \n  x &lt;- 0:n\n  \n  nullProb &lt;- pbinom(x, n, 0.03)\n  \n  xMax &lt;- max(x[nullProb &lt; alpha])\n  pwr[n] &lt;- pbinom(xMax, n, 0.015)\n}\n\nplot(N, pwr)\n\n\n\n\n\n\n\nFigure 14.1: Power vs number of trials.\n\n\n\n\n\nIt is seen that indeed a very high number of samples needs to be collected in order to be fairly certain that the trial will statistically confirm that the number of nonconforming products has dropped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.2 - Effect of caffeine on activity - power\n\n\n\n\n\n\nThe test in Example 9.1 reveal a p-value of p = 0.12 (see test below), which is not to be considered as strong support/evidence of a difference between the two groups. However, we do believe that there should be a difference between whether a mice is given water or red bull on their level of activity, and hence speculates that the study design is too small to be able to show a statistical significant difference between the two groups.\n\nt.test(x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"],\n       x_subset[x_subset$Caffeine == \"Red Bull\", \"RPM7\"],\n       var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  x_subset[x_subset$Caffeine == \"Water\", \"RPM7\"] and x_subset[x_subset$Caffeine == \"Red Bull\", \"RPM7\"]\nt = -1.615, df = 18, p-value = 0.1237\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.5208153  0.4604011\nsample estimates:\nmean of x mean of y \n 8.649382 10.179589 \n\n\nThe null hypothesis assumes equal means between the two groups. However, if we believe that this is not true. I.e. that there is a difference of say 2 points, then given a sample size of \\(n \\ ( = n1 + n2)\\) what is the chance that the study turns out to give data that accepts (or rejects) the null hypothesis of no difference?\nTo answer this, we simulate a bunch of trials and check how many times \\(H0\\) is accepted:\n\nm1 &lt;- 8 # Mean of group 1\nm2 &lt;- 10 # Mean of group 2\n\ns &lt;- 2.1 # Standard deviation in both groups\n\nn1 &lt;- n2 &lt;- 10 # Number of observations in each group\n\nset.seed(2000) # Set seed for reproducibility of simulation\n\nx1 &lt;- rnorm(n1, m1, s) # Simulated normal distributed data for group 1\nx2 &lt;- rnorm(n2, m2, s) # Simulated normal distributed data for group 2\n\nt.test(x1, x2, var.equal = T)$p.value # p-value from t-test\n\n[1] 0.08542594\n\n\nIn this first trial \\(p = 0.085\\) and \\(H0\\) is hence accepted.\nNow, let us simulate this 1000 times to see how often the null-hypothesis is accepted.\n\nk &lt;- 1000 # Number of simulations\np_values &lt;- numeric(k) # Initialize vector for storage of values\n\n\nfor (i in 1:k) {\n  \n  x1 &lt;- rnorm(n1, m1, s) # Simulated normal distributed data for group 1\n  x2 &lt;- rnorm(n2, m2, s) # Simulated normal distributed data for group 2\n  \n  p_values[i] &lt;- t.test(x1, x2, var.equal = T)$p.value # p-value from t-test\n  \n}\n\nWe now have a vector of p-values from 1000 simulations.\n\nhist(p_values)\n\n\n\n\n\n\n\n\nWe can calculate how big a fraction of these simulated p-values that are under 0.05. That is, how many times the null-hypothesis was rejected.\n\nsum(p_values &lt; 0.05) / k\n\n[1] 0.501\n\n\nFor the \\(1000\\) repeated trials half of them (\\(501\\) out of \\(1000\\)) accepts \\(H0\\) even thoug WE KNOW that the two groups are different (as they were simulated to be).\nAs such this is unacceptable, and we need to increase the power of our study.\nThere are several ways to increase the power:\n\nThe two groups can be selected to be even more different (increse the effect size).\nThe spread could be lowered.\nThe number of observations (\\(n1\\) and \\(n2\\)) could be increased.\n\nThe function in R power.t.test() calculates the power as a function of these different settings.\n\npower.t.test(delta = m2 - m1, sd = s, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 18.31862\n          delta = 2\n             sd = 2.1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSo, in order to achieve a power of \\(0.8\\) with the current population settings (mean and std) at least \\(18.3\\) samples (\\(n1 = n2 = 18.3\\)) should be included in each group of the study.\nA more generic overview can also be achieved, where the power is calculated based on effect size (x-axis) and number of samples.\n\neffect_sizes &lt;- seq(-5, 5, by = 0.01) # Vector of effect sizes (delta)\nsample_sizes &lt;- c(5, 10, 20, 50, 100) # Vector of sample sizes (n)\n\npwr_df &lt;- data.frame() # Empty data frame for storage\n\nfor (n in sample_sizes) { # Calculate for each sample size\n\n  pwr &lt;- power.t.test(delta = effect_sizes, sd = s, n = n)$power\n    \n  # Add data row by row\n  pwr_df &lt;- rbind(pwr_df, \n                  data.frame(delta = effect_sizes, N = n, power = pwr))\n  \n}\n\n# Plot data\nggplot(pwr_df, aes(delta, power, color = factor(N))) +\n  geom_line() +\n  labs(\n    x = \"Effect size\",\n    y = \"Power\",\n    color = \"Sample size\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html#videos",
    "href": "chapters/week5/power_calculation.html#videos",
    "title": "14  Power calculation",
    "section": "14.3 Videos",
    "text": "14.3 Videos\n\n14.3.1 Playing with Power: P-Values Pt 3 - CrashCourse\n\n\n\n14.3.2 Power Analysis, Clearly Explained - StatQuest\n\n\n\n14.3.3 Visualizing a Power Calculation - NCSS Statistical Software\n\n\n\n14.3.4 Calculating Power in R - Ed Boone",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/week5/power_calculation.html#exercises",
    "href": "chapters/week5/power_calculation.html#exercises",
    "title": "14  Power calculation",
    "section": "14.4 Exercises",
    "text": "14.4 Exercises\n\n\n\n\n\n\nExercise 14.1\n\n\n\n\n\n\n\n\n\nExercise 14.1 - Comparison of senses\n\n\n\n\n\n\nA study wants to compare two types of trout samples, being meat stored under different conditions. The instrument used is a sensorical panel of 23 judges using either their visual sense, smelling sense or tasting sense. At each trial, each judges is presented with three pieces of meat - two similar and one odd. The task for the judge is to identify the odd sample using one of the senses. Data from such an experiment is presented in table 14.1.\n\n\n\nTable 14.1: Data from a sensory study using 23 judges.\n\n\n\n\n\n\n\nCorrect\nNot correct\n\n\n\n\nSmell\n14\n9\n\n\nTaste\n16\n7\n\n\nVisual appearance\n22\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFor now, stick to the sense Taste. State a statistical model for the outcome of each trial.\nFormulate a null hypothesis based on the model, and test how different the observed results are compared to this hypothesis.\n\n\n\n\n\nBased on the previous result, it seems like the different meat pieces is identifiable based on tasting. Now the questions is whether the two other senses performs similar in identification of the odd sample? (Hint: In this exercise you should use the method sketched in Method 7.19 and Method 7.21 in Brockhoff.)\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nGive a frank ranking of the senses based on the observed data.\nFormulate a model for each of the three senses.\nState a null hypothesis in relation to the question of similarity between senses.\nCompute by hand the expected values under this null hypothesis, \\(\\chi^2_{obs}\\) and the degrees of freedom.\nUse pchisq() to test the null hypothesis.\nTest the hypothesis using a function in R (try to figure out which one that does the job in a single line) - compare the results with your own calculation.\nReport the results in such a way, that differences between the three senses are communicated.\n\nHint: This can either be done by pairwise contrasts or confidence intervals for the central parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 14.2\n\n\n\n\n\n\n\n\n\nExercise 14.2 - Our sensorical trial\n\n\n\n\n\n\nIn September 2015 the students in food science conducted two sensorical trials. A triangle test and a duo trio test. Both on apple juice for discrimination between normal juice and juice with added citric acid. In table 14.2 is listed the results from the 69 judges in the study.\n\n\n\nTable 14.2: Results of the two sensorical trials.\n\n\n\n\n\n\n\nN\nCorrect\n\n\n\n\nDuo-Trio\n29\n12\n\n\nTriangle\n40\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the chance probability for answering correct in the two tests?\nInitially, inspect data to see how the frequencies of correct answers compare with the chance probability. Can you draw conclusions without doing any stats?\nState two models for the data obtained from the Duo-trio test and from the triangle test.\nState null- and alternative hypothesis for the relevant data.\nCalculate the probability of observing the results (and what is more extreme compared to the null hypothesis) under the null-assumption.\nIn case of rejection of the null hypothesis, give a estimate (and confidence interval) for the parameter.\nHow big is the proportion of people who are actually able to taste differences based on this result?\n\nHint: you need to take into account the amount of people who gives correct answer by guessing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 14.3\n\n\n\n\n\n\n\n\n\nExercise 14.3 - Power calculation - triangle test\n\n\n\n\n\n\nA chocolate company have a product, which they want to improve the quality of. In order to do so, they change some of the raw material to a more expensive alternative. In order to test whether this change actually ”pays of” they set up an experiment - A triangle test. Among a set of consumers (\\(n\\)), each consumer is presented to three pieces of chocolate, two of type \\(A\\) and one of type \\(B\\), with the task of finding type \\(B\\). The recorded data from such an experiment may look like:\n\\[\n0,0,1,0,1,1,...0\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWrite up the statistical model underlying these data.\nIf it is not possible to identify type \\(B\\), what is the value of the parameter?\n\n\n\n\n\nA trial based on responses from 20 consumers results in half (\\(x = 10\\)) correctly identified.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat is the probability of observing such a results (or more extreme) under the null hypothesis of no differences between type \\(A\\) and type \\(B\\)?\n\n\n\n\n\nThe product manager is not satisfied with the results, after all half of the consumers were able to distinguish he says. You think 50% - Aaahhh that is not actually what the data says - some of them were random!, but you leave it there, as your boss is from CBS and basically without any technical insight!, so instead you assumme that this is indeed the case for the entire population (that the probability of selecting type \\(B\\) is \\(p = 0.5\\)).\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nHow many customers should you include in your trial in order to have a fair chance of achieving a significant result?\n\nHint i: Calculate for \\(n = 20, 21, ...\\) the least number of correct findings in order for the results to be significant under the null hypothesis. I.e. what is \\(x\\) in \\(P(X ≥ x) &lt; 0.05\\) under \\(p = p0\\) (the guessing probability).\nHint ii: Calculate the probability of observing this (and more extreme) given the population probability of \\(p = 0.5\\).\n\nPlot the combination of number of customers (\\(n\\)) and the probability of getting significant results, and device a new trial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 14.4\n\n\n\n\n\n\n\n\n\nExercise 14.4 - Triangle or Duo-Trio?\n\n\n\n\n\n\nThere exist a bunch of sensorical discrimination test, which all have the same aim, but are different in setup. In this exercise we are going to deal with the power of Triangle test and Duo-Trio test.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the statistical model for the two test types.\nState the chance probabilities, and formulate null- and alternative hypothesis, for both tests.\nNow assume you conduct both trials \\(n = 20\\) times. What is the least number of correct answers needed to get significant discrimination between the products.\nAssume that 50% of the people in the population are actually able to discriminate the samples. What is the expected frequency of correct answers under this assumption?\n\nHINT: think of 100 persons.\n\nCalculate the power for a Duo-trio and a Triangle test with \\(n = 20\\) assuming these populations probabilities.\nComment on why somebody still uses the Duo-trio test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 14.5\n\n\n\n\n\n\n\n\n\nExercise 14.5 - Power calculation in T-test\n\n\n\n\n\n\nShort version\nYou have been able to isolate a fiber from a novel source. With the increasing lifestyle related health problems, you think that your new fiber might actually* save the world*. You just need to prove it. In order to do so, you set up an experiment similar to the one described in “Fiber and Cholesterol”. I.e. a paired setup with baseline measurements, followed by an intervention period and end-of-trial measurements. You will have proven your case if the fiber supplemented to the diet is able to lower the cholesterol level. The question is: How many patients do you need in order to run such a trial?\nLong version\nYou have been able to isolate a fiber from a novel source. With the increasing lifestyle related health problems, you think that your new fiber might actually* save the world*. You just need to prove it. In order to do so, you set up an experiment similar to the one described in “Fiber and Cholesterol”. I.e. a paired setup with baseline measurements, followed by an intervention period and end-of-trial measurements. You will have proven your case if the fiber supplemented to the diet is able to lower the cholesterol level. The question is: How many patients do you need in order to run such a trial?\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nBased on your knowledge on fiber and health give some numbers for effect size, and standard deviation.\n\nHint: You might use the results from a previous similar study to get reliable numbers.\n\nWrite up the test statistics for this experiment.\nCalculate the test statistics for varying number of included persons, with the parameters (effect size and standard deviation) fixed.\nUse the function power.t.test() to calculate how many persons are needed for a trial with power of \\(\\beta = 0.80\\) and a significance threshold of \\(\\alpha = 0.05\\).",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Power calculation</span>"
    ]
  },
  {
    "objectID": "chapters/week6/week_6.html",
    "href": "chapters/week6/week_6.html",
    "title": "Week 6",
    "section": "",
    "text": "Hand-in assignment\nIn this week we are going to expand on inferential statistics by introdcuing the very general notion of statistical model formulation. In short, that is to set up an equation or function that specifies how the observed response is made up. In addition we are going to work with the analysis of variance (ANOVA) and the associated F statistics and distribution.\nExercise 16.3 Diet and fat metabolism - ANOVA question 1 to 6 is to be handed in (through absalon or as hard-copy Friday night).\nQuestion 7 to 11 verifies the questions, so please use those for checking your results - however, we DO NOT correct these if you include them in the assignment.\nYou are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 6"
    ]
  },
  {
    "objectID": "chapters/week6/week_6.html#exercises",
    "href": "chapters/week6/week_6.html#exercises",
    "title": "Week 6",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 16.1 (question 1 to 3) \nExercise 16.2 \n\nFor Wednesday work through the following exercises:\n\nExercise 16.2 (question 4 to 5) \nExercise 16.5 \nExercise 16.6 \nExercise 16.7 \n\n\n\n\n\n\n\nImportant\n\n\n\nExercise 16.7 includes more than two factors and further interaction between those - this is not part of the curriculum, but being able to compute the numbers in an ANOVA table is.",
    "crumbs": [
      "Week 6"
    ]
  },
  {
    "objectID": "chapters/week6/week_6.html#case-iii",
    "href": "chapters/week6/week_6.html#case-iii",
    "title": "Week 6",
    "section": "Case III",
    "text": "Case III\nThe third case should be handed in as a slide-show with voice no later than Friday evening.",
    "crumbs": [
      "Week 6"
    ]
  },
  {
    "objectID": "chapters/week6/model_formulation.html",
    "href": "chapters/week6/model_formulation.html",
    "title": "15  Model formulation",
    "section": "",
    "text": "15.1 Reading material",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model formulation</span>"
    ]
  },
  {
    "objectID": "chapters/week6/model_formulation.html#reading-material",
    "href": "chapters/week6/model_formulation.html#reading-material",
    "title": "15  Model formulation",
    "section": "",
    "text": "Chapter 3.1.10 and 8 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model formulation</span>"
    ]
  },
  {
    "objectID": "chapters/week6/anova.html",
    "href": "chapters/week6/anova.html",
    "title": "16  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "16.1 Reading material",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/week6/anova.html#reading-material",
    "href": "chapters/week6/anova.html#reading-material",
    "title": "16  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Analysis of Variance (ANOVA) - in short\nA brief video going through the concepts of both one-way and two-way ANOVA:\n\nAnalysis of Variance (ANOVA)\n\nA video going through how to do ANOVA in R:\n\nANOVA in R\n\nChapter 8 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/week6/anova.html#sec-anova-intro",
    "href": "chapters/week6/anova.html#sec-anova-intro",
    "title": "16  Analysis of Variance (ANOVA)",
    "section": "16.2 Analysis of Variance (ANOVA) - in short",
    "text": "16.2 Analysis of Variance (ANOVA) - in short\n\n16.2.1 Model formulation\nDepending on the notes, there are several ways to write up the data for an ANOVA model. The following three versions are exactly similar for a setup with \\(k\\) groups, and possibly not equal number of observations within each group. \\\nVersion 1:\n\\[\\begin{aligned}\nX_{11},X_{12},X_{13},...,X_{1n_1} &\\sim \\mathcal{N}(\\mu_1,\\sigma^2) \\\\\nX_{21},X_{22},X_{23},...,X_{2n_2} &\\sim \\mathcal{N}(\\mu_2,\\sigma^2) \\\\\n& \\vdots \\\\\nX_{k1},X_{k2},X_{k3},...,X_{kn_k} &\\sim \\mathcal{N}(\\mu_k,\\sigma^2)\n\\end{aligned}\\]\nVersion 2: \\[\\begin{gather}\nX_{ij}\\sim  \\mathcal{N}(\\mu_i,\\sigma^2)\\\\\n\\text{for } j=1,...,n_i \\text{ and } i=1,..,k \\nonumber\n\\end{gather}\\]\nVersion 3: \\[\\begin{gather}\nX_{ij} = \\mu_i + e_{ij} \\nonumber \\\\\n\\text{where } e_{ij}\\sim \\mathcal{N}(0,\\sigma^2) \\text{ and  independent} \\\\\n\\text{for } j=1,...,n_i \\text {and } i=1,..,k \\nonumber\n\\end{gather}\\]\nFor all three versions, the parameters describing the groups (\\(\\mu_i\\) for \\(i=1,..,k\\)) and the dispersion within the groups (\\(\\sigma\\)) appear, and are exactly similar. Naturally, all three versions are correct, but in order to harmonize with notation for ANOVA with several factors and regression we stick with version 3.\nFor these models it is assumed that the residuals are coming from the same normal distribution. That is: \\(e_{ij}\\sim \\mathcal{N}(0,\\sigma^2)\\). By construction, this distribution has mean equal to zero. Further it is assumed that these residuals are independent, that is: Changing one will not affect the others. If these two assumptions (same distribution and independent) are not fulfilled, the model is not valid, and we cannot trust the tests. Therefor this is checked by visual inspection of the residuals for normality (qq-plot, histogram, residuals vs. predicted value etc.) and independence (residuals vs. predicted value, line-plotting). As this is a very inherent part of model validation, it is made easy in R, where plot() on the object created by aov() or lm() produces graphics which can be used directly; fast’n’easy.\n\n\n16.2.2 Hypothesis\n\\(\\mu_1,..,\\mu_k\\) describes the center of each of the \\(1,..,k\\) samples, and is naturally what is interesting to compare. Usually we are looking for differences between these means, why the null hypothesis is:\n\\[\\begin{equation}\nH0: \\mu_1 = \\mu_2 = ... =  \\mu_k\n\\end{equation}\\]\nwith the alternative, that at least one group .\n\n\n16.2.3 ANOVA table and test\nIf we have two groups, the natural choice would be to use the differences between the observed means. However, for \\(k\\) groups there are \\(k(k-1)/2\\) combinations, why this approach would not lead to a single test, but merely \\(k(k-1)/2\\) comparisons.\nAs an alternative for the differences between the observed means \\(\\bar{X}_1,...,\\bar{X}_k\\), the variance across these numbers are used (only totally true for groups of equal size), that is: \\(MS_{bewteen} = var(\\bar{X}_1,...,\\bar{X}_k)\\). If there is large differences between the group means, then this measure \\(MS_{bewteen}\\) is big, and the null hypothesis should be discarded. Formalized, this needs to be compared to the variance within the groups, that is the average spread around the points.\nIt turns out, that this process can be seen as a partitioning (splitting) of the total variance in the entire dataset into\n\nhow much can be ascribed to the fact that samples are from different groups, and\n\nhow much which is caused by natural variation within the groups.\n\nA natural way to organize these variances is in an ANOVA table. That is a table including the Sum of Squares (SSQ), Degrees of Freedom (DF), Mean Square (MS), observed F-statistic (\\(F_{obs}\\) ), and \\(p\\)-value for each F-statistic \\(Pr(F_{df_A,df_e}&gt;F_{obs})\\). table 16.1 corresponds to a one-way ANOVA table (an analysis with one factor).\n\n\n\nTable 16.1: ANOVA table for one-way analysis of variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSSQ\nDF\nMS\n\\(F_{obs}\\)\n\\(Pr(F_{df_A,df_e}&gt;F_{obs})\\)\n\n\n\n\nFactor(A)\n\\(SS_A\\)\n\\(df_A = k-1\\)\n\\(MS_A = SS_A/df_A\\)\n\\(F_A = MS_A/MS_e\\)\n\\(p_A\\)\n\n\nResiduals\n\\(SS_e\\)\n\\(df_e = n-k\\)\n\\(MS_e = SS_e/df_e\\)\n\n\n\n\nTotal\n\\(SS_{tot}\\)\n\\(df_{tot} = n-1\\)\n\n\n\n\n\n\n\n\n\n\nHere the total variance is described as the sums of squares across all samples\n\\[\nSS_{tot} = \\sum{(X_{ij} - \\bar{\\bar{X}})^2} \\quad \\text{for all} \\quad j=1,...,n_i, \\text{ and } i = 1,..,k,\n\\]\nwhere \\(\\bar{\\bar{X}}\\) is the overall average and \\(X_{ij}\\) is the individual observations.\nIn the columns Sum of squares and Degrees of freedom, is the sum of the elements.\nIn the two sample t-test we use the pooled variance \\(s_{X_{pooled}}^2\\), In ANOVA this number is reflected by the residual variation \\(MS_e\\), which is also the estimate of \\(\\sigma^2\\) (I.e. \\(\\hat{\\sigma}^2 = MS_e\\)).\n\n\n16.2.4 ANOVA with several factors\nExperiments often consist of several factors. For instance Coffee served at different Temperatures evaluated by different Judges, beer produced from different Hops and different Yeast cultures or oil stored at different Temperature, over Time, at different Oxygen levels and different Light conditions.\n\nModel\nImagine an experiment with two factors: \\(A\\) and \\(B\\), where \\(A\\) has two levels, and \\(B\\) has three levels, then the model is simply an extension of the one-way model:\n\\[\\begin{gather}\nX_{i} = \\alpha(A_i) + \\beta(B_i) + e_{i} \\nonumber \\\\\n\\text{where } e_{i}\\sim \\mathcal{N}(0,\\sigma^2) \\text{ and independent} \\\\\n\\text{for } i=1,...,n \\nonumber\n\\end{gather}\\]\nIn this equation, \\(\\alpha()\\) describes the level with respect to \\(A\\), and has two levels: \\(\\alpha(A=1)\\) and \\(\\alpha(A=2)\\), and \\(\\beta()\\) describes the level with respect to \\(B\\), and has three levels: \\(\\beta(B=1)\\), \\(\\beta(B=2)\\) and \\(\\beta(B=3)\\).\n\n\nHypotheses\nThe associated null hypothesis are:\n\nFactor A: \\(H0: \\alpha(A=1) = \\alpha(A=2)= .... = \\alpha(A=k_A)\\)\nFactor B: \\(H0: \\beta(B=1) = \\beta(B=2)= .... = \\beta(B=k_B)\\)\n\n\n\nANOVA table\nIn the test of these hypothesis, the ANOVA table is simply extend with rows in relation to these effects table 16.2.\n\n\n\nTable 16.2: ANOVA table for a two-way ANOVA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSSQ\nDF\nMS\n\\(F_{obs}\\)\n\\(Pr(F_{df_{Source},df_e}&gt;F_{obs})\\)\n\n\n\n\nFactor(A)\n\\(SS_A\\)\n\\(df_A = k_A-1\\)\n\\(MS_A = SS_A/df_A\\)\n\\(F_A = MS_A/MS_e\\)\n\\(p_A\\)\n\n\nFactor(B)\n\\(SS_B\\)\n\\(df_B = k_B-1\\)\n\\(MS_B = SS_B/df_B\\)\n\\(F_B = MS_B/MS_e\\)\n\\(p_B\\)\n\n\nResiduals\n\\(SS_e\\)\n\\(df_e = n-df_A - df_B - 1\\)\n\\(MS_e = SS_e/df_e\\)\n\n\n\n\nTotal\n\\(SS_{tot}\\)\n\\(df_{tot} = n-1\\)\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions\nFor some experiments the effect of one factor might be dependent on the other factor, that is referred to as interaction. The interaction model for a two factor experiment is written as:\n\\[\\begin{gather}\nX_{i} = \\alpha(A_i) + \\beta(B_i) + \\gamma(A_i,B_i) +  e_{i} \\nonumber \\\\\n\\text{where } e_{i}\\sim \\mathcal{N}(0,\\sigma^2) \\text{ and independent} \\\\\n\\text{for } j=1,...,n \\nonumber\n\\end{gather}\\]\nwhere \\(\\gamma()\\) describes the interaction between the two factor. In a model with e.g. \\(2\\) and \\(3\\) levels for the factors, this interaction term naturally has \\(2\\cdot 3 = 6\\) levels. However, in the model above, the main effects (\\(\\alpha\\) and \\(\\beta\\)) are included, so that consumes some of the levels. Actually, this factor has \\((k_A - 1)(k_B - 1)\\) levels.\n\n\nANOVA table - interactions\nTesting the interaction leads to an extension of the ANOVA table, as seen in table 16.3.\n\n\n\nTable 16.3: ANOVA table for a two-way ANOVA with interaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSSQ\nDF\nMS\n\\(F_{obs}\\)\n\\(Pr(F_{df_{Source},df_e}&gt;F_{obs})\\)\n\n\n\n\nFactor(A)\n\\(SS_A\\)\n\\(df_A = k_A-1\\)\n\\(MS_A = SS_A/df_A\\)\n\\(F_A = MS_A/MS_e\\)\n\\(p_A\\)\n\n\nFactor(B)\n\\(SS_B\\)\n\\(df_B = k_B-1\\)\n\\(MS_B = SS_B/df_B\\)\n\\(F_B = MS_B/MS_e\\)\n\\(p_B\\)\n\n\nFactor(A*B)\n\\(SS_{AB}\\)\n\\(df_{AB} = (k_A-1)(k_B-1)\\)\n\\(MS_{AB} = SS_{AB}/df_{AB}\\)\n\\(F_{AB} = MS_{AB}/MS_e\\)\n\\(p_{AB}\\)\n\n\nResiduals\n\\(SS_e\\)\n\\(df_e = n-df_A - df_B - df_{AB} - 1 = n - (k_A)(k_B)\\)\n\\(MS_e = SS_e/df_e\\)\n\n\n\n\nTotal\n\\(SS_{tot}\\)\n\\(df_{tot} = n-1\\)\n\n\n\n\n\n\n\n\n\n\nWe see that the only thing that changes is the sums of squares and the degrees of freedom for the residuals.\n\n\n\n\n\n\nExample 16.1\n\n\n\n\n\n\n\n\n\nExample 16.1 - Natural phenolic antioxidants for meat preservation - ANOVA\n\n\n\n\n\n\nThis example is based on the sensory data on the meat sausages treated with green tea (GT) and rosemary extract (RE) or control, used previously in example @#exa-natural-phenolic-antiox-paired-t-test.\nA two-way anova can be used to evaluate the effect of several factors, and possible interaction effects on a single response variable. Here, we define a model with systematic effects of Treatment (3 levels), Assessor (8 levels) and Week (2 levels). Since it may be that the effect of the storage time differs for each of the antioxidant treatments, an interaciton effect between Treatment and Week is also included.\nThe starting model is therefore formulated as:\n\\[\\begin{equation}\nY_i=\\alpha (Treat_i)+\\beta(Assessor_i)+\\gamma(Week_i)+\\theta(Treat_i\\times Week_i)+e_i\n\\end{equation}\\]\nwhere \\(e_i \\sim \\mathcal{N}(0,\\sigma ^2)\\) and independent for \\(i=1,\\ldots,n\\)\n\nPlot the data\nWe wish to visualize three factors on a single response variable (figure 16.1). Here we use the x-axis for Week and Treatment, whereas the points are connected within Assessor.\n\nggplot(X, aes(Treatment, Smell_Old, group = Assessor)) +\n  geom_point(size = 5, aes(color = week)) +\n  geom_line() +\n  facet_wrap(~week) +\n  geom_text(\n    data = X[X$Treatment == \"Control\", ],\n    aes(label = Assessor), \n    hjust = 1.5 # Move the labels a little to the left\n  )\n\n\n\n\n\n\n\nFigure 16.1: Feedback on Smell_Old for all judges as a function of treatment. The plot is divided into week 0 and week 4.\n\n\n\n\n\nFrom the figure there are a few observations: First, some assessors (e.g.~128 and 197) use the entire range whereas some only scores in a narrow range (e.g.~12, 193 and 187). Generally it seems as the control treatment (at both time points) obtain higher scores compared to the treated samples. There are no apparent indications for the two treatments being different.\n\n\nCalculate two-way ANOVA\n\nUsing base RUsing Tidyverse\n\n\n\n# Compute ANOVA using specified model:\n# Smell_Old = Assessor + Treatment + Week + Treatment * Week\nfit &lt;- aov(Smell_Old ~ factor(Assessor) + factor(Treatment) * factor(week),\n           data = X)\n\n# Show ANOVA table\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Smell_Old\n                               Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nfactor(Assessor)                7  77.833  11.119  1.9861 0.086116 . \nfactor(Treatment)               2  70.153  35.077  6.2653 0.004826 **\nfactor(week)                    1   5.629   5.629  1.0054 0.323084   \nfactor(Treatment):factor(week)  2   1.062   0.531  0.0948 0.909763   \nResiduals                      34 190.352   5.599                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nImportant! The factor() calls are important as they convert the variables from numeric/strings to factors. What happens if you remove factor()?\n\n\n\n# Turn column \"Assessor\", \"Treatment\" and \"week\" into factors\nX &lt;- X |&gt; \n  mutate(across(c(Assessor, Treatment, week), factor))\n\n# Compute ANOVA using specified model:\n# Smell_Old = Assessor + Treatment + Week + Treatment * Week\nfit &lt;- aov(Smell_Old ~ Assessor + Treatment * week,\n           data = X)\n\n# Show ANOVA table\nanova(fit)          \n\nAnalysis of Variance Table\n\nResponse: Smell_Old\n               Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nAssessor        7  77.833  11.119  1.9861 0.086116 . \nTreatment       2  70.153  35.077  6.2653 0.004826 **\nweek            1   5.629   5.629  1.0054 0.323084   \nTreatment:week  2   1.062   0.531  0.0948 0.909763   \nResiduals      34 190.352   5.599                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nHere it can be seen that the only effect which is found significant (\\(p &lt; 0.05\\)) for the Old_Smell response is the Treatment factor. No significance was found for the Week or interaction factors. The Assessor effect is not strictly signifcant, but on the borderline (\\(p=0.09\\)). If we plot the raw data (figure 16.1), it seems each of the assesors have different base levels of scoring the old smell descriptor. Therefore, it is chosen to keep the systematic effect of Assessor in the model. Furthermore, it can be seen that there is some variation in the Assessor evaluation of the treatment effect (non-parallel), indicating that the sensory panel is not very precise, and may be poorly trained.\n\n\nFinal model\nThe final model is therefore the model only including the effects of the treatment and Assessor factors:\n\\[\\begin{equation}\nY_i=\\alpha (Treat_i)+\\beta(Assessor_i)+e_i\n\\end{equation}\\]\nwhere \\(e_i \\sim \\mathcal{N}(0,\\sigma^2)\\) and independent for \\(i=1,\\ldots,n\\).\nThe estimates for the factors (\\(\\alpha(Control),\\alpha(GT), \\alpha(RE), \\beta(Ass12),\\ldots,\\beta(193)\\)) as well as the standard deviation (\\(\\hat{\\sigma}\\)) can be obtained by first calculating a new model including only significant terms followed by the anova() and summary() commands.\n\n\n\n\n\n\n\n\n\n16.2.5 Contrasts\nIf it turns out that there is a significant effect of the factor, a natural next step is to compare the individual levels by estimation of confidence intervals and/or testing differences. This is quite similar to comparing two means by t-test. The only difference is, that \\(\\hat{\\sigma}^2 = MS_e\\) is used as the measure of random uncertainty (instead of \\(s_{X_{pooled}}^2\\)), and the degrees of freedom is NOT \\(n_1 + n_2 - 2\\), but the degrees of freedom associated with the uncertainty estimate: \\(df_e\\). Apart from that, the method is similar.\nAssume that Factor(A) has five levels, then the comparison of e.g. level \\(2\\) and \\(5\\) is done as follows:\n\nConfidence intervals\n\\[\\begin{equation}\n\\bar{X_5} - \\bar{X_2} \\pm t_{1 - \\alpha/2,df_e}\\sqrt{MS_e}\\sqrt{\\frac{1}{n_5} + \\frac{1}{n_2}}\n\\end{equation}\\]\nWhere \\(t_{1 - \\alpha/2,df_e}\\) is the t-fractile at a user-specified level \\(\\alpha\\) with \\(df_e\\) degrees of freedom.\n\n\nTest of contrast\nThe associated hypothesis are stated as:\n\\[\\begin{equation}\nH_0: \\mu_5=\\mu_2\n\\end{equation}\\]\nWith the alternative:\n\\[\\begin{equation}\nH_A: \\mu_5 \\neq \\mu_2\n\\end{equation}\\]\nThen the test statistics is calculated as: \\[\\begin{equation}\nt_{obs} = \\frac{\\bar{X_5} - \\bar{X_2}} {\\sqrt{MS_e}\\sqrt{\\frac{1}{n_5} + \\frac{1}{n_2}}}\n\\end{equation}\\] With the associated (2-sided) p-value: \\(p = 2 \\cdot P(\\mathcal{T}_{df=df_e}&gt;t_{obs})\\).\n\n\n\n\n\n\nExample 16.2\n\n\n\n\n\n\n\n\n\nExample 16.2 - Natural phenolic antioxidants for meat preservation - contrasts\n\n\n\n\n\n\nIn this example, we wish to compare pairs of levels within a factor from an ANOVA model. In Example 16.1 the Treatment effect from a two way model as shown below is significant, however, that does not imply that all levels are different, but only that at least a single strikes out.\nWe start by computing the ANOVA and assigning it to the variable fit.\n\n# Compute ANOVA\nfit &lt;- aov(Smell_Old ~ factor(Assessor) + factor(Treatment),\n           data = X)\n\n# Print ANOVA table\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Smell_Old\n                  Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nfactor(Assessor)   7  77.833  11.119  2.0879 0.069457 . \nfactor(Treatment)  2  70.153  35.077  6.5866 0.003573 **\nResiduals         37 197.043   5.325                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can use model.tables() to get means for each level of each factor.\n\n# Show means across both factors\nmodel.tables(fit, type = \"means\")\n\nTables of means\nGrand mean\n         \n3.025532 \n\n factor(Assessor) \n       12   128   147 149   187   190   193   197\n    5.725 2.312 2.237 1.1 3.062 2.763 3.187 3.975\nrep 6.000 6.000 6.000 6.0 6.000 6.000 6.000 5.000\n\n factor(Treatment) \n    Control     GT     RE\n      4.752  1.865  2.568\nrep  15.000 16.000 16.000\n\n\nThe rows of numbers contain the means across each level. rep is the number of observations.\n\nContrasts\nWe wish to compare the treatments RE and GT. This a basically a two sample t-test, just where the entire dataset and model is used to calculate the standard deviation.\nThe hypothesis are: \\[\nH_0: \\mu_{GT}=\\mu_{RE} \\quad \\text{and} \\quad H_A: \\mu_{GT} \\neq \\mu_{RE}\n\\]\nNaturally, the driver of this test is the observed difference between the two means. In total the test-statistics amount to:\n\\[\\begin{equation}\nt_{obs} = \\frac{\\bar{X}_{GT} - \\bar{X}_{RE}} {\\sqrt{MS_e}\\sqrt{\\frac{1}{n_{GT}} + \\frac{1}{n_{RE}}}}\n\\end{equation}\\]\nWith the associated (2-sided) p-value:\n\\[\np = 2 \\cdot P(\\mathcal{T}_{df=df_e}&gt;t_{obs})\n\\]\n\nm1 &lt;- 1.86 # GT mean\nm2 &lt;- 2.57 # RE mean\nn1 &lt;- 16 # GT n\nn2 &lt;- 16 # RE n\ns &lt;- sqrt(5.325) # SD from ANOVA table\ndf &lt;- 37 # DF for residuals\nt_obs &lt;- (m1 - m2) / (s*sqrt(1/n1 + 1/n2)) # Test statistic\n2*(1 - pt(abs(t_obs),df)) # p-value for GT vs RE\n\n[1] 0.3897755\n\n\nIn line with the raw data, the observed difference between the two treatments, is not statistically singificant, and we conclude that the two treatments has similar properties in terms of Old_Smell.\n\n\nConfidence interval on difference between RE and Control\nAs an alternative, the confidence interval on the difference between two means can be calculated. As example this is done for RE vs Control.\n\nm3 &lt;- 4.75 # Control mean\nn3 &lt;- 15 # Control n\nCIlow &lt;- m3 - m2 - qt(0.975,df)*s*sqrt(1/n2 + 1/n3) # lower bound\nCIhigh &lt;- m3 - m2 + qt(0.975,df)*s*sqrt(1/n2 + 1/n3) # upper bound\nc(CIlow,CIhigh)\n\n[1] 0.4995882 3.8604118\n\n\nWe see that the confidence interval on the difference between the control and RE treated samples are positive and do not overlap \\(0\\), that is, in general the control samples has a higher score with respect to Old_Smell and further that this is a significant difference on \\(95\\%\\) level.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/week6/anova.html#videos",
    "href": "chapters/week6/anova.html#videos",
    "title": "16  Analysis of Variance (ANOVA)",
    "section": "16.3 Videos",
    "text": "16.3 Videos\n\n16.3.1 Analysis of Variance (ANOVA) - J David Eisenberg\n\n\n\n16.3.2 Anova in R - Ed Boone",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/week6/anova.html#exercises",
    "href": "chapters/week6/anova.html#exercises",
    "title": "16  Analysis of Variance (ANOVA)",
    "section": "16.4 Exercises",
    "text": "16.4 Exercises\n\n\n\n16.4.1 ANOVA\nLoad Results panel.xlsx. Look at the study design and:\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFormulate a One-way ANOVA model for the coffees served at different Temperatures and the response bitter.\nFormulate null-hypothesis and alternative hypothesis between the different coffee temperatures.\nTest the hypothesis.\nFormulate a Two-way ANOVA model for the taste of bitter with the two factors; Temperature and Judge\nFormulate null-hypothesis and alternative hypothesis.\n\n\n\n\n\n\n\nExercise 16.1\n\n\n\n\n\n\n\n\n\n\nExercise 16.2\n\n\n\n\n\n\n\n\n\nExercise 16.1 - Wine and one-way ANOVA\n\n\n\n\n\n\nWines from four different countries (Argentina, Australia, Chile and South Africa) were analyzed for aroma compounds with GC-MS (gas chromatography coupled with mass spectrometry). The dataset can be found in the file “Wine.xlsx”. The wine data should already be familiar to you otherwise look at the exercises from week 1 and 2.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhen working with ANOVA which assumption is then made about the data, and how does the model look?\nWe would like to investigate if the wines from the four different countries are different. State the null hypothesis and the alternative hypothesis.\nMake a combined jitter and boxplot on the variable X3.Hexenol. Are there any suspicious samples?\nMake an ANOVA on X3.Hexenol, look at the summary() and draw conclusions.\nCompare the result with the boxplot from Q3. Was the result of the ANOVA expected? If yes; why?\nWe would like to state which countries are significantly different from the others. For this you need to install the R-package multcomp. Use the command TukeyHSD() to investigate the differences/contrasts between the countries. Use the confidence intervals for judging differences.\nWhich countries are significantly different and which are not? Compare with the boxplot produced in Q3.\nMake the boxplot and the ANOVA for the following variables too; Diethyl.succinate, X1.Hexanol, Ethyl.hexanoate and Ethyl.propanoate. Are any of these variables different between countries?\nCheck for normality of the model residuals by making a qq-plot.\n\nHint: You need to extract the residuals from the model, for instance by the function resid(). Use qqnorm() and qqline() to make the plot.\n\n\nCan we trust the assumption about normality?\nAfter doing this manually, try the function plot() on the aov() object, which produces four plots for assumption checking.\n\nFor some of the results, the jitter plot indicates, a few extreme samples. What happens if these are removed? Does the conclusions change?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.3\n\n\n\n\n\n\n\n\n\nExercise 16.2 - Diet and fat metabolism - ANOVA\n\n\n\n\n\n\nThe diet is a central factor involved in general health, and especially in relation to obesity, where a balance between intake of protein, fat and carbohydrates, as well as types of these nutrients seems important. Therefore various controlled studies are conducted to show the effect of different diets. A study examining the effect of protein from milk (casein or whey) and amount of fat on growth biomarkers of fat metabolism and type I diabetes was conducted in 40 mice over an intervention period of 14 weeks.\nThe data for this exercise is the same as for Exercise 9.3.\nFor this exercise we are going to focus on cholesterol as a biomarker related to fat metabolism, and on three types of diet:\n\nHigh fat with the milk protein casein HF casein (n=15).\nHigh fat with whey protein from milk HF whey (n=15).\nLow fat with the milk protein casein LF casein (n=10).\n\nThe cholesterol level at the end of the 14 week intervention is listed in table 16.4 including some descriptive stats.\n\n\n\nTable 16.4: Some summary statistics for cholesterol across the three diets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n…\n9\n10\n11\n…\n15\n\\(\\sum{X}\\)\n\\(\\sum{(X - \\bar{X})^2}\\)\n\\(\\bar{X}\\)\n\\(s_{X}\\)\n\n\n\n\nCholesterol (HF casein)\n4.68\n3.60\n…\n4.60\n4.84\n4.84\n…\n4.37\n67.29\n4.06\n\n\n\n\nCholesterol (HF whey)\n3.79\n2.82\n…\n3.77\n4.63\n3.44\n…\n3.24\n54.86\n4.69\n\n\n\n\nCholesterol (LF casein)\n3.97\n3.69\n…\n3.62\n3.53\n…\n…\n…\n34.37\n2.07\n\n\n\n\n\n\n\n\n\nThe total sums of squares is: \\(SS_{tot} = \\sum{(X_{ij} - \\bar{\\bar{X}})^2} = 18.99\\).\nThe first six questions are supposed to be done by hand where the computer only is used as a pocket calculator.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate descriptive statistics for the three groups\n\nHint: Fill in the table the missing values in table 16.4.\n\nSketch these results in a graph (by pen and paper - no computer).\nState a model for these data, and a hypothesis of similarity between the three dietary treatments (with regards to cholesterol level).\nConstruct an ANOVA table and calculate and fill in the numbers including test statistics (\\(F_{obs}\\)) and the corresponding p-value.\n\nFor the translation of \\(F_{obs}\\) to \\(p\\), use the function pf() in R.\n\nThis p-value should be significant. Guided by the initial descriptive stats, do you believe that all three diets are different? Or is there two groups which are close in estimate?\nFormulate a hypothesis of similarity between the two most similar groups. Test this contrast.\n\n\n\n\n\nThe data can be found in the file Mouse_diet_intervention.xlsx.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport the data, and make a plot of cholesterol ($cholesterol) and dietary intervention ($Fat_Protein)\nRepeat the ANOVA analysis using R with the function aov() for construction of model and the function anova() for analysis of this model.\nCheck that the model assumptions are ok.\n\nplot(model) where model is the object created by aov().\n\nFor the individual contrasts you can use the function TukeyHSD() on the model.\n\nBe aware that the p-values in this analysis is adjusted for multiple testing, and therefore are bigger than the ones done one by one.\n\nBased on these results, which dietary component do you think causes differences in cholesterol level?\nThere are some indications of differences between HF whey and LF casein, however, not significant. How many samples would have been needed in order to achieve significance with an appropriate power level?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.4\n\n\n\n\n\n\n\n\n\nExercise 16.3 - Diet and fat metabolism - multivariate ANOVA\n\n\n\n\n\n\nIn this exercise, we are going to extend the analysis from Exercise 16.3 to include several biomarkers.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nStart out by construction of a PCA on the biomarkers: insulin, cholesterol, triglycerides, NEFA, glucose and HbA1c (NEFA = nonesterified fatty acids), including all 40 samples, and comment on the results with respect to differences between diets. This is computationally identical to the task in Exercise 9.3.\nRepeat the ANOVA for several biomarkers including plotting.\nZack out the components in the PCA model and glue them together with the original data (use cbind()).\n\nYou can find inspiration on how to do this in Exercise 5.1.\n\nUse the components from the PCA model as response variables, and repeat the ANOVA (including plotting).\nComment on the similarity / dis-similarity between the univariate results, and the ones based on the PCA.\nCompare the results (plot and ANOVA) for PC1 and PC6. Why do you think the dietary signal is more pronounced in the first component?\n\nHint: Think of how much, and which type of variation that is captured in the individual components.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.5\n\n\n\n\n\n\n\n\n\nExercise 16.4 - Analysis of coffee serving temperature\n\n\n\n\n\n\nServing temperature of coffee seems of importance of how this drink is perceived. However, it is not totally clear how this relation is. In order to understand this, studies on the same type of coffee served at different temperature is conducted. In this exercise we are going to use the data from a trained Panel of eight judges, evaluating coffee served at six different temperatures on a set of sensorical descriptors. Each judge is presented with each temperature in a total of four replicates leading to a total of \\(6 \\times 8 \\times 4 = 192\\) samples.\nIn the dataset Results Panel.xlsx the results are listed. In this exercise we are going to analyse the differences between the individual temperatures while utilizing the design of repeated scoring by the same judges. This exercise is an extension of Exercise 5.2 from week one.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWe want to summarize data, such that each judge have one score for each coffee sample and each attribute (instead of four).\n\nYou can use the aggregate() function to average over replicates, which is to retain Assessor and Sample.\n\nPlot this descriptive measure for Intensity across temperature (x-axis) and join the points from the same judge.\nMake an ANOVA model with the response Intensity including both factors (Temperature and Judge).\nCheck out what the function factor() is doing? What happens if it is removed (check the degrees of freedoms consumed by the two options).\nCheck that the model assumptions are ok. (plot(model) where model is the object created by aov()).\nComment on these results (in relation to Temperature and Judge).\nUse this data as input for construction of a PCA model, and make a bi-plot.\nWhat do you see with respect to differences in temperature and judges?\nZack out the first couple of component and glue them together with the original data.\nMake ANOVA model for component 1 to 5 and find the component with the strongest Temperature and Judge signature respectively.\nPlot the PCA model with these two components (This is specified by the option choice = ...). Color this plot according to Temperature and Judge.\nWhich sensorical markers seems mostly related to differences between Temperature and Judge respectively?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.6\n\n\n\n\n\n\n\n\n\nExercise 16.5 - Carcass suspension\n\n\n\n\n\n\nToughness is found to be the most important quality characteristic in beef. Variation in toughness is primarily related to the muscle fibers and the connective tissue. Connective tissue is thought to be responsible for the relative fixed background toughness, largely affected by animal age. The toughness of the muscle fibers depends on a couple of factors and is more likely to be manipulated by e.g. carcass suspension to prevent post-mortem contraction of the muscle fibers.\nAn experiment was conducted in order to test the effects of Animal Age, Carcass Suspension and the interaction between Animal Age and Carcass Suspension. The muscle of interest was longissimus dorsi. The hypothesis for the experiment was that pelvic suspension would prevent post-mortem contraction of the muscle fibers and thereby result in a more tender longissimus dorsi compared with suspension in the Achilles tendon. Furthermore, the toughness was believed to increase with increasing age. This increase in toughness was assigned to connective tissue.\nIn the experiment, animals were slaughtered and the carcasses were spilt. One side was suspended in in the pelvic bone (figure 16.2, left) and the other side was suspended in the Achilles tendon (figure 16.2, right). The experiment was balanced and 15 carcass sides were assigned to each group.\n\n\n\n\n\n\n\nFigure 16.2: Left: Pelvic suspension. Right: Achilles suspension.\n\n\n\n\nIn the table 16.5 are shown the average results from each cell in the design.\n\n\n\nTable 16.5: Mean values for the design (n = 15 in each group)\n\n\n\n\n\n\nSuspension\nAge 2y\nAge 3y\nAge 4y\n\n\n\n\nPelvic (A)\n58.5\n65.0\n66.0\n\n\nAchilles (B)\n72.0\n82.5\n93.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nSketch these results with the response on the y-axis, one factor on the x-axis, and join the points based on the other factor. What is your first impression on the effect of age of the animal and of suspension method.\n\nFurther, does the lines seem to be deviating from being parallel? (This indicates interaction between the factors).\n\nWrite up a model including interaction term.\nWrite up the \\(H0\\) (and the alternative) hypotheses for the main effects in this experiment.\n\n\n\n\n\n\n\n\nTable 16.6: ANOVA table for response shear force.\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\np-value\n\n\n\n\nAge\n3083.7\n\n\n\n\n\n\nSuspension\n8410.0\n\n\n\n\n\n\nAge*Suspension\n\n\n\n\n\n\n\nError\n\n\n375.0\n-\n-\n\n\nTotal\n43715.0\n89\n-\n-\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFill the missing values in the table 16.6.\nWhat factors are having a significant effect (significance level of 0.05)?\nModify the two-way ANOVA model. Should the interaction be included in the final model for the present study? Why/Why not?\nToughness is explained by muscle fibers and connective tissue. The contribution from muscle fibers is impacted by suspension method. The contribution from connective tissue is impacted by age. Would you expect an effect of the interaction term between suspension method and age? Why/Why not? How does your answer correspond with the answers in question 6?\nTraditionally, carcasses suspension is done by Achilles, what would you recommend based on this study? What are the limitations/assumptions for such a recommendation?\n\nHint: Have we checked all muscles? Look at the picture - do you think the conclusions extrapolate to other parts of the animal?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.7\n\n\n\n\n\n\n\n\n\nExercise 16.6 - Stability of oil under different conditions\n\n\n\n\n\n\nThis exercise is a direct extension of Exercise 9.5.\nOil are primary made up of triglycerides, where some of the fatty acids are unsaturated. This causes such a product to be susceptible to oxidation both from chemical oxidative agents such as metal ions, or from exposure to light. Oxidation of the unsaturated fatty acids changes the sensorical and physical properties of the oil.\nIn the southern part of Africa grows a robust bean - the Marama bean. This bean has a favorable dietary composition, including dietary fibers, fats and proteins, as most similar types of nuts, therefore this crop could be utilized for making healthy products by the locals for the locals. One such product is Marama bean oil. A study has been conducted to investigate the oxidative stability of the oil under various conditions. In the dataset MaramaBeanOilOx.xlsx are listed the results from such an experiment (including data from both normal and roasted beans). The experimental factors are:\n\nStorage time (Month)\nProduct type (Product)\nStorage temperature (Temperature)\nStorage condition (Light)\nPackaging air (Air)\n\nThe response variable reflecting oxidative stability is peroxide value and named PV.\nThe first three questions are identical to Exercise 9.5.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nRead in the data, and subset so that you only include data related to product type Oil.\nMake descriptive plots of the response variable PV imposing storage temperature, condition, and time.\n\nHint: factor(Temperature):Light specifies all combinations of these two factors. facet_grid(.~Month) splits the plot into several plots according to Month.\n\nWhat do you observe in terms of storage time, temperature and condition from this plot?\nBased on the plot, suggest a model including the three factors Month, Temperature and Light. Which factors do you think is additive (i.e. have the same effect regardless of the other factors), and which do you think interacts?\nBuild a model including all combinations of the three factors.\n\nThat is: m &lt;- aov(data=Marama.oil, PV~factor(Temperature)*Light*factor(Month)).\n\nCheck the assumption concerning the distribution of the residuals. If necessary, make an appropriate transformation of the response variable, and check that it is doing better in terms of distributional assumptions for the model residuals.\nIt seems like we need all combinations of everything. Construct a single variable, that has the seven levels indicating the design, and use this in an ANOVA model.\n\nHint: Allcomp &lt;- factor(Marama.oil$Temperature):Marama.oil$Light:factor(Marama.oil$Month\n\nUse TukeyHSD() on that model to make all pairwise comparisons of the seven individual levels. And comment on what you obtain. Does the initial plot correspond to the pairwise observations?",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/week7/week_7.html",
    "href": "chapters/week7/week_7.html",
    "title": "Week 7",
    "section": "",
    "text": "Hand-in assignment\nThis week is going to be on regression analysis. Regression might be the most influential and most widely used technique for relating response with outcomes. Although regression is a general framework for different types of responses (continuous, binomial, poisson, categorical,…) we are only going to deal with continuous outcomes in this course. However, knowing the basis for continuous data, makes it easy to extend into other types of data.\nUsing regression as an example, this week will introduce the mathematical concept behind analysis of continuous normally distributed data. Namely Least Squares estimation. In the estimation of parameters for a model, ANOVA, linear regression, correlation, PCA and also non-linear models, there is often stated a so-called objective in the form of a minimization problem. I.e. give me the parameters that results in the minimum sum of squared errors, that is the least squares parameter estimates. The most simple situation is the center of a distribution; Using the mean/average as the center results in a minimum overall distance to the center.\nExercise 18.1 Diet and fat metabolism - ANOVA question 1 to 6 is to be handed in (through absalon or as hard-copy Wednesday night).  You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 7"
    ]
  },
  {
    "objectID": "chapters/week7/week_7.html#exercises",
    "href": "chapters/week7/week_7.html#exercises",
    "title": "Week 7",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 17.1 \nExercise 17.2 \n\nFor Wednesday work through the following exercises:\n\nExercise 17.3 \nExercise 17.5 \nExercise 17.6 \n\nFurther, this week might allow you to recap on some of the exercises you did not make during the last weeks.",
    "crumbs": [
      "Week 7"
    ]
  },
  {
    "objectID": "chapters/week7/week_7.html#case-iv",
    "href": "chapters/week7/week_7.html#case-iv",
    "title": "Week 7",
    "section": "Case IV",
    "text": "Case IV\nIn this dataset there is a lot of possibilities, so it is very important that you take choices on what questions you want to pursue, and then use all your knowledge and tools to grasp how the biological system works.\nCase four is contributing to the final grade.\nThe case should be made in groups similarly as to case 1-3.",
    "crumbs": [
      "Week 7"
    ]
  },
  {
    "objectID": "chapters/week7/regression.html",
    "href": "chapters/week7/regression.html",
    "title": "17  Regression",
    "section": "",
    "text": "17.1 Reading material",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/week7/regression.html#reading-material",
    "href": "chapters/week7/regression.html#reading-material",
    "title": "17  Regression",
    "section": "",
    "text": "Regression - in short\nVideos on linear regression:\n\nLinear Regression, Clearly Explained\nUsing Linear Models for t-tests and ANOVA, Clearly Explained\n\nChapter 5 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/week7/regression.html#sec-intro-to-regression",
    "href": "chapters/week7/regression.html#sec-intro-to-regression",
    "title": "17  Regression",
    "section": "17.2 Regression - in short",
    "text": "17.2 Regression - in short\nRegression models are simply extensions of the ANOVA framework allowing for descriptive variables being continuous.\nThe simplest form is univariate regression with a single response variable (\\(Y\\)) described by a single predictor (\\(X\\)).\n\n17.2.1 The model\n\\[\n\\begin{align*}\nY_i &= \\alpha + \\beta \\cdot X_i + e_i \\\\\n\\text{where } &e_i \\sim \\mathcal{N}(0,\\sigma^2) \\text{ and independent} \\\\\n\\text{for } &i=1,...,n\n\\end{align*}\n\\]\nWhere \\(\\alpha\\) is the level of (\\(Y\\)) at \\(x=0\\) and \\(\\beta\\) is the slope of the line.\nEach of these two parameters consumes one degree of freedom.\n\n\n17.2.2 Hypotheses\nThe natural hypothesis is to check whether (\\(X\\)) has an effect on (\\(Y\\)), that is: \\(H_0: \\beta=0\\) with the alternative \\(H_A: \\beta \\neq 0\\). This can be tested with either an F-test or a T-test (yielding the same results). For some applications the intercept (\\(\\alpha\\)) might be related to a question, just as other values for \\(\\beta\\) could be relevant to test, but the most common is testing the relation between \\(X\\) and \\(Y\\).\n\n\n17.2.3 Estimation\nThe model is done by least squares fit, which is explained in detail elsewhere in these notes.\nUtilizing the summary statistics on \\(X\\) and \\(Y\\):\n\n\\(\\bar{X} = \\sum(X) / n\\)\n\\(\\bar{Y} = \\sum(Y) / n\\)\n\\(\\sum(X - \\bar{X})(Y - \\bar{Y})\\)\nvar\\((X) = \\sum(X - \\bar{X})^2 / (n-1)\\)\n\nThe parameters of the model: slope (\\(\\beta\\)), intercept (\\(\\alpha\\)) and the residual variance (\\(\\sigma^2\\)) are estimated as follows.\nSlope: \\[\n\\hat{\\beta} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\]\nIntercept: \\[\n\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\n\\]\nResidual variance: \\[\n\\hat{\\sigma}^2 = \\frac{\\sum\\hat{e}^2_i}{n-2} = \\frac{\\sum(Y_i - \\hat{Y}_i)^2}{n-2}\n\\]\nFor especially the slope and intercept, the associated uncertainty can be of great interest. The associated standard errors for those are:\nSlope standard error: \\[\n\\hat{\\sigma}_{\\hat{\\beta}} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(X_i - \\bar{X})^2}}\n\\]\nIntercept standard error: \\[\n\\hat{\\sigma}_{\\hat{\\alpha}} = \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_i - \\bar{X})^2}}\n\\]\n\n\n17.2.4 Confidence intervals for the parameters\nA combination of the estimates and uncertainties can be used for calculating confidence intervals via the general formula as a symmetric range on each side of the estimate.\nConfidence interval for any (central) parameter (\\(\\alpha, \\beta, \\mu\\)… here we use \\(\\beta\\)) have the usual form:\n\\[\nCI_{\\beta,1-\\alpha}: \\hat{\\beta} \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma}_{\\hat{\\beta}}\n\\]\nwhere the subscript \\((1-\\alpha)\\) refers to the coverage of the interval. Usually \\(\\alpha = 0.05\\) giving \\(95\\%\\) coverage of the interval with a corresponding \\(1-\\alpha/2 = 0.975\\) level for the \\(t-\\)fractile.\n\n\n17.2.5 Confidence- and prediction intervals for the line\nConfidence bounds around the line can be calculated for each value of \\(X\\). For a single point \\(X^*\\) the formula is as follows:\n\\[\nCI_{Y(X^*),1-\\alpha}: \\hat{\\alpha} + \\hat{\\beta}\\cdot X^* \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{X})^2}{ \\sum(X_i - \\bar{X})^2}}\n\\]\nGiven a single new sample with a known \\(X^*\\), one might want to estimate the associated (unknown) \\(Y\\) including an interval indicating the uncertainty. This is called a prediction interval and has the following form:\n\\[\nPI_{Y(X^*),1-\\alpha}: \\hat{\\alpha} + \\hat{\\beta}\\cdot X^* \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(X^* - \\bar{X})^2}{ \\sum(X_i - \\bar{X})^2}}\n\\]\nThe prediction- and confidence interval have a very similar form. The only difference is the \\(1\\) in the \\(\\sqrt{}\\) in the calculation of the standard error. This \\(1\\) can be seen as (one over) the number samples for which the interval is calculated: For prediction it is \\(\\frac{1}{1} = 1\\) whereas for confidence it assumes an infinitely large population \\(\\frac{1}{\\inf} = 0\\).\n\n\n17.2.6 Model assumptions\nSimilarly to other types of ANOVA models, regression models assumes independent residuals with the same variance, why regression model should be exposed to the same procedures for model validation.\n\n\n17.2.7 Transformations\nThe model assumes a linear relation between \\(X\\) and \\(Y\\). However, inspection of data or mechanistic insight might reveal, that this relation is suboptimal, why the response and/or the descriptor variable might be subjected to a relevant transformation. This is investigated and checked by visual plotting of the raw and transformed data.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/week7/regression.html#videos",
    "href": "chapters/week7/regression.html#videos",
    "title": "17  Regression",
    "section": "17.3 Videos",
    "text": "17.3 Videos\n\nLinear Regression, Clearly Explained - StatQuest\n\n\n\nUsing Linear Models for t-tests and ANOVA, Clearly Explained - StatQuest",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/week7/regression.html#exercises",
    "href": "chapters/week7/regression.html#exercises",
    "title": "17  Regression",
    "section": "17.4 Exercises",
    "text": "17.4 Exercises\n\n\n\n\n\n\nExercise 17.1\n\n\n\n\n\n\n\n\n\nExercise 17.1 - Linear regression\n\n\n\n\n\n\nTo get a feeling of how linear regression works we can consider two variables, \\(x\\) and \\(y\\), that are linearly dependent. We generate \\(x\\) as a random variable and \\(y\\) is related to \\(x\\) plus some noise.\n\n# Generate 20 normally distributed data points (mean = 0, sd = 1)\nx &lt;- rnorm(20) \n\n# Add a slope (m = 2) and normally distributed noise (mean = 0, sd = 1)\ny &lt;- 2 * x * + rnorm(20) \n\n# Convert to dataframe\ndf &lt;- data.frame(x, y)\n\n# Create plot\nplt &lt;- ggplot(df, aes(x, y)) +\n  geom_point() + \n  geom_abline(slope = 1, intercept = 0) # Add straight line\n\n# Show plot\nplt\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nChange the slope and intercept of geom_abline() to get the line to fit the data as well as possible.\nAdd a line estimated by least squares (see code below). Does the manually estimated line fit with the regression line?\n\n\n\n\n\n\n# Add regression line\nplt &lt;- plt + geom_smooth(method = \"lm\", se = F, color = \"red\")\n\n# Show plot\nplt\n\nFrom the data generation step above we can see that the true relationship is slope = 2 and intercept = 0. Add a line reflecting the thr\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAdd a line reflecting the true relationship of \\(x\\) and \\(y\\).\nIs the estimated line the same as your new line (the truth)? Why/why not? Try to run the script a couple of times.\nHow does the least squares line relate to the true relationship line?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 17.2\n\n\n\n\n\n\n\n\n\nExercise 17.2 - Diet and fat metabolism - regression by hand\n\n\n\n\n\n\nThe diet is a central factor involved in general health, and especially in relation to obesity, where a balance between intake of protein, fat and carbohydrates, as well as type of these nutrients seems important. Therefore various controlled studies are conducted to show the effect of different diets. A study examining the effect of protein from milk (casein or whey) and amount of fat on growth biomarkers of fat metabolism and type I diabetes was conducted in 40 mouse over an intervention period of 14 weeks.\nThe data for this exercise is the same as for Exercise 9.3.\nFor this exercise we are going to focus on predicting different biomarkers from weight data.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nSet the independent variable (\\(X\\)) to be weight at 14 weeks (measured in grams), and let the dependent variable (\\(Y\\)) be the cholesterol level. Write up the model for the relation between \\(X\\) and \\(Y\\).\nMake a small drawing, and infer all the model parameters on this figure.\nEstimate the parameters based on the descriptive numbers in the table below.\n\n\n\n\n\n\n\n\nTable 17.1: Some descriptive numbers computed on the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sum{X}\\)\n\\(\\sum{(X - \\bar{X})^2}\\)\n\\(\\sum{Y}\\)\n\\(\\sum{(Y - \\bar{Y})^2}\\)\n\\(\\sum{(X - \\bar{X})(Y - \\bar{Y})}\\)\n\\(\\sum{(Y - \\hat{Y})^2}\\)\n\n\n\n\n1426.2\n1099.3\n156.5\n18.99\n116.44\n6.65\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nConstruct confidence intervals for the intercept and the slope for this model, and judge whether there seem to be a relation between weight and cholesterol level.\n\nHint: Chapter 5.4 in Brockhoff have formulas for the variance of the parameters.\n\nCalculate a prediction interval for a new mouse with a weight of 40g\n\nHint: Box 5.17 in Brockhoff have the relevant formulas.\n\nLoad data, plot it, and verify the results using aov() for construction of a regression model, and predict() for the prediction of new samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 17.3\n\n\n\n\n\n\n\n\n\nExercise 17.3 - Diet and fat metabolism - regression and PCA\n\n\n\n\n\n\nThis exercise is an extension of Exercise 17.2.\n\nMake regression models between weight at week 14 and all 7 biomarkers. That includes: Scatter plot, regression modeling and check of model assumptions.\nFor the response variable insulin, there are 3 outlying points. Is the results robust towards removal of those?\nMake a PCA on the biomarkers and plot it using ggbiplot().\nInfer the external information $bw_w14 on the plot using geom_point(aes(color = ...)). If you are not satisfied with the colors, then use scale_color_gradient(low=-..., high=...) to modify it.\nExtract the first couple of components and use those as response variables in regression models versus weight.\nCompile the results, and give an answer to which pattern of biomarkers that are related to weight.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 17.4\n\n\n\n\n\n\n\n\n\nExercise 17.4 - Standard addition\n\n\n\n\n\n\nThis exercise shows how regression modeling is used for calculating the concentration of a chemical substance in a sample.\nThe setup is: We have a sample with a molecule of unknown concentration (\\(C\\)). We have an indirect technique which gives a response (\\(y\\)) proportional to the concentration of this molecule. That is \\(y \\propto C\\) or \\(y = \\alpha C\\). However, we do not know the coefficient \\(\\alpha\\). In order to determine this, a series of five measurements with added volume of the molecule is conducted. Schematically that results in measurements like in figure 17.1. Due to proportionality then the concentration in the sample is simply the value where the line crosses the \\(x\\)-axis.\n\n\nCode\ndata.frame(\n  conc = c(0.0, 0.8, 1.5, 2.2, 3.0),\n  sig  = c(2.2, 3.3, 4.0, 4.8, 6.0) \n) |&gt; \n  ggplot(aes(conc, sig)) +\n  geom_smooth(method = \"lm\", se = F, fullrange = T, color = \"black\") +\n  geom_point(color = \"red\") +\n  labs(\n    x = \"Concentration added\",\n    y = \"Signal\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nFigure 17.1: Principle in standard addition. Sample concentration is read off in the point where the line crosses the x-axis.\n\n\n\n\n\nIn table 17.2 are listed five measurements from a standard addition experiment.\n\n\n\nTable 17.2: Standard addition vs signal for a calibration set.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\nConcentration added (\\(x\\))\n0.0\n0.8\n1.5\n2.2\n3.0\n\n\nSignal (\\(y\\))\n2.2\n3.3\n4.0\n4.8\n6.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nChuck the data into R.\nMake a scatter plot, and add a straight line indicating the best fit.\n\nIf you use ggplot2 then adding + geom_smooth(method = 'lm', fullrange = T) to the plot will do it.\n\nState a statistical model between \\(y\\) and \\(x\\), and fit it using either aov() or lm().\nBased on the model parameters (slope and intercept) derive an expression for \\(-x_{y=0}\\).\nBased on the estimated model, estimate the concentration in the sample.\n\n\n\n\n\nIt should be quite easy to give a central estimate for the concentration. However, giving a confidence interval for this measure is not so easy. In order to do so, confidence limits for the estimated line is used to assess the bounds of the confidence interval.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nConstruct confidence intervals for a broad range of \\(x\\)-values. This is done by constructing a new data frame and predicting responses for this using the estimated model (see code below).\nUse the upper, central and lower limits of this series of x-values to estimate the limits for \\(-x_{y=0}\\).\n\n\n\n\n\n\n# Load in data\ndf &lt;- data.frame(\n  conc = c(0.0, 0.8, 1.5, 2.2, 3.0),\n  sig  = c(2.2, 3.3, 4.0, 4.8, 6.0) \n)\n\n# Fit linear model\nfit &lt;- lm(sig ~ conc, data = df)\n\n# Create data for prediction\nnew_data &lt;- data.frame(\n  conc = seq(-3, 7, length.out = 1000)\n)\n\n# Predict data using fitted lm\nsig_pred &lt;- predict(fit, new_data, interval = \"confidence\")\n\nsig_pred[ , 1] # Fitted values\nsig_pred[ , 2] # Lower CI\nsig_pred[ , 3] # Upper CI\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 17.5\n\n\n\n\n\n\n\n\n\nExercise 17.5 - Standard curve for calcium in milk - hand calculations using R\n\n\n\n\n\n\nMilk coagulation is needed during cheese making. A number of factors influence the coagulation properties of milk. Obviously \\(\\kappa\\)-casein is important. However, also the amount of calcium (Ca\\(^{2+}\\)) is essential during milk coagulation.\nCalcium in a milk sample may be quantified by atom absorption spectroscopy. This technique relies on Beer’s law:\n\\[\nA = \\varepsilon \\times L \\times C\n\\]\nWhere \\(A\\) is the absorbance, \\(\\varepsilon\\) is the molar absorptivity (constant), \\(L\\) is the path length of the sample (constant) and \\(C\\) is the concentration. Hence, Beer’s law reveals proportionality between absorbance and concentration.\nIn this exercise we determine the concentration of Ca\\(^{2+}\\) from a standard curve. Five standard solutions with known Ca\\(^{2+}\\) concentrations were prepared and the absorbance for each sample was measured (table 17.3).\n\n\n\nTable 17.3: Measured absorbance of standard solutions.\n\n\n\n\n\n\nCa\\(^{2+}\\) (ppm)\nAbsorbance at 422.7 nm\n\n\n\n\n0.0\n0.000\n\n\n2.0\n0.063\n\n\n5.0\n0.141\n\n\n8.0\n0.218\n\n\n10.0\n0.265\n\n\n\n\n\n\n\nUse the following code to get the data into Rstudio and plot the data. We want to find the relationship between concentration of Ca\\(^{2+}\\) and Absorbance. This is a linear model: \\(y = b + ax + e\\)\nIn this case we control the concentration of Ca\\(^{2+}\\) and we measure the absorbance. Hence, in the linear model \\(y\\) is the absorbance and \\(x\\) is the concentration of Ca\\(^{2+}\\). Here, \\(e\\) is the part of the variation not captured by the model.\n\n# Create dataframe with data\ndf &lt;- data.frame(\n  calcium = c(0.00,  2.00,  5.00,  8.00, 10.00),\n  abs     = c(0.000, 0.063, 0.141, 0.218, 0.265)\n)\n\n# Plot the data\nplt &lt;- ggplot(df, aes(calcium, abs)) +\n  geom_point()\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate slope and offset of the straight line (the standard curve) that best describes the relationship between Ca\\(^{2+}\\) and absorbance. Add it to the plot as a straight line.\n\nSee formulas and code below.\nHint: plt &lt;- plt + geom_abline(intercept = b, slope = a)\n\n\n\n\n\n\nThe slope, \\(a\\) is estimated by\n\\[\n  \\hat{a} =  \\frac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\n  \\]\nAnd the offset, \\(b\\) is estimated by \\[\n  \\hat{b} = \\bar{y}-\\hat{a}\\bar{x}\n  \\]\nHere is some R-code to help you.\n\nx &lt;- df$calcium\ny &lt;- df$abs\nmx &lt;- mean(x)\nmy &lt;- mean(y)\nSxx &lt;- sum((x - mx)^2)\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nTest for proportionality, i.e. no offset (\\(b = 0\\)). First calculate a confidence interval around your offset. Then state the null hypothesis and the alternative hypothesis. Test the null hypothesis. Does the t-test result correspond with the confidence interval?\n\n\n\n\n\nHere is some code to help you\n\nn &lt;- 5 # number of standard solutions\ne &lt;- y - (a * x + b) # model error\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nThree new milk samples with unknown concentration of Ca\\(^{2+}\\) were measured with atom absorption spectroscopy. Use the standard curve to quantify the concentrations of Ca\\(^{2+}\\) in the new samples. The absorbance values for the three new samples are found in table 17.4.\n\n\n\n\n\n\n\n\nTable 17.4: Absorbances for samples with unknown Ca\\(^{2+}\\) concentration.\n\n\n\n\n\n\nSample\nAbsorbance at 422.7 nm\nCa\\(^{2+}\\) (ppm)\n\n\n\n\nU1\n0.101\n\n\n\nU2\n0.147\n\n\n\nU3\n0.243\n\n\n\n\n\n\n\n\nUse the following code to get the unknown samples into R and add the samples to the standard curve.\n\n# Add you estimated concentrations here\nestimated_conc &lt;- c(1, 2, 3)\n\n# Create dataframe with unkown samples\ndf_new &lt;- data.frame(\n  sample = c(\"U1\", \"U2\", \"U3\"),\n  calcium = estimated_conc,\n  abs = c(0.101, .0147, 0.243)\n)\n\n\n# Add new samples to plot\nplt &lt;- plt +\n  geom_point(data = df_new, aes(calcium, abs), color = \"red\") +\n  geom_text(data = df_new, aes(label = sample), nudge_x = -.3)\n\nIn order to get the confidence interval for these estimates, we need to calculate the confidence limits for the estimated regression line.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate confidence bounds for a sequence of \\(x\\)-values (concentrations). Add the confidence bounds to the plot.\n\n\n\n\n\nHere is some code to help you:\n\n# Create a sequence of 1000 x values\nx &lt;- seq(0, 10, length.out = 1000)\n\n# Combine data into data frame.\ndf_conf &lt;- data.frame(\n  calcium = x,\n  lower = a * x + b ..., # Add equation for CI here\n  upper = a * x + b ..., # Add equation for CI here\n)\n\n\n# Add CI to plot\nplt &lt;- plt + \n  geom_line(data = df_conf, aes(calcium, lower), color = \"blue\") +\n  geom_line(data = df_conf, aes(calcium, upper), color = \"blue\")\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFind the confidence interval limits (upper and lower) for each prediction of the new samples (U1, U2, U3).\n\n\n\n\n\nHere is some code to help you:\n\n# Initiate vectors to store data in\nupper &lt;- c()\nlower &lt;- c()\n\nfor (i in 1:3) {\n  \n  # Find the x-values where the absolute difference is smallest\n  lower[i] &lt;- x[which.min(abs(df_conf$lower - df_new$abs[i]))]\n  upper[i] &lt;- x[which.min(abs(df_conf$upper - df_new$abs[i]))]\n  \n}\n\n# Add to data frame\ndf_new$lower_ci &lt;- lower\ndf_new$upper_ci &lt;- upper\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nHave a look at the widths of the confidence intervals (lower to upper) for each estimated concentration of Ca\\(^{2+}\\) (the three new samples).\n\n\nAre the confidence intervals having the same widths? Why/why not?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 17.6\n\n\n\n\n\n\n\n\n\nExercise 17.6 - Standard curve - quantification of phenol content in spice extracts\n\n\n\n\n\n\nOxidation of foodstuff is one of the main reasons leading to decreased quality. During lipid oxidation, lipid radicals are formed, which will increase the speed of oxidation. However, a number of spices contain antioxidants. These antioxidants are often phenol-containing molecules, which are able to donate a hydrogen atom to the lipid radicals and thereby decrease the speed of oxidation. By using a standard curve, we will in this exercise estimate the phenol content (and thereby the antioxidative effect) of extracts originating from oregano and tarragon (estragon).\nSix standard solutions have been prepared and measured with a spectrophotometer (absorbance at \\(765 nm\\)). Find the data in table 17.5.\nIn the previous exercise on standard curves we did hand calculations. In this exercise we will solve the problems using the built-in functions in R.\n\n\n\nTable 17.5: Concentrations and absorbances of standard solutions.\n\n\n\n\n\n\nConcentration (mg/L)\nAbsorbance\n\n\n\n\n2.5\n0.2747\n\n\n5.0\n0.5541\n\n\n7.5\n0.8363\n\n\n6.5\n1.2375\n\n\n10.0\n1.0931\n\n\n12.5\n1.3234\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nState the statistical model for the linear relationship between the phenol concentration and the absorbance at \\(765 nm\\).\nType the data into R and plot it using the following code. Here the shaded area corresponds to the confidence interval for the regression line.\n\n\n\n\n\n\n# Create the data frame\ndf &lt;- data.frame(\n  conc = c(2.5, 5.0, 7.5, 6.5, 10.0, 12.5),\n  abs  = c(0.2747, 0.5541, 0.8363, 1.2375, 1.0931, 1.3234)\n)\n\n# Plot data with regression line\nggplot(df, aes(conc, abs)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFit the model using the function lm(y ~ x, data = df), where y is your absorbance values and x is the concentrations.\nState the null hypothesis and the alternative hypothesis for both the slope and the intercept.\n\n\nCheck your coefficients with the function coef(mod), where mod is the estimated model object. Anything that seems suspicious? Look into the confidence interval for each coefficient. You can get the confidence interval using the function confint(mod). Anything here that seems suspicious?\nIn R, try to call; summary(mod)$coefficients. This will return your estimates, std. errors, t-values and the probabilities of your null hypotheses being true. Is the slope significantly different from 0?\n\n\nInspect the plot of the standard curve and decide whether some points should be excluded from the analyses. If you remove some data points, plot the reduced data, recalculate your model (reduced data) and investigate the coefficients (like question 4).\nMake a Q-Q plot to investigate whether the model residuals are normally distributed. You can extract your model residuals by mod_new$residuals, where mod_new is your model object. Are the residuals approximately normally distributed?\nUse the standard curve to estimate the antioxidative effect (phenol concentration) of the oregano and tarragon extracts. The two extracts were measured with a spectrophotometer (absorbance at 765 nm). Find the absorbance values in table 17.6.\n\n\n\n\n\n\n\n\nTable 17.6: Spice extracts and their absorbances.\n\n\n\n\n\n\nSample\nAbsorbance (765 nm)\nConcentration (mg/L)\n\n\n\n\nOregano\n0.2911\n\n\n\nTarragon\n0.9413\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nEstimate the confidence bounds around the phenol concentration of both Oregano and Tarragon extracts. Again we need to calculate confidence bounds for a sequence of \\(x\\)-values (concentrations) and then look for the point in each confidence band (lower and upper) where \\(y\\) equals the measured absorbance for each sample.\n\n\n\n\n\nHere is some code to help you:\n\n# Sequence of x-values (Concentrations)\nxx &lt;- seq(0, 12, length.out = 1000) \n\n# Predict y for each value in xx - including the confidence interval\nyy &lt;- predict(\n  your_model_object, \n  newdata = data.frame(Conc = xx),\n  interval = ’confidence’\n)\n\n# Initialize vectors for storage\nupper &lt;- vector()\nlower &lt;- vector()\n\nfor (i in 1:2) {\n  upper[i] &lt;- xx[which.min(abs(yy[,3]-UK[i,1]))]\n  lower[i] &lt;- xx[which.min(abs(yy[,2]-UK[i,1]))]\n}\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nUse these confidence bounds to decide if the concentration of phenol is different in the two extracts. What extract is having the best antioxidative effect?",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "chapters/week7/least_squares.html",
    "href": "chapters/week7/least_squares.html",
    "title": "18  Least squares",
    "section": "",
    "text": "18.1 Reading material",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Least squares</span>"
    ]
  },
  {
    "objectID": "chapters/week7/least_squares.html#reading-material",
    "href": "chapters/week7/least_squares.html#reading-material",
    "title": "18  Least squares",
    "section": "",
    "text": "Least squares - in short\nVideos on least squares and linear regression\n\nLinear Regression - Least Squares Criterion Part 1\nThe Main Ideas of Fitting a Line to Data\n\nChapter 5.1 and 5.2 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Least squares</span>"
    ]
  },
  {
    "objectID": "chapters/week7/least_squares.html#sec-intro-to-least-squares",
    "href": "chapters/week7/least_squares.html#sec-intro-to-least-squares",
    "title": "18  Least squares",
    "section": "18.2 Least squares - in short",
    "text": "18.2 Least squares - in short\nA model (ANOVA, linear regression, PCA,…) can be seen as a representation of the observed data, such that:\n\\[\nObserved = Systematic + Residuals\n\\]\nHere the aim is to choose some parameters for the systematic part, which makes the residuals small. Specifically small often refers to a small sum of squares. Brockhoff describes the case for linear regression, so here we will mention ANOVA problems and PCA.\n\n18.2.1 ANOVA - least Squares\nThe formula for an additive ANOVA model with two factors is listed below:\n\\[\n\\begin{gather}\nX_{i} = \\alpha(A_i) + \\beta(B_i) + e_{i} \\\\\nwhere \\ e_{i}\\sim \\mathcal{N}(0,\\sigma^2) \\ and \\ independent \\\\\nfor  \\ i=1,...,n\n\\end{gather}\n\\]\nIn this case the aim is to estimate some numbers for the parameters (\\(\\alpha(1),..,\\alpha(k_A)\\), and \\(\\beta(1),..,\\beta(k_B)\\)) - \\(k_A + k_B\\) in total - such that \\(\\sum{e_i^2}\\) is as small as possible. For what is called balanced studies, it turns out that using the group means within each factor, as estimates for \\(\\alpha()\\) and \\(\\beta()\\), gives the least \\(\\sum{e_i^2}\\) (sum of squared errors). The proof for this is found by calculating \\(\\sum{e_i^2}\\):\n\\[\nL = \\sum_{i=1}^n{e_i^2} =\\sum_{i=1}^n{(X_{i} - \\alpha(A_i) + \\beta(B_i))^2}\n\\]\nDifferentiation of this and setting it to zero:\n\\[\n\\frac{\\delta L}{\\delta \\alpha,\\delta \\beta} = .... = 0\n\\]\nFollowed by isolation of the parameters, it is possible to derive the estimates. These are called the Least Squares estimates.\nThe math is very similar to regression. It is however, beyond the curriculum to be able to do it for ANOVA and PCA problems.\n\n\n18.2.2 Principal Component Analysis - least squares\nIn PCA the multivariate dataset (\\(\\mathbf{X}\\)) is parameterized by scores (\\(\\mathbf{T}\\)) and loadings (\\(\\mathbf{P}\\)):\n\\[\n\\mathbf{X} = \\mathbf{T}\\mathbf{P}^T + \\mathbf{E} \\tag{1}\n\\]\nContrary to the univariate cases mentioned above, the data and parameters are here matrices, however, the aim is still to find some scores and loadings that minimizes \\(\\mathbf{E}\\) in a least squares sense: \\(\\sum{\\mathbf{E}_{ij}^2}\\), where \\(i\\) refers to the sample \\(i\\) and \\(j\\) refers to variable \\(j\\). I.e. \\(\\mathbf{E}_{3,4}\\) is the residual for sample \\(3\\) variable \\(4\\).\n\n\n\n\n\n\nExample 18.1\n\n\n\n\n\n\n\n\n\nExample 18.1 - Near-infrared spectroscopy of marzipan - least squares\n\n\n\n\n\n\nThe following example illustrates how we can use scores from a principal component analysis to predict the sugar content in marzipan bread (marcipanbrød).\nFirst we import the data. It is saved as an RData-file, so that is easy.\n\n# Load data\nload(\"Marzipan.RData\")\n\nThen we can plot the data.\n\n# Plot data colored according to sugar content\nggplot(Xm, aes(wavelength, value, color = sugar)) +\n  geom_line() +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  labs(\n    y = \"Absorbance\",\n    x = \"Wavelength (nm)\",\n    title = \"NIR spectrum of marzipan\",\n    color = \"Sugar content (%)\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n    )\n\n\n\n\n\n\n\nFigure 18.1: NIR spectra of 6 marzipan samples colored by sugar content.\n\n\n\n\n\nIn this example we want to make a model which can predict the sugar content from a spectrum.\nWe now make a PCA on the data and plot PC1 vs sugar content.\n\n# Transposing the data and removing the \"wavelength\" column\nXt = t(X[, -1])\n\n# Compute a PCA model on meant centered Xt\npca &lt;- prcomp(Xt, center = T, scale = F)\n\n# Extract the scores from the PCA model and the sugar content from data\ndf &lt;- data.frame(\n  scores = pca$x,\n  sugar = Y$sugar\n)\n\n# Extract the variance explained (%) for PC1\nvar_exp_pc1 &lt;- summary(pca)$importance[2, 1] * 100\n\n# Plot PC1 scores vs sugar content. Color points according to sugar content\nggplot(df, aes(scores.PC1, sugar)) + \n  geom_point(size = 2.5, aes(color = sugar)) +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  geom_smooth(method = \"lm\", se = F) + # Show regression line\n  labs(\n    y = \"Sugar content (%)\",\n    x = paste(\"PC1 (\", var_exp_pc1, \"%)\", sep = \"\"), # Add var.exp% as label\n    color = \"Sugar content (%)\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 18.2: PC1 scores plotted against sugar content of samples.\n\n\n\n\n\nThere is indeed a linear relation between the scores on PC1 and the sugar content in the marzipan breads. Let us make a linear regression model using the least squares approach with sugar content as dependent variable and the scores from PC1 as predictors:\n\nlinreg = lm(sugar ~ scores.PC1, df)\n\nFrom which we can extract the intercept, slope and \\(R^2\\):\n\nsummary(linreg)$r.squared\n\n[1] 0.8531228\n\nlinreg$coefficients\n\n(Intercept)  scores.PC1 \n  46.253124    4.620843 \n\n\nOur model of sugar content (\\(Y\\)) can be written as:\n\\[\nY = 4.6 \\cdot PC1_{scores} + 46.3\n\\]\nThis model is explaining \\(85\\%\\) of the variance of the response variable (sugar content).",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Least squares</span>"
    ]
  },
  {
    "objectID": "chapters/week7/least_squares.html#videos",
    "href": "chapters/week7/least_squares.html#videos",
    "title": "18  Least squares",
    "section": "18.3 Videos",
    "text": "18.3 Videos\n\nLinear Regression - Least Squares Criterion Part 1 - Patrick J\n\n\n\nThe Main Ideas of Fitting a Line to Data - StatQuest",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Least squares</span>"
    ]
  },
  {
    "objectID": "chapters/week7/least_squares.html#exercises",
    "href": "chapters/week7/least_squares.html#exercises",
    "title": "18  Least squares",
    "section": "18.4 Exercises",
    "text": "18.4 Exercises\n\n\n\n\n\n\nExercise 18.1\n\n\n\n\n\n\n\n\n\nExercise 18.1 - Least squares estimation\n\n\n\n\n\n\nThis exercise has the purpose of showing least squares estimation of the center of a distribution.\nLet \\(X_1,X_2,...,X_n\\) be some observed values. The least squares problem for the center of the distribution \\(\\mu\\) is defined as:\n\\[\nL(\\mu) = \\sum{(X_i - \\mu)^2}\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nShow that the solution that minimizes \\(L\\) is \\(\\mu = \\bar{X}\\) (the mean of \\(X\\)). That is done by differentiation of \\(L\\) with respect to \\(\\mu\\) and setting this to zero:\n\n\n\n\n\n\\[\n\\frac{\\delta L(\\mu)}{\\delta \\mu} = .... = 0\n\\]\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nSimulate some numbers in R using the function rnorm() and calculate the mean and median.\nCalculate \\(e_{mean,i} = X_i - \\bar{X}\\) and \\(e_{median,i} = X_i - X_{median}\\). (I.e. subtracting the mean and median from each value).\nPlot the residuals \\(e_{mean}\\) and \\(e_{median}\\), and add a horizontal line at \\(0\\) (see code below).\n\n\n\n\n\n\n# Set grid for plotting\npar(mfrow = c(1, 2))\n\n# Plot 1\nplot(x - mean(x))\nabline(h = 0, col = \"red\")\n\n# Plot 2\nplot(x - median(x)) \nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nAdd two positive outliers to the data by e.g. x &lt;- c(rnorm(30), 20, 23), and redo the plotting.\nComment on what you see. In case of outliers, which method produces meaningful estimates and residuals?",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Least squares</span>"
    ]
  },
  {
    "objectID": "chapters/week8/week_8.html",
    "href": "chapters/week8/week_8.html",
    "title": "Week 8",
    "section": "",
    "text": "Hand-in assignment\nThis week is going to extend regression to several predictors. Further, least squares for more complicated problems will be pursued.\nExercise 19.1 Diet and fat metabolism - regression with several variables is to be handed in (through absalon or as hard-copy Wednesday night). You are welcome to put in R-code in the assignment, but it is your argumentation and interpretation that are the most important.",
    "crumbs": [
      "Week 8"
    ]
  },
  {
    "objectID": "chapters/week8/week_8.html#exercises",
    "href": "chapters/week8/week_8.html#exercises",
    "title": "Week 8",
    "section": "Exercises",
    "text": "Exercises\nFor Monday work through the following exercises:\n\nExercise 20.1 \nExercise 20.2 \n\nFor Wednesday work through the following exercises:\n\nExercise 20.3 \nExercise 20.4 \n\nFurther, this week might allow you to recap on some of the exercises you did not make during the last weeks.",
    "crumbs": [
      "Week 8"
    ]
  },
  {
    "objectID": "chapters/week8/week_8.html#case-iv",
    "href": "chapters/week8/week_8.html#case-iv",
    "title": "Week 8",
    "section": "Case IV",
    "text": "Case IV\nThe fourth case should be handed in as a slide-show with voice no later than the Friday evening this week.",
    "crumbs": [
      "Week 8"
    ]
  },
  {
    "objectID": "chapters/week8/mlr.html",
    "href": "chapters/week8/mlr.html",
    "title": "19  Multiple linear regression",
    "section": "",
    "text": "19.1 Reading material",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "chapters/week8/mlr.html#reading-material",
    "href": "chapters/week8/mlr.html#reading-material",
    "title": "19  Multiple linear regression",
    "section": "",
    "text": "Intro to regression\nVideos on mutiple linear regression:\n\nMultiple Linear Regression - An Introduction\nMultiple Regression, Clearly Explained\n\nChapter 6.1 of Introduction to Statistics by Brockhoff",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "chapters/week8/mlr.html#intro-to-multiple-linear-regression",
    "href": "chapters/week8/mlr.html#intro-to-multiple-linear-regression",
    "title": "19  Multiple linear regression",
    "section": "19.2 Intro to multiple linear regression",
    "text": "19.2 Intro to multiple linear regression\nRegression models with several predictors are simply extensions of the simple linear regression with a single predictor.\nA two-predictor model with \\(X1\\) and \\(X2\\) and a single response variable (\\(Y\\)) can be formalized as:\n\\[\n\\begin{gather}\nY_i = \\alpha + \\beta_1 \\cdot X1_i + \\beta_2 \\cdot X2_i + e_i \\\\\nwhere \\ e_i\\sim \\mathcal{N}(0,\\sigma^2) \\ and \\ independent \\\\\nfor \\ i=1,...,n\n\\end{gather}\n\\]\nWhere \\(\\alpha\\) refers to the level of \\(Y\\) at \\(X1=X2=0\\) and \\(\\beta_1\\) and \\(\\beta_2\\) are slopes in relation to \\(X1\\) and \\(X2\\).\nEach of these three parameters consumes one degree of freedom.\nThe natural hypothesis is to check whether \\(X1\\) in the presence of \\(X2\\) has an effect on (\\(Y\\)), that is: \\(H0: \\beta_1=0\\) with the alternative \\(HA: \\beta_1 \\neq 0\\) (or the other way around). This can be tested with either an F-test or a T-test (yielding the same results).\n\n19.2.1 Marginal and Crude estimates\nIn a setup as described above, the investigation of the effect of \\(X1\\) on \\(Y\\) can be done by two approaches:\n\nA simple regression model: \\(Y_i = \\alpha + \\beta X1_i + e_i\\)\nA multiple regression model: \\(Y_i = \\alpha + \\beta_1 X1_i +\\beta_2 X2_i + e_i\\)\n\nNaturally these two approaches yield different results due to the absence/presence of the variable \\(X2\\). From the simple regression model \\(\\beta\\) shows the crude effect of \\(X1\\) on \\(Y\\), whereas in the multiple regression model \\(\\beta_1\\) shows the marginal effect of \\(X1\\) on \\(Y\\). Sometimes comparison of these models are referred to as adjusting for \\(X2\\) or controlling for \\(X2\\). These models often elucidate the direct and indirect relation between predictors and responses as is seen in exercise 1.\n\n\n\n\n\n\nExample 19.1\n\n\n\n\n\n\n\n\n\nExample 19.1 - Near-infrared spectroscopy of marzipan - regression\n\n\n\n\n\n\nIn the following example we want to investigate if we can improve a prediction model by using scores from more than one principal component to predict the sugar content in marzipan bread (marcipanbrød).\nThe PCA model is exactly the same as in Example 18.1.\n\n\nCode\npca &lt;- prcomp(t(X[, -1]), center = T, scale = F)\nvar_exp &lt;- summary(pca)$importance[2, ] * 100\ndf &lt;- data.frame(\n  pca$x,\n  sugar = Y$sugar\n)\n\nggplot(df, aes(PC1, PC2)) + \n  geom_point(size = 2.5, aes(color = sugar)) +\n  scale_color_gradient(low = \"green\", high = \"red\") +\n  labs(\n    y = paste(\"PC2 (\", var_exp[2], \"%)\", sep = \"\"),\n    x = paste(\"PC1 (\", var_exp[1], \"%)\", sep = \"\"),\n    color = \"Sugar content (%)\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 19.1: Score plot of PC1 vs PC2 on a PCA model computed on NIR spectra of marzipan samples\n\n\n\n\n\nWe now make two models:\n\nA linear regression model on the sugar content versus the scores from PC1 (see @Example 18.1).\nA linear regression model on the sugar content versus the scores from PC1 and PC2.\n\n\nlinreg1 &lt;- lm(sugar ~ PC1, df)\nlinreg2 &lt;- lm(sugar ~ PC1 + PC2, df)\n\nLet us look at the summary of the two models:\n\nsummary(linreg1)\n\n\nCall:\nlm(formula = sugar ~ PC1, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1351 -4.3117 -0.6425  3.6072 11.6073 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2531     0.8872   52.13  &lt; 2e-16 ***\nPC1           4.6208     0.3501   13.20 4.97e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.019 on 30 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8482 \nF-statistic: 174.3 on 1 and 30 DF,  p-value: 4.969e-14\n\nsummary(linreg2)\n\n\nCall:\nlm(formula = sugar ~ PC1 + PC2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7699 -3.5350 -0.7164  2.7634 11.2274 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2531     0.8063  57.367  &lt; 2e-16 ***\nPC1           4.6208     0.3181  14.526 7.67e-15 ***\nPC2          -1.1724     0.4331  -2.707   0.0113 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.561 on 29 degrees of freedom\nMultiple R-squared:  0.8827,    Adjusted R-squared:  0.8747 \nF-statistic: 109.2 on 2 and 29 DF,  p-value: 3.178e-14\n\n\nFrom the summary of linreg2 we see that the PC2 estimate is slightly significantly different from \\(0\\) (\\(p-value = 0.01\\)) and does contribute to improving the model. If we think back on example Example 5.2, we learned that the second principal component had something to do with the color of the marzipan bread. In that regard it does not make a lot of sense that adding a color term (captured by the scores on PC2) into your model improve the prediction of the sugar content. However, when comparing the \\(R^2\\) values for the model including only one component (\\(R^2 = 0.85\\)) and a model with two components (\\(R^2 = 0.88\\)) it is seen that the improvement does not practically make a difference.\nOne should always be careful not to build models on meaningless data. PCA is a powerful tool for exploring data, but you need to choose meaningful principal components for your analysis. This is also relevant when selecting variables for the analysis.\nMaybe the wavelengths covering the visible part of the spectrum should have been left out of the analysis from the beginning as they do not contain information about the sugar content?",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "chapters/week8/mlr.html#videos",
    "href": "chapters/week8/mlr.html#videos",
    "title": "19  Multiple linear regression",
    "section": "19.3 Videos",
    "text": "19.3 Videos\n\nMultiple Linear Regression - An Introduction - Lee Rusty Waller\n\n\n\nMultiple Regression, Clearly Explained - StatQuest",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "chapters/week8/mlr.html#exercises",
    "href": "chapters/week8/mlr.html#exercises",
    "title": "19  Multiple linear regression",
    "section": "19.4 Exercises",
    "text": "19.4 Exercises\n\n\n\n\n\n\nExercise 19.1\n\n\n\n\n\n\n\n\n\nExercise 19.1 - Diet and fat metabolism - regression with several variables\n\n\n\n\n\n\nThis exercise examines the relation between a biomarker, dietary intervention and weight in order to disentangle the effects causing elevated levels of cholesterol.\nThe data for this exercise is the same as for Exercise “Diet and Fat”.\nIn this exercise we are going to focus on predicting cholesterol from both weight and dietary intervention.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nPlot, formulate and build univariate models predicting cholesterol level from:\n\n\nDietary intervention ($Fat_Protein) - A oneway ANOVA model.\nBody weight at \\(14\\) weeks ($bw_w14) - A regression model.\n\n\nState and test relevant hypothesis for these two models.\nComment on the relations between the conclusions of the two models.\nMake a scatter plot of cholesterol versus weight colored or shaped according to dietary intervention.\nMake a regression model for cholesterol with two predictors: i) Weight at week \\(14\\) and ii) Dietary intervention, and test the factors.\n\n\n\n\n\nNote: In contrast to ANOVA with one dependent variable, the order of the dependent variables makes a difference in the anova(). In order to surpass this, use the drop1(aov(),test='F') function to get results independent of order.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nWhat happens to the significance of the individual factors when you change the analysis as described above?\nDo you think that the dietary intervention directly affects the cholesterol level, or that it is mediated through body weight?",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "chapters/week8/explained_variance.html",
    "href": "chapters/week8/explained_variance.html",
    "title": "20  Explained variance",
    "section": "",
    "text": "20.1 Reading material",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Explained variance</span>"
    ]
  },
  {
    "objectID": "chapters/week8/explained_variance.html#reading-material",
    "href": "chapters/week8/explained_variance.html#reading-material",
    "title": "20  Explained variance",
    "section": "",
    "text": "Explained variance - in short\nVideos on R-squared:\n\nWhat does r squared tell us? What does it all mean\nR-squared, Clearly Explained",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Explained variance</span>"
    ]
  },
  {
    "objectID": "chapters/week8/explained_variance.html#sec-explained-var-intro",
    "href": "chapters/week8/explained_variance.html#sec-explained-var-intro",
    "title": "20  Explained variance",
    "section": "20.2 Explained variance - in short",
    "text": "20.2 Explained variance - in short\nExplained variance refers to how well the estimated model describes the observed data. That is, how much of the variation is systematic and how much is residual. In the figure below are shown two examples.\n\n\n\n\n\n\n\n\nFigure 20.1: Systematic variation (blue) and residual variation (red).\n\n\n\n\n\nThe explained variance is summarized in the so-called \\(R^2 \\in \\left[ 0,1 \\right]\\) metric. Where a value close to \\(1\\) indicates high degree of explained variance, and a value close to \\(0\\) the contrary.\n\\(R^2\\) is based on the sums of squares of the different model terms. Given the model:\n\\[\nX = S + E\n\\]\nWhere \\(X\\) refers to the observed data, \\(S\\) the systematic parameterized part of the model and \\(E\\) the residuals, then:\n\\[\nSS(X) = SS(S) + SS(E)\n\\]\nI.e. the sums of squares are additive. From this the \\(R^2\\) value is defined as:\n\\[\nR^2 = \\frac{SS(S)}{SS(X)} = \\frac{SS(X) - SS(E)}{SS(X)} = 1 - \\frac{SS(E)}{SS(X)}\n\\]\nIn both ANOVA and linear regression these \\(SS()\\) values are readily available from the analysis table, where \\(SS(X) = SS_{tot}\\) and \\(SS(E) = SS_e\\). Further, it is possible to calculate the \\(R^2\\) for the entire model, but also for individual factors in twoway ANOVA and regression with multiple descriptors, simply by using the individual \\(SS()\\) contributions.\n\n20.2.1 Model estimates (\\(\\hat{y}\\)) and \\(R^2\\)\nIn linear regression the model is stated as:\n\\[\n\\begin{gather}\ny_{i} = \\alpha + \\beta \\cdot x_i + e_i \\\\\nwhere \\ e_{i}\\sim \\mathcal{N}(0,\\sigma^2) \\ and \\ independent \\\\\nfor  \\ i=1,...,n\n\\end{gather}\n\\]\nThe predicted values for \\(y\\) (referred to as \\(\\hat{y}\\)) can be expressed as:\n\\[\n\\hat{y}_{i} = \\hat{\\alpha} + \\hat{\\beta} \\cdot x_i\n\\]\nwhere the estimates of the slope and the intercept is used for calculating \\(y\\) based on \\(x\\).\nThe \\(R^2\\) value for such a model can also be seen as:\n\\[\nR^2 = r_{\\hat{y},y}^2\n\\]\nThat is, taking the squared value of the correlation coefficient between the observed response and the predicted response yields the explained variance by the model.\nThis concept is valid for for a range of models including ANOVA, PCA and linear regression.\n\n\n20.2.2 Correlation coefficient and \\(R^2\\)\nIn bi-variate correlation analysis, the correlation coefficient is directly related to \\(R^2\\), as: I\nThe implication of this, is that the direction is lost (\\(R^2 = 0.3^2 = (-0.3)^2\\)), but the strength of the relation is not.\n\n\n20.2.3 \\(R^2\\) for PCA\nIn a PCA model, it is possible to talk about \\(R^2\\) values for the entire model or for the individual components. It is so, by definition, that the contribution from the individual components is decreasing, that is:\n\\[\nR_{PC1}^2 \\geq R_{PC2}^2 \\geq \\ldots \\geq R_{PCk}^2\n\\]\nFurther, from a \\(k\\) component model follows that total explained variance is:\n\\[\nR_{PC1-PCk}^2  = R_{PC1}^2 + R_{PC2}^2 + \\ldots + R_{PCk}^2 = \\sum_{i=1}^k{R_{PCi}^2}\n\\]\n\nPCA vs Regression/ANOVA\nIn PCA, by increasing the number of components you will eventually reach explained variance of \\(1\\). This is due to the parameters of this model is solely based on the response variables, so there is no set of independent variables, as there is in ANOVA and linear regression. This has the implication, that comparing \\(R^2\\) values between e.g. an ANOVA model and a PCA model is not at all straight forward.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Explained variance</span>"
    ]
  },
  {
    "objectID": "chapters/week8/explained_variance.html#videos",
    "href": "chapters/week8/explained_variance.html#videos",
    "title": "20  Explained variance",
    "section": "20.3 Videos",
    "text": "20.3 Videos\n\nWhat does r squared tell us? What does it all mean - MrNystrom\n\n\n\nR-squared, Clearly Explained - StatQuest",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Explained variance</span>"
    ]
  },
  {
    "objectID": "chapters/week8/explained_variance.html#exercises",
    "href": "chapters/week8/explained_variance.html#exercises",
    "title": "20  Explained variance",
    "section": "20.4 Exercises",
    "text": "20.4 Exercises\n\n\n\n\n\n\nExercise 20.1\n\n\n\n\n\n\n\n\n\nExercise 20.1 - Explained variance (R-squared) - regression n’ correlation\n\n\n\n\n\n\nThis exercise deals with explained variance and its relation to the residual deviation. This exercise uses the dietary intervention on mice and the effect on biomarkers related to fat metabolism.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nFirst, we are going to deal with insulin and cholesterol. Make three analyses:\n\nRegress cholesterol on insulin.\nRegress insulin on cholesterol.\nMake a correlation analysis between the two.\n\nMake a drawing (not exact, just some dots) of cholesterol versus insulin and indicate a regression line, and the the residuals for the first two models in Q1.\nFor each model, calculate \\(R^2\\).\n\n\nHint: You can use the \\(SS\\) measures of an aov() model, or use summary() of a lm() model directly.\n\n\nComment on what you observe.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.2\n\n\n\n\n\n\n\n\n\nExercise 20.2 - R-squared and outliers\n\n\n\n\n\n\nThis exercise should show how extreme outliers can influence the \\(R^2\\) measure by making it unrealistically high.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nSimulate two sets of vector \\(x\\) and \\(y\\), each of \\(n=15\\) points which are not related. (use rnorm() in R for doing so).\nScatter plot \\(x\\) and \\(y\\) and add the best regression line. (code hint: + stat_smooth(method = \"lm\"))\nCalculate the correlation coefficient (\\(r\\)) and the \\(R^2\\) value.\nDo the stats (\\(r\\) and \\(R^2\\)) and the plot tells the same story?\nNow add an extreme point to both \\(x\\) and \\(y\\) (x &lt;- c(x,extremenumber)).\nRepeat plotting and stat calculation (\\(r\\) and \\(R^2\\)).\nWhat happened?\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTake home message: \\(R^2\\) alone without visual inspection can be misleading.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.3\n\n\n\n\n\n\n\n\n\nExercise 20.3 - R-squared and transformations\n\n\n\n\n\n\nThis exercise should show how transformation influences the \\(R^2\\) measure. The exercise is using the wine dataset.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nImport data, and make a plot of the response variable Ethyl.pyruvate inferring country membership.\nMake a oneway ANOVA model for this response variable, and check (or calculate) the \\(R^2\\) value.\nMake a transformation of the response variable, and repeat plotting, modeling and \\(R^2\\) calculation.\nCompare the \\(R^2\\) for the raw response and transformed response, and figure out (based on the plots) which samples that are causing the difference.\nMake a check of model assumptions for both models, and try to fix the issues by outlier removal.\n\n\nHint: a simple way to remove samples and update models can be seen below.\n\n\n\n\n\n\nic &lt;- wine$Ethyl.pyruvate &lt; ...\nm &lt;- lm(data = wine[ic, ], ...)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 20.4\n\n\n\n\n\n\n\n\n\nExercise 20.4 - Explained variance and PCA\n\n\n\n\n\n\nThis exercise deals with explained variance in relation to PCA. This exercise uses the dietary intervention on mice and the effect on biomarkers related to fat metabolism.\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nInitially extract and scale the biomarkers in the dataset (use X &lt;- scale() for this purpose).\nCalculate the total sums of squares for these data? (sum(X^2))\nCalculate a PCA model on these preprocessed data, and plot it using ggbiplot().\nCalculate the residuals after the first component. This is done by calculating the predicted values and subtracting those from the data (see code below).\n\n\n\n\n\n\n# \"pca\" refers to a PCA model built by \"prcomp()\"\nXhat &lt;- pca$x[ , 1] %o% mm$rotation[ , 1]\nE &lt;- X - Xhat\n\n\n\n\n\n\n\nTasks\n\n\n\n\n\n\n\nCalculate the residual sums of squares after removing the first component.\nCalculate the \\(R^2\\) for the first component.\nTry to calculate the \\(R^2\\) value from the correlation between \\(\\mathbf{X}\\) and \\(\\hat{\\mathbf{X}}\\): CODE HERE\nTry to modify the code in Q4 to be able to calculate for component \\(2\\), \\(3\\),…\n\n\n\n\n\nEventually you can match your results with the ones produced by ggbiplot().",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Explained variance</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html",
    "href": "chapters/appendix/data_wrangling.html",
    "title": "Appendix A — Wrangling data in R",
    "section": "",
    "text": "A.1 Logical indexing\nLogical indexing is one of the most powerful data wrangling techniques. It uses logical operators like == (equal to) and != (not equal to) to subset data based on multiply criteria. Below are some examples to explain indexing, logical operators, and finally subsetting data with logical indexing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#logical-indexing",
    "href": "chapters/appendix/data_wrangling.html#logical-indexing",
    "title": "Appendix A — Wrangling data in R",
    "section": "",
    "text": "A.1.1 Indexing data\nIf x is a vector of the letters A through F in alphabetical order, then we can call the fourth letter (D) by the syntax x[4]:\n\nx &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\n\nx[4]\n\n[1] \"D\"\n\n\nIf x is a matrix (or a dataframe) then we can index the data by the syntax x[row, column]. For example\n\nx[2, 3] - Extract the data in row 2, column 3.\nx[ , 3] - Extract all rows in column 3.\nx[2, ] - Extract all columns in row 2.\n\n\n# Create a matrix with 3 columns\nx &lt;- matrix(c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"), ncol = 3)\n\n# Print full matrix\nx\n\n     [,1] [,2] [,3]\n[1,] \"A\"  \"C\"  \"E\" \n[2,] \"B\"  \"D\"  \"F\" \n\n# Row 2, column 3\nx[2, 3]\n\n[1] \"F\"\n\n# All rows, column 3\nx[ , 3]\n\n[1] \"E\" \"F\"\n\n# Row 2, all columns\nx[2, ]\n\n[1] \"B\" \"D\" \"F\"\n\n\n\n\nA.1.2 Logical operators in R\nLogical operators are basic statements that return TRUE or FALSE (table A.1).\n\n\n\nTable A.1: Some of the logical operators available in R\n\n\n\n\n\n\n\n\n\n\n\nName\nOperator\nReturns TRUE\nReturns FALSE\n\n\n\n\nEqual to\n==\n\"A\" == \"A\"\n\"A\" == \"B\"\n\n\nNot equal to\n!=\n\"A\" != \"B\"\n\"A\" != \"A\"\n\n\nLess than\n&lt;\n2 &lt; 3\"\n3 &lt; 2\n\n\nGreater than\n&gt;\n15 &gt; 10\n10 &gt; 15\n\n\nLess than or equal to\n&lt;=\n10 &lt;= 10 or 9 &lt;= 10\n10 &lt;= 9\n\n\nGreater than or equal to\n&gt;=\n22 &gt;= 22 or 25 &gt;= 22\n20 &gt;= 15\n\n\nIn\n%in%\n\"A\" %in% c(\"A\", \"B\" \"C\")\n\"F\" %in% c(\"A\", \"B\" \"C\")\n\n\n\n\n\n\nIf we have a vector x with multiple instances of A, B and C we can index it as:\n\n# Create the vector\nx &lt;- rep(c(\"A\", \"B\", \"C\"), each = 3)\n\n# Print the vector\nx\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n# Check where the elements of x equals \"B\"\nx == \"B\"\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nIf we want to check multiple statements at once we can use the AND & and the OR | operator.\n\n# Check where the elements of x equals \"B\" OR \"C\"\nx == \"A\" | x == \"C\"\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n# Check where the elements of x equals \"B\" AND \"C\"\nx == \"A\" & x == \"C\"\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n\nA.1.3 Logical indexing of data\nIt is possible to use a logical vector (i.e. a vector that only contains TRUE and FALSE) to index data. When passing a logical vector to a data object it returns the data in all the places where the vector equals TRUE.\nAs an example, we load the palmerpenguins dataset, inspect it and do some simple logical indexing.\n\n# Inspect the data\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Extract all rows where species equals \"Adelie\"\nx &lt;- penguins[penguins$species == \"Adelie\", ]\n\nhead(x)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Extract all rows where species equals \"Adelie\"\n# AND body mass in greater than 4000\nx &lt;- penguins[penguins$species == \"Adelie\" & penguins$body_mass_g &gt; 4000, ]\n\nhead(x)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 &lt;NA&gt;    &lt;NA&gt;                NA            NA                  NA          NA\n2 Adelie  Torgersen           39.2          19.6               195        4675\n3 Adelie  Torgersen           42            20.2               190        4250\n4 Adelie  Torgersen           34.6          21.1               198        4400\n5 Adelie  Torgersen           42.5          20.7               197        4500\n6 Adelie  Torgersen           46            21.5               194        4200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#sec-pipe-operator",
    "href": "chapters/appendix/data_wrangling.html#sec-pipe-operator",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.2 The pipe operator",
    "text": "A.2 The pipe operator\nIn R you have the opportunity to use a special syntax called the pipe operator |&gt;. It can be thought of as a pipe that “sends data downstream” to the next call in your script.\nIf we for example want to take the square root of some data df and afterwards show the first 6 values of that data with head() we can write\n\ndf |&gt;\n  sqrt() |&gt;\n  head()\n\nwhich is equivalent to\n\nhead(sqrt(df))\n\nThis might not make much sense for this example, but when things get a bit more complex the pipe operator can really help by making code easier to read.\nIf we for example want to format some data df and then plot it in ggplot2 we can write the following script\n\ndf |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"group\",\n    names_prefix = \"X\",\n    values_to = \"response\"\n    ) |&gt;\n  mutate(group = str_c(\"Group \", group)) |&gt;\n  ggplot(aes(group, response)) +\n  geom_boxplot()\n\nWithout the pipe we would need to create several intermediate variables cluttering up our script and environment in the process.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#tidy-data",
    "href": "chapters/appendix/data_wrangling.html#tidy-data",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.3 Tidy data",
    "text": "A.3 Tidy data\n“Tidy data” is a way of storing data in tables that is commonly used in R. A dataset is considered tidy when it adheres to these two principles:\n\nEach column is variable.\nEach row is an observation.\n\nIt is easier to understand by looking at an example:\nLet us imagine that we have conducted a weight loss study by weighing 10 persons each week for 4 weeks total. That is\n\\[\nn = 10 \\times 4 = 40 \\text{ observations.}\n\\]\nThe data from the study is loaded into R and displayed.\n\nhead(weight_data) # Show the first 6 rows of the data\n\n  person    week1     week2     week3    week4\n1      1 71.41917 102.63595  42.91850 71.61371\n2      2 40.20826  82.03842  87.91434 69.71862\n3      3 82.35437  75.65833  89.92353 72.54436\n4      4 61.89362  34.66988  31.93082 83.58691\n5      5 51.56536  99.22212 121.20656 69.10402\n6      6 80.41455 105.88376  65.17454 96.28712\n\n\nIt is clear that this data does not currently conform to the principles of tidy data. The variable “Which week is this measurement from?” is spread out across the column names of the last 4 columns. That is not practical for working with it in R, and something as simple as plotting the data using ggplot2 would be almost impossible.\nIn the next section, we use a bit of code to convert this into a tidy data format.\n\nA.3.1 How to convert to tidy data\nUsing Tidyverse (remember to load it with library(tidyverse)), it is possible to pivot our current data frame into the longer tidy format.\n\nweight_data_tidy &lt;- weight_data |&gt; \n  pivot_longer(\n    cols = -person, # Which columns to pivot (everything but \"person\")\n    names_to = \"week\", # Name of the column to store previous column names\n    values_to = \"weight\" # Name of the column to store data in\n  )\n\nhead(weight_data_tidy)\n\n# A tibble: 6 × 3\n  person week  weight\n   &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1      1 week1   71.4\n2      1 week2  103. \n3      1 week3   42.9\n4      1 week4   71.6\n5      2 week1   40.2\n6      2 week2   82.0\n\n\nNow the data has been converted into a tidy format. Each row is an observation, and each column is a variable describing a parameter of the observation:\n\nperson is the person number.\nweek is the week of the weight measurement.\nweight is the weight recorded in that observation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "href": "chapters/appendix/data_wrangling.html#sec-summarise-by-group",
    "title": "Appendix A — Wrangling data in R",
    "section": "A.4 Summarise by group",
    "text": "A.4 Summarise by group\nSometimes we want to calculate a statistic per group. There are many different ways of doing this, and in these examples we are going to present a way possible ways.\nFor these examples we use the Palmer Penguins dataset. It consists of some data regarding the bill size, flipper length and body mass of three penguin species across three islands.\nLet us start by taking a look at our data. This can be done via the head() function or the glimpse() function from the Tidyverse.\n\nhead(penguins) # Show the first 6 rows of dataframe\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nA.4.1 One statistic per group\nIf we want to summarise one statistic, that could be the mean or the standard deviation, grouped by species or island (or both), we can do it in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.2.\n\n\nTo compute the mean of all numeric variables we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want the mean for all numeric variables grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(\n      .cols = where(is.numeric), # Chose columns that are numeric \n      .fns = mean # Set the function we want to use\n           ),\n    .by = species # Group by species\n    )\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3706. 2008.\n2 Gentoo              47.6          15.0              217.       5092. 2008.\n3 Chinstrap           48.8          18.4              196.       3733. 2008.\n\n\nBe aware that the function drops all non-numeric variables that are not part of the grouping. So the output of the above code is “missing” the island and sex variables. Also, the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass a vector of the variables to the .by = argument.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    across(where(is.numeric), mean),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Torgersen Adelie              39.0          18.5              192.       3709.\n2 Biscoe    Adelie              39.0          18.4              189.       3710.\n3 Dream     Adelie              38.5          18.2              190.       3701.\n4 Biscoe    Gentoo              47.6          15.0              217.       5092.\n5 Dream     Chinstrap           48.8          18.4              196.       3733.\n# ℹ 1 more variable: year &lt;dbl&gt;\n\n\n\n\nTo compute the mean of all variables grouped by species we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\naggregate(penguins_clean, list(species = penguins_clean$species), mean)\n\n    species species island bill_length_mm bill_depth_mm flipper_length_mm\n1    Adelie      NA     NA       38.82397      18.34726          190.1027\n2 Chinstrap      NA     NA       48.83382      18.42059          195.8235\n3    Gentoo      NA     NA       47.56807      14.99664          217.2353\n  body_mass_g sex     year\n1    3706.164  NA 2008.055\n2    3733.088  NA 2007.971\n3    5092.437  NA 2008.067\n\n\nBe aware that all non-numeric variables will return NA (and thus a lot of warnings). Also, the na.omit() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\naggregate(penguins_clean, \n          list(species = penguins_clean$species,\n               island = penguins_clean$island), \n          mean)\n\n    species    island species island bill_length_mm bill_depth_mm\n1    Adelie    Biscoe      NA     NA       38.97500      18.37045\n2    Gentoo    Biscoe      NA     NA       47.56807      14.99664\n3    Adelie     Dream      NA     NA       38.52000      18.24000\n4 Chinstrap     Dream      NA     NA       48.83382      18.42059\n5    Adelie Torgersen      NA     NA       39.03830      18.45106\n  flipper_length_mm body_mass_g sex     year\n1          188.7955    3709.659  NA 2008.136\n2          217.2353    5092.437  NA 2008.067\n3          189.9273    3701.364  NA 2008.018\n4          195.8235    3733.088  NA 2007.971\n5          191.5319    3708.511  NA 2008.021\n\n\n\n\n\n\n\nA.4.2 Create a summary table\nSometimes we want to create a summary table grouped by some variable. This can be done in the following ways.\n\nUsing TidyverseUsing base R\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the pipe operator |&gt; is unkown to you see Section A.2.\n\n\nTo compute a summary table of a numeric variable we can use the summarise() function from the dplyr package in Tidyverse. Remember to load the tidyverse package (library(tidyverse)) - this package includes dplyr as well as a whole lot of other nice packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = species # Group by species\n  )\n\n# A tibble: 3 × 6\n  species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie      146 3706.   3700  459.  638.\n2 Gentoo      119 5092.   5050  501.  800 \n3 Chinstrap    68 3733.   3700  384.  462.\n\n\nBe aware that the drop_na() function removes all rows with missing values - if this is not done all columns with missing values will return NA.\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the list.\n\npenguins |&gt;\n  drop_na() |&gt; # Remove rows with missing values\n  summarise(\n    N = length(body_mass_g),\n    Mean = mean(body_mass_g),\n    Median = median(body_mass_g),\n    Std = sd(body_mass_g),\n    IQR = IQR(body_mass_g),\n    .by = c(island, species) # Group by island and species\n  )\n\n# A tibble: 5 × 7\n  island    species       N  Mean Median   Std   IQR\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Torgersen Adelie       47 3709.   3700  452.  662.\n2 Biscoe    Adelie       44 3710.   3750  488.  588.\n3 Dream     Adelie       55 3701.   3600  449.  588.\n4 Biscoe    Gentoo      119 5092.   5050  501.  800 \n5 Dream     Chinstrap    68 3733.   3700  384.  462.\n\n\n\n\nTo compute a summary table of a numeric variable we can use the aggregate() function. This is included in base R, so there is no need to load any packages.\nFor this example we want some summary statistics for the body_mass_g variable grouped by species.\n\n# Remove rows with missing values \npenguins_clean &lt;- na.omit(penguins)\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n  )\n\nprint(penguins_summary)\n\n    Species   N     Mean   Median      Std      IQR\n1    Adelie 146 3706.164 3706.164 3706.164 3706.164\n2 Chinstrap  68 3733.088 3733.088 3733.088 3733.088\n3    Gentoo 119 5092.437 5092.437 5092.437 5092.437\n\n\nIf we want to group by multiple variables, for example species and island, we just need to pass another grouping variable to the my_groups list.\n\n# Define variables for grouping\nmy_groups &lt;- list(species = penguins_clean$species,\n                  island = penguins_clean$island)\n\n# Compute summary stats\npenguins_n &lt;- aggregate(penguins_clean, my_groups, length)\npenguins_mean &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_median &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_std &lt;- aggregate(penguins_clean, my_groups, mean)\npenguins_iqr &lt;- aggregate(penguins_clean, my_groups, mean)\n\n# Collect everything in a dataframe\npenguins_summary &lt;- data.frame(\n  \"Species\" = penguins_n$species,\n  \"Island\" = penguins_n$island,\n  \"N\" = penguins_n$body_mass_g,\n  \"Mean\" = penguins_mean$body_mass_g,\n  \"Median\" = penguins_median$body_mass_g,\n  \"Std\" = penguins_std$body_mass_g,\n  \"IQR\" = penguins_iqr$body_mass_g\n)\n\nprint(penguins_summary)\n\n    Species    Island   N     Mean   Median      Std      IQR\n1    Adelie    Biscoe  44 3709.659 3709.659 3709.659 3709.659\n2    Gentoo    Biscoe 119 5092.437 5092.437 5092.437 5092.437\n3    Adelie     Dream  55 3701.364 3701.364 3701.364 3701.364\n4 Chinstrap     Dream  68 3733.088 3733.088 3733.088 3733.088\n5    Adelie Torgersen  47 3708.511 3708.511 3708.511 3708.511",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Wrangling data in R</span>"
    ]
  }
]